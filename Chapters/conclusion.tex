% Chapter Template

\chapter{Conclusion and Future Outlook} 
\label{conclusion} 

This thesis lies at the intersection of multiscale modelling (MM) and machine learning (ML). In the following, the main results of this work are summarized. To this end, 

\section{Multsicale Modelling}

%Understanding molecular processes is central to a wide range of modern research areas. The motions of microscopic particles can be described by quantum-mechanics or Newton's equation of motion. However, analytical solutions for most molecular systems are intractable, because of the enormous number of microscopic degrees of freedom. Therefore, 

Computer simulations of molecular systems are routinely used to study molecular processes. The resolution of such computer models is generally only bound by computational effort. However, while quantum mechanics provides the most fundamental discription of matter, ab initio molecular dynamics simulations quickly reach their limits. As a remedy, a coarser description of matter can be used to push the limits of accessible length and time-scales. In a first step, the resolution of molecular systems can be reduced to the level of single atoms. Such all-atom (AA) models are routinely implemented by molecular dynamics (MD) simulations that numerically integrate Newton's equation of motion. The interaction potentials for the atoms are often empirical and aim at correctly modeling structural, thermodynamic and/or dynamic properties of a target system \cite{mackerell2004empirical}. However, the exploration of many relevant molecular processes, such as protein folding or binding, require access to length- and time-scales that are still out of reach for AA models \cite{plattner2015protein, paul2017protein}. Therefore, the resolution is frequently further reduced by averaging over atomistic degrees of freedom. Such coarse-grained (CG) models represent chemical compounds as particles in a similar fashion to AA MD simulations. In general, coarse-graining reduces the computational effort of the simulation and enables larger integration time steps \cite{marrink2010comment, nielsen2003coarse}. In addition, averaging over degrees of freedom yields "softer" interactions between coarse-grained sites and therefore, dynamics of the CG system are accelerated and a faster exploration of configuration space is possible. 

The exploration of some phenoma require to consider wide range of length- and time-scales, because molecular processes on multiple scales can be linked and interwoven. This is especially true for soft-matter systems, where local interactions can impact large scale conformational changes. Therefore, a single model is sometimes not sufficient to capture the interplay of processes that are potentially linked to various different scales. As a solution, multiscale modeling (MM) can be applied, where models of different resolution are combined to address phenomena at multiple scales \cite{ayton2007multiscale, voth2008coarse, peter2010multiscale}: While coarse-graining is deployed to study the large scale behavior of the system, higher resolution models are used to explore the behavior at local scales. 

This thesis focuses on an important aspect of MM that is referred to as reverse-mapping: To establish a tight and consistent link between models of different resolutions, an approach to reintroduce details based on a lower resolution representation is required. Reverse-mapping is routinely used in the MM community to analyse simulation results on a local scale \cite{brocos2012multiscale, pandey2014multiscale, deshmukh2016water, pezeshkian2020backmapping}, or for a direct comparison with experimental data, for example obtained with spectroscopic methods \cite{hess2006long}. Moreover, reverse-mapping is applied to obtain a starting point for further high-resolution simulations \cite{shimizu2018reconstruction, pandey2014multiscale}, or to asses the stability and accuracy of the obtained CG structures \cite{shimizu2018reconstruction}.

A reverse-mapping scheme has to generate new degrees of freedom and thereby taking all their dependencies into account. In particular, generated microstates should be consistent with the given CG representation and should agree with the Boltzmann distribution at a particular state point. Most existing reverse-mapping schemes generate an initial atomistic structure that fulfills the consistency condition. Generic approaches place atoms close to their corresponding CG site, either randomly or based on some geometric rules \cite{rzepiela2010reconstruction, wassenaar2014going}. Fragment-based schemes rely on a presampled library of atomistic fragments that are projected onto the CG conformation \cite{peter2009multiscale, zhang2019hierarchical, hess2006long, brasiello2012multiscale}. In either case, further energy minimization to resolve steric clashes and intra-molecular tension is required. Subsequently, the relaxed structure has to be equilibrated in a MD simulation to recover the correct statistical weights for the reinserted degrees of freedom. The computational effort for the subsequent energy minimization and equilibration procedures of such reverse-mapping schemes can become significant. As such, applications to large systems or high-throughput simulations are still limited. In addition, poorly initialized structures can get trapped into local minima with high energy barriers. Therefore, human intervention is frequently required for the reverse-mapping of more complex molecular structures and hence, hinder the automation of such processes. 

\section{Machine Learning}

In this work, machine learning (ML) is applied to improve the state-of-the-art in reverse-mapping of molecular structures. In the past decades, ML has emerged as a prominent research field that has an transformative impact on many domains, such as computer vision \cite{voulodimos2018deep}, speech recognition \cite{nassif2019speech} or medical image analysis \cite{fatima2017survey}. In its core, ML algorithms construct statistical models from data without relying on explicit program instructions. As such, the recent success of ML models is further fueled by the availability of large data sets. Recently, ML is gaining significant attention in many fields of modern science as well, especially particle physics and computational chemistry \cite{noe2020machine, vamathevan2019applications, radovic2018machine}.

Within the field of ML, deep neural networks (DNNs) have received considerable attention. In particular, DNNs have dramatically improved the state-of-the-art in computer vision \cite{voulodimos2018deep}. For example, deep generative models based are able to synthesize photorealistic images of complex objects, such as human faces or animals \cite{zhang2017stackgan, karras2017progressive, brock2018large}. In its core, DNNs are computational models that are based on a multiscale approach: Multiple layers are arranged subsequently and each layer transforms its input into a more abstract and composite representation, i.e. a DNN learn representations of data with multiple levels of abstraction. It is shown emprically that such deep learning approaches are successfull in discovering complex structure in large data sets.

A milestone in the development of DNNs are convolutional neural networks (CNNs). Each layer of a CNN consists of a bank of convolutional kernels, also called filter, that slide over the input of the layer. This procedure is applied to obtain a translation-equivariant response for the applied filters. Unlike traditional approaches that deploy hand-engeenered filters, CNNS use paramterized filters, i.e. relevant pattern are learned from the data. This allows CNNs to learn a suitabel represetnation of the data for a given task without relying on handcrafted features. In addition, the CNN approach benefits from weight sharing, i.e. learned filters are transferred across the whole input, which reduces number of required parameters dramatically. 

DNNs are routinely used for generative modeling, i.e. to provide an estimate for the probability $p_{\Theta}(\mathbf{x})$ of an observation $\mathbf{x}$, where $\Theta$ are the parameters of the model. The general goal is to approximate the true distribution $\mathcal{X}$, i.e. find the optimal parameters $\Theta^{*}$ such that $p_{\Theta^{*}}(\mathbf{x}) \approx p_{\mathcal{X}}(\mathbf{x})$. The major route to train a generative model is to maximize the data \textit{likelihood}. However, directly assessing the data likelihood is typically based on approximations or computational models that provide a tractable functional form for the likelihood, which might limit the expresivity of the model.

Implicit generative models do not require direct access of the likelihood function but define a stochastic procedure to generate new samples. Generative adversarial networks (GANs) have become one of the most successful implicit generative models known in the ML community \cite{creswell2018generative, gui2021review}. In its core, a GAN consists of two competing models trained in a game: A generator $g_{\Theta}$ maps samples $\mathbf{z} \in \mathbb{R}^d$ from a latent distribution $\mathcal{Z}$ into the ambient space $\mathbb{R}^D$. A second model, the discriminator $c_{\Psi}$, has to distinguish between synthetic samples $g_{\Theta}(\mathbf{z})$ from the generator and real samples $\mathbf{x}$ from the training set $\mathcal{D} = \{\mathbf{x}\}$, where $\mathbf{x}$ are drawn from $\mathcal{X}$. As such, the discriminator $c_{\Psi}$ acts as a distance measure in ambient space for the target distribution $\mathcal{X}$ and the distribution of synthetic samples $g_{\Theta}(\mathcal{Z})$, which $g_{\Theta}$ is trained to minimize.

\section{Methology of Deepbackmap: Adversarial Backmapping of Condensed-Phase Molecular Structures}

This chapter forms the core of this thesis. The insights gained from the preceding theory chapters lead to the development of deepbackmap (DBM): A new method for the reverse-mapping of molecular structures based on a ML model. In particular, DBM is applicable to condensed-phase molecular systems. Unlike other reverse-mapping schemes, DBM aims at directly predicting equilibrated molecular structures that resemble the Boltzmann distribution. As such, the method does not rely on further energy minimization for relaxation and MD simulations for equilibration of the fine-grained structures.  Moreover, DBM requires little human intervention, since the reinsertion of local details is learned from training examples.

DBM is trained with the generative adversarial approach. In particular, pairs of corresponding CG and fine-grained molecular structures are used for the training. While the fine-grained configurations serves as the target, the CG structure is treated as a conditional variable for the generative process: The generator has to generate missing degrees of freedom based on the CG structure. In order to evaluate the performance of the generator, a discriminative network is used to compare the generated structures with the training examples. Specifically, the input for the discriminator consists of both, the CG and the fine-grained configuration. As such, the discriminator evaluates not only the quality of the generated fine-grained structure, but also its consistency with the given CG structure. 

The architecture of both model is based on CNNs. As the CNN architecture requires a regular discretization of 3D space, scaling to larger spatial structures is prohibited. Therefore, the generator is combined with an autoregressive approach that reconstructs the fine-grained structure incrementally, i.e. atom by atom. In each step only local information is used, which makes the method scalable to larger system sizes. To this end, molecular graphs, which are generally undirected and can be cyclic or acyclic, are traversed by the depth-first alogirthm. The algorithm starts by sampling the variables with no parents from a prior distribution and generates subsequent variables based on the atoms generated in the previous steps. However, such forward sampling yields accurate results only if the underlying graph structure has a topological order, i.e. a graph traversal in which each node is visited only after all of its dependencies are explored. As such, accurate sampling of molecular structures calls for more feedback than a simple forward sampling strategy allows. This is especially true for condensed-phase systems, where great care has to be taken to avoid steric clashes. To this end, a variant of Gibbs sampling is applied, which subsequently refines the initial molecular structures.

Unlike data sets commonly used in the ML community, the target distribution for the desired molecular structures is already known up to a normalization constant, i.e. the partition function. This knowledge can be incorporated in the training of DBM to improve its performance and to monitor the training process. Specifically, the potential energy $U$ of generated structures is utilized as an additional term $\mathcal{C}_{\text{pot}}$ in the cost function of the generator. As such, $\mathcal{C}_{\text{pot}}$ acts as a regularizer that effectively narrows down the functional space of the generator by penalizing structures with high potential energy.

\section{Performance and Transferability of DBM: Reverse-mapping of Syndiotactic Polystyrene}

In this chapter, the performance and transferability of the new reverse-mapping method DBM is evlauated. To this end, DBM is applied to a challenging condensed-phase molecular system of syndiotactic polystyrene (sPS). In addition, a baseline method based on geometric rules and energy minimization is applied.

The general ability of DBM to reproduce a reference AA distribution from CG configurations is probed first. To this end, DBM is applied to high-temperature data of the polymeric system. DBM yields well-equilibrated configurations for this particular state point in terms of structural and energetic properties. The baseline method, on the other hand, over-stabilizes the system and therefore does not reproduce the specific state point accurately. 
  
To probe the temperature transferability, training of DBM is fixed to melt configurations obtained at a high temperature and afterwards transferred to crystalline structures at lower temperatures. Again, DBM reproduces structural and energetic distributions of the reference system with remarkable accuracy. A higher-order investigation, facilitated by the SM algorithm, highlights the structural accuracy. The baseline method displays limited transferability to the crystalline phase. In particular, MD simulations starting from backmapped configurations in the crystalline phase get stuck in local minima and would require further human intervention to achieve proper equilibration.

The remarkable transferability of DBM across different state points can be rationalized in terms of a scale-separation: The backmapped structure is composed of the local features learned by DBM and the large-scale features of the CG structure. It is hypothesized that most of the temperature dependence is carried by the CG structure. In particular, local features of the sPS system are assumed to be less temperature sensitive, because associated covalent interactions operate on energy scales significantly larger than $k_B T$. Therefore, local correlations learned in the melt can be transferred toward the crystalline phase.

Finally, the chemical transferability of DBM is probed. To this end, training of DBM is fixed to liquids of octane and cumene. Afterwards, the model is transferred to the more complex sPS system without retraining. The performance of such chemically-transferred models varies in terms of bonded interactions: While the learned local correlations from octane and cumene allow for an accurate reconstruction of phenyl groups, reconstructed polymer backbones display discrepancies compared to the reference system. However, non-bonded features can be reproduced with high accuracy, as indicated by distributions of Lennard-Jones energies and pair correlation functions. In addition, the reference and backmapped ensembles cover similar spots in a two-dimensional projection of configuration space obtained with the SM algorithm. However, relative statistical weights of reference and backmapped microstates display some discrepancies.
  
The overall encouraging performance of chemically-transferred models demonstrates that small-scale features can be shared between different molecules. In other words, DBM is able to interpolate across parts of chemical space because of the deployed local environment descriptions. However, the limits of the generalization are shown as well, which becomes most apparent for the limited quality of the reconstructed carbon backbone. It can be hypothesized that the observed accuracy bottlenecks arise from missing features in the training set. In particular, local environments of backbone carbons connecting monomers are absent in the training examples. Therefore, increasing the number of building blocks should systematically improve the transferability across chemical space. Moreover, force field inconsistencies between sPS and octane/cumene have to be noted that additionally hinder the transferability.
  
Finally, the effect of the different regularization terms applied during training is investigated. The energy-matching term $\mathcal{C}^{(2)}_{\text{pot}}$ has a marginal impact and does not yield a significant improvement compared to training without regularization. On the other hand, applying the energy-minimizing regularization $\mathcal{C}^{(1)}_{\text{pot}}$ has an significant impact. While $\mathcal{C}^{(1)}_{\text{pot}}$ does not affect distributions associated with bonded interactions, it is crucial in order to reproduce Lennard-Jones distributions for chemically-transferred models. However, $\mathcal{C}^{(1)}_{\text{pot}}$ over-stabilizes the system when it is applied to chemically-specific models trained directly on sPS. It can be hypothesized, that $\mathcal{C}^{(1)}_{\text{pot}}$ encourages the model to learn more general aspects that are better transferable across chemistry, such as avoiding too close contacts between non-bonded atoms. On the other hand, the regularization term $\mathcal{C}^{(2)}_{\text{pot}}$ and no regularization presumably encourage the model to focus on more specific features in the training set and therefore, possible force field inconsistencies become even more severe. 

In summary, the ML-based method DBM is able to generate equilibrated AA molecular configurations based on CG structures. It is a well suited tool to automate backmapping processes as it learns the AA reconstruction from training data and therefore requires little human intervention. Moreover, avoiding unnecessary equilibrations upon upscaling will help to establish a tighter and more consistent link between models at different scales. 

The autoregressive reconstruction splits the backmapping task into a sequence of less complex tasks and thereby enables a local environment representation. The locality of DBM is a key feature to achieve remarkable transferability properties, i.e. transferability across thermodynamic state points and across chemical space. As such, DBM offers the perspective to recycle learned local correlations. For example, samples of small molecules can serve as training data for a model that is ultimately deployed on more complex systems. This enables the backmapping of complex molecular structures without necessarily simulating the specific fine-grained system in the first place.

\section{Morphing of Local Statistics: Mapping Through a Resolution Bottleneck}

In this chapter, a ML-based approach to adjust local properties of molecular configurations is introduced. The method aims at improving the quality of structures obtained with chemically-specific top-down models that already capture the correct large-scale behavior of a target system, but differ locally. 

In order to correct local discrepancies, molecular configurations are mapped through a resolution bottleneck. In particular, molecular structures are projected onto a lower resolution, and DBM is used to reinsert degrees of freedom. Importantly, DBM is trained solely on structures of a more faithful target distribution and is afterwards transferred to configurations obtained with the top-down model. Therefore, local details learned from the target distribution are inserted into the top-down structures, which is referred to as morphing of local properties. 

Two different morphing schemes are probed: Scheme A consists of forward sampling and additional Gibbs sampling, while scheme B starts from the original top-down structure and deploys Gibbs sampling only. It is observed that scheme B has a smaller impact on local properties compared to scheme A. This is reasonable, as scheme A has to generate local features from scratch, while scheme B starts from local features obtained with the top-down model, which might hinder the morphing. 

Moreover, the extent to which local features are varied can be controlled by the resolution of the bottleneck. In particular, the number of degrees of freedom in the resolution bottleneck is reduced by a constant factor $s$. For larger values of $s$, the reinsertion of details becomes less restricted by the representation at the resolution bottleneck, which enables larger variations. However, larger values of $s$ lead to a more complex exercise for the morphing model, as the dependencies between particles that the morphing model has to learn also increases with $s$. Therefore, choosing the value for $s$ is a trade-off between the complexity and the impact of the morphing. In this study, small values of $s$ yield superior results than large values in most cases.

The morphing approach is tested on Kuhn scale matched KG sPS melts and liquids of the alkane TCS obtained with the Martini model. The sPS melts obtained with KG model yield similar large-scale characteristics as the higher resolution and solely-structure based model by Fritz \emph{et al.} However, Kuhn scale matching does not take local features below the Kuhn scale into account. As such, local structural distributions of both models differ, which is demonstrated by projecting the melt structures obtained with the Fritz model onto the KG resolution. In particular, the Fritz model yields a broader range of bond lengths compared to the KG model. Furthermore, the Fritz model suppresses small angles and smooths the pair correlation function. Morphing of KG configurations is performed by DBM, which is trained on the Fritz distribution. While morphing has no significant impact on large-scale characteristics, such as the mean square end-to-end distance, it is able to reconstruct local structural features that agree remarkably well with the Fritz distribution. For a higher-order investigation of local properties, FESs are computed in SM coordinates. The analysis of the obtained FESs underpins the aforementioned results.

TCS liquids obtained with the Martini model are compared to AA simulations using the GROMOS-96 force field, which are projected onto the Martini resolution. The GROMOS configurations yield more complex structural distributions compared to the Martini liquids. In particular, bond length and angle distributions obtained for the GROMOS model display multiple peaks and are more compressed than the distributions obtained for the Martini model. Moreover, a analysis of the SM FESs indicate that the occupied region in configuration space is more compact for the GROMOS model compared to the Martini model. To adjust local features, morphing models are trained on the GROMOS distribution and are transferred to the Martini configurations. Unfortunately, none of the morphing models is able to correct local features sufficiently. It can be hypothesized that the discrepancy between the Martini and the GROMOS model are too significant, such that morphing of local properties is not sufficient to match both distributions, i.e. the distributions do not match at the resolution bottleneck. 

This project aims at introducing a two-step backmapping scheme for top-down CG models, where local statistics of the CG structure are corrected before it is served as an input for a backmapping algorithm. To assess the impact of the morphing on the quality of backmapped structures, backmapping of morphed KG and Martini structures is performed with DBM. Specifically, DBM is trained to increase the resolution of KG structures to the level of the original Fritz model and Martini structures to the AA level. Unfortunately, only a minor impact of the morphing on the quality of backmapped structures is observed for both systems. In particular, backmapping of KG structures and CG Fritz structures already yield similar distributions of local structural features without morphing. This can be rationalized by the robust transferability of the backmapping model DBM, which was observed in Chpt. \ref{bm_ps}. It can be hypothesized that strong local interactions, such as covalent bonds, at the higher resolution yield local correlations that separate from larger scales. Therefore, local correlations learned by DBM transfer well across the CG configuration space and backmapping of KG structures yield similar local structural properties compared to backmapping of CG Fritz structures. On the other hand, small differences in the distributions of structural properties are observed between backmapped Martini and CG GROMOS TCS liquids. However, the morphing model for TCS reproduces the complex structural distributions of the GROMOS model only with limited accuracy, such that morphing does not improve the quality of the backmapped liquids significantly. 

\section{Backmapping as a Quality Measure for Coarse-Grained Models}

In this chapter, backmapping is deployed to assess the quality of structure-based CG models at the AA resolution. To this end, CG force fields for TMBT are parameterized using DBI for bonded interactions and IBI for non-bonded interactions. Three different models are parameterized differing in their bonded interactions. It is demonstrated that the CG models reproduce structural properties targeted in the parameterization with remarkable accuracy. Afterwards, test sets are constructed for the backmapping task: (1) An in-distribution test set denotes snapshots obtained in a AA MD simulation that are projected onto the CG resolution. (2) Generalization test sets are constructed consisting of snapshots obtained in MD simulations deploying the CG force fields. While the former is used to assess the baseline accuracy of the backmapping method, a comparison between backmapped in-distribution and generalization test sets yields insights into the quality of the deployed CG models.

Backmapping of CG structures is performed following two different strategies: (1) The machine learning approach DBM and (2) a baseline method that relies on EM are applied. While DBM is able to reproduce AA pair correlation functions for the in-distribution test set with remarkable accuracy, application to the generalization test sets yields AA structures that contain steric clashes. On the other hand, the baseline backmapping method is more robust and maintains its performance for both test sets. However, the baseline method yields pair correlation functions that are overly peaked compared to the atomistic reference due to the relaxation. These findings can be rationalized with respect to two requirement a backmapping scheme has to fulfill: (1) Reconstructed AA details have to be consistent with the underlying CG structure and (2) the backmapped structure has to have high statistical weight. A visual inspection reveals that the generalization test sets contain CG conformations that prohibit reconstructing AA details that fulfill both requirements simultaneously. In particular, DBM generates AA structures that are consistent with the CG structure but consequently display unavoidable steric clashes. The baseline method generates structures with high statistical weight, i.e. no steric clashes are detected, but violates the consistency criteria. More specifically, an analysis of the root mean-square deviations between backmapped structures projected to the CG resolution and the original CG configurations reveal a significant shift upon application of the baseline method, while DBM generates AA structures that are close to the given CG configuration.

A more quantitative measure to identify steric clashes is given by the Jenson-Shannon divergence computed between force distributions. In particular, forces acting on the atoms are computed deploying the AA force field and then projected onto the CG resolution. DBM yields a force distribution for the backmapped in-distribution test set that matches the AA reference distribution remarkably well, while distributions for the generalization test sets display long tails towards large forces. Moreover, the JS divergences allow for a clear ranking for the quality of the different CG models contained in the generalization test set. Force distributions obtained with the baseline backmapping method are not insightful, since the involved energy minimization yields indistinguishable force distributions that are shifted towards small forces.

Future research might focus on new parameterization strategies for CG force fields that incorporate quality measures at the atomistic resolution. Here, an approach is outlined based on the multiscale force-matching strategy that deploys backmapping to evaluate the CG ensemble in terms of the AA force field. In particular, the proposed parameterization scheme aims at suppressing CG configurations that yield large atomistic forces upon backmapping.



\section{Temporal Coherent Backmapping of Molecular Trajectories}

In this chapter, a new ML-based method for temporal coherent backmapping of molecular trajectories is introduced. In particular, a conditional variational autoencoder (cVAE) is trained to non-deterministically reinsert atomistic details conditioned on the current coarse-grained and the previous atomistic frame. 

The approach is applied to two popular biomolecular systems: Alanine dipeptide (ADP) and the miniprotein chignolin (CLN). The performance of the ML model is analyzed regarding its ability to reproduce structural, thermodynamic and dynamic properties of the reference system. To this end, the method is applied to an in-distribution test set that consists of atomistic structures projected onto the coarse-grained resolution. Excellent structural similarity between the reference system and the backmapped in-distribution test set is observed evaluated in terms of potential energy distributions. Moreover, thermodynamic properties match remarkably well, which is tested by analyzing free energy surfaces that are constructed in terms of collective variables. In order to analyze dynamic properties, MSMs are constructed in the space of collective variables to identify slow processes and their associated timescales. The obtained timescales of the backmapped trajectory agree remarkably well with the dynamics observed for the atomistic reference system. Temporal coherence between consecutive frames is evaluated in terms of intra-frame velocity distributions, which are reproduced with excellent accuracy deploying the ML model. The benefit of incorporating the previous state of the system as an additional input is highlighted by a comparison against a frame-based baseline method, which yields velocity distributions that differ significantly from the reference.  

In addition, backmapping of a generalization test set is performed, which consists of trajectories obtained in a coarse-grained simulation based on approximate force fields generated by CGSchNet. While structural and thermodynamic properties of the reference system are reproduced with remarkable accuracy upon backmapping of this generalization data, dynamic properties differ. Timescale ratios of slow transitions between metastable states are recovered for ADP, but deviate from the atomistic reference for CLN. Moreover, intra-frame velocities of the backmapped generalization trajectories are significantly higher compared to the reference. However, a difference of dynamic properties is expected for this test set: Coarse-grained simulations typically display faster dynamics compared to atomistic simulations as a direct consequence of deploying a reduced representation. Averaging over degrees of freedom effectively smoothens the energy landscape allowing for a faster exploration of phase space. However, timescales for transitions between metastable states are typically not rescaled uniformly yielding timescale ratios that differ from the atomistic reference system \cite{fritz2011multiscale}. As such, backmapped trajectories reflect the kinetics of the underlying coarse-grained trajectories.

Moreover, the cVAE approach non-deterministically reinserts atomistic details along the coarse-grained variables. This feature is highlighted by a visual inspection of backmapped structures sampled from a fixed coarse-grained structure but differing latent samples. This procedure yields an ensemble of atomistic microstates that are all consistent with the given coarse-grained structure but still display variations.

In summary, the proposed method is able to backmap coarse-grained trajectories such that (1) each reconstructed frame has a high statistical weight, (2) each frame is a valid reconstruction of the given coarse-grained structure, i.e. atomistic degrees of freedom are reinserted along the coarse-grained variables and (3) consecutive frames are temporally coherent, i.e. shifts in atomic positions between consecutive frames follow the same distribution as the atomistic reference system. As such, the proposed method offers the ability to analyze the dynamics of a coarse-grained simulation at atomistic resolution.

Future work might focus on different strategies to improve the training protocol of the approach: (1) In order to improve sampling of configuration space, training samples of sparsely populated regions in configuration space can be emphasized more. This could be realized by accompanying training samples with thermodynamic or dynamical path weights. (2) An autoregressive training protocol could be applied to improve the temporal coherence. In particular, a recurrent neural network approach could add information of multiple consecutive frames to the gradients used during backpropagation. (3) To further encourage the ML model to utilize knowledge of previous states for its predictions, the training loss could be augmented with a reconstruction error of properties that are explicitly based on such information, for example intra-frame velocities. 









%Generative models are further distinguished between those that access the model probability distribution $p_{\Theta}(\mathbf{x})$ \textit{explicitly} through some functional form and those that define it \textit{implicitly} through a sampler. The former approach has the benefit that maximizing the likelihood $\mathcal{L}$ is straightforward, as the probability distribution can be accessed directly. However, explicit models require to assume some functional form for the probability $p_{\Theta}(\mathbf{x})$, which often becomes a bottleneck for the expressive power of the model. Therefore, the design of an explicit model is often a trade-off between the complexity of the true data distribution and tractability. On the other hand, implicit models do not require direct access of the likelihood function but define a stochastic procedure to generate new samples. 

%The various variants of explicit DGMs fall into two categories: Models that carefully construct a tractable functional form for the likelihood $\mathcal{L}$ and models that use a tractable approximation.\cite{goodfellow2020generative} Two major examples for tractable explicit models are autoregressive models and normalizing flow models. Examples for explicit models that require approximation are the variational auto encoder and the Boltzmann machine. The most prominent member of implicit generative models is the generative adversarial approach.


%Despite being a complex, non-convex optimization problem, simple methods such as
%stochastic gradient descent (SGD) are able to recover good solutions that minimize the training error.
%More surprisingly, the networks learned this way exhibit good generalization behavior, even when
%the number of parameters is significantly larger than the amount of training data

%The capabilities of deep learning models are often examined under notions of expressivity and capacity: their ability to learn a function of some complexity from a given set of examples. It has been shown that deep learning models are capable of high expressivity, and are hence able to learn any function under certain architectural constraints. However, classical measures of machine learning model expressivity (such as Vapnikâ€“Chervonenkis (VC) dimension [4], Rademacher complexity [5], etc.), which successfully characterize the behavior of many machine learning algorithms, fail to explain the generalization abilities of DNNs. Since DNNs are typically over-parameterized models with substantially less training data than model parameters, they are expected to overfit the training data and obtain poor generalization as a consequence [6]. However, this is not the case in practice. Thus, a specific line of work has been dedicated to study the generalization of these networks.

%Recently, DNNs have been deployed in a conditional framework as well \cite{isola2017image}. In particular, labels or cartoons of objects have been used as a conditional input for the network in order to generate a corresponding high-resolution image. Is it therefore possible to take advantage of DNNs for the reverse-mapping of molecular structures?

%Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.


%Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.

