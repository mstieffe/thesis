% Chapter Template

\chapter{Conclusion and Outlook} 
\label{conclusion} 

To conclude this thesis, the main results of each chapter are summarized to highlight the importance of the results presented and the conclusions drawn. The first two sections recapitulate the underlying theory and refresh the motivation for this project. Afterwards, summaries of each of the subsequent chapters are presented, which restate the discussion sections of each chapter. Finally, an outlook for future research related to this thesis is given.

\section{Multiscale Modeling}

%Understanding molecular processes is central to a wide range of modern research areas. The motions of microscopic particles can be described by quantum-mechanics or Newton's equation of motion. However, analytical solutions for most molecular systems are intractable, because of the enormous number of microscopic degrees of freedom. Therefore, 

Computer simulations of molecular systems are routinely used to study molecular processes. The resolution of such computer models is generally only bound by computational effort. However, while quantum mechanics provides the most fundamental description of matter, ab initio molecular simulations quickly reach their limits. As a remedy, a coarser description of matter can be used to push the limits of accessible length- and timescales. In a first step, the resolution of molecular systems can be reduced to the level of single atoms. Such all-atom (AA) models are routinely implemented by molecular dynamics (MD) simulations that numerically integrate Newton's equation of motion. The interaction potentials for the atoms are often empirical and aim at correctly modeling structural, thermodynamic and/or dynamic properties of a target system \cite{mackerell2004empirical}. However, the exploration of many relevant molecular processes, such as protein folding or binding, requires access to length- and timescales that are still out of reach for AA models \cite{plattner2015protein, paul2017protein}. Therefore, the resolution is further reduced by averaging over atomistic degrees of freedom. Such coarse-grained (CG) models represent chemical compounds as particles in a similar fashion to AA MD simulations. In general, coarse-graining reduces the computational effort of the simulation and enables larger integration time steps \cite{marrink2010comment, nielsen2003coarse}. In addition, averaging over degrees of freedom yields "softer" interactions between coarse-grained sites and therefore, dynamics of the CG system are accelerated and a faster exploration of configuration space is possible. 

The exploration of some phenomena require to consider wide range of length- and timescales, because molecular processes on multiple scales can be linked and interwoven. This is especially true for soft matter systems, where local interactions can impact large scale conformational changes. Therefore, a single model is sometimes not sufficient to capture the interplay of processes that are potentially linked to various different scales. As a solution, multiscale modeling (MM) can be applied, where models of different resolution are combined to address phenomena at multiple scales \cite{ayton2007multiscale, voth2008coarse, peter2010multiscale}: While coarse-graining is deployed to study the large-scale behavior of the system, higher resolution models are used to explore the behavior at local scales. 

This thesis focuses on an important aspect of MM that is referred to as reverse-mapping: To establish a tight and consistent link between models of different resolutions, an approach to reintroduce details based on a lower resolution representation is required. Reverse-mapping is routinely used in the MM community to analyze simulation results on a local scale \cite{brocos2012multiscale, pandey2014multiscale, deshmukh2016water, pezeshkian2020backmapping}, or for a direct comparison with experimental data, for example obtained with spectroscopic methods \cite{hess2006long}. Moreover, reverse-mapping is applied to obtain a starting point for further high-resolution simulations \cite{shimizu2018reconstruction, pandey2014multiscale}, or to asses the stability and accuracy of the obtained CG structures \cite{shimizu2018reconstruction}.

A reverse-mapping scheme has to generate new degrees of freedom and thereby take all their dependencies into account. In particular, generated microstates should be consistent with the given CG representation and should agree with the Boltzmann distribution at a particular state point. Most existing reverse-mapping schemes generate an initial atomistic structure that requires subsequent energy minimization for relaxation. In addition, MD simulations are typically performed to recover the correct statistical weights for the reinserted degrees of freedom. The computational effort for the subsequent energy minimization and equilibration procedures of such reverse-mapping schemes can become significant. As such, applications to large systems or high-throughput simulations are still limited. In addition, poorly initialized structures can get trapped into local minima with high energy barriers. Therefore, human intervention is frequently required that hinders the automation of such processes. 

\section{Machine Learning}

In this work, machine learning (ML) is applied to improve the state-of-the-art in reverse-mapping of molecular structures. In the past decades, ML has emerged as a prominent research field that has a transformative impact on many domains, such as computer vision \cite{voulodimos2018deep}, speech recognition \cite{nassif2019speech} or medical image analysis \cite{fatima2017survey}. At its core, ML algorithms construct statistical models from data without relying on explicit program instructions. As such, the recent success of ML models is further fueled by the availability of large data sets. Recently, ML is gaining significant attention in many fields of modern science as well, especially particle physics and computational chemistry \cite{noe2020machine, vamathevan2019applications, radovic2018machine}.

Within the field of ML, deep neural networks (DNNs) have received considerable attention. In particular, DNNs have dramatically improved the state-of-the-art in computer vision \cite{voulodimos2018deep}. For example, deep generative models are able to synthesize photorealistic images of complex objects, such as human faces or animals \cite{zhang2017stackgan, karras2017progressive, brock2018large}. At its core, DNNs are computational models that are based on a multiscale approach: Multiple layers are arranged subsequently and each layer transforms its input into a more abstract and composite representation, i.e. DNNs learn representations of data with multiple levels of abstraction. It is shown empirically that such deep learning approaches are successful in discovering complex structures in large data sets.

A milestone in the development of DNNs are convolutional neural networks (CNNs). Each layer of a CNN consists of a bank of convolutional kernels, also called filter, that slide over the input of the layer. This procedure yields a translation-equivariant response for the applied filters. Unlike traditional approaches, CNNs use parameterized filters, i.e. relevant pattern are learned from the data. This allows CNNs to learn a suitable representation of the data for a given task without relying on handcrafted features. In addition, the CNN approach benefits from weight sharing, i.e. learned filters are transferred across the whole input, which reduces the number of required parameters dramatically. 

DNNs are routinely used for generative modeling, i.e. to provide an estimate for the probability $p_{\Theta}(\mathbf{x})$ of an observation $\mathbf{x}$, where $\Theta$ are the parameters of the model. The general goal is to approximate a target distribution $\mathcal{X}$, i.e. to find the optimal parameters $\Theta^{*}$ such that $p_{\Theta^{*}}(\mathbf{x}) \approx p_{\mathcal{X}}(\mathbf{x})$. The major route to train a generative model is to maximize the data likelihood. However, directly assessing the data likelihood is typically based on approximations or computational models that provide a tractable functional form for the likelihood, which might limit the expressivity of the model.

Implicit generative models do not require direct access of the likelihood function but define a stochastic procedure to generate new samples. Generative adversarial networks (GANs) have become one of the most successful implicit generative models known in the ML community \cite{creswell2018generative, gui2021review}. At its core, a GAN consists of two competing models trained in a game: A generator $g_{\Theta}$ produces synthetic samples by transforming samples $\mathbf{z}$ from a prior distribution. A second model, the discriminator $c_{\Psi}$ with parameters $\Psi$, has to distinguish between synthetic samples $g_{\Theta}(\mathbf{z})$ from the generator and real samples $\mathbf{x}$ from the training set $\mathcal{D} = \{\mathbf{x}\}$, where $\mathbf{x}$ are drawn from the target distribution $\mathcal{X}$. As such, the discriminator $c_{\Psi}$ acts as a distance measure for the target distribution $\mathcal{X}$ and the distribution of synthetic samples $g_{\Theta}(\mathcal{Z})$. This distance measure serves as a training objective for the generator $g_{\Theta}$, i.e. $g_{\Theta}$ is trained to minimize this distance.

\section{Methodology of Deepbackmap: Adversarial Reverse-mapping of Condensed-phase Molecular Structures}

Chapter 4 forms the core of this thesis. The insights gained from the preceding theory chapters lead to the development of deepbackmap (DBM): A new method for the reverse-mapping of molecular structures based on ML. A key feature of DBM is its applicability to condensed-phase molecular systems. Unlike other reverse-mapping schemes, DBM aims at directly predicting equilibrated molecular structures that resemble the Boltzmann distribution. As such, the method does not rely on further energy minimization for relaxation and MD simulations for equilibration of the fine-grained structures. Moreover, DBM requires little human intervention, since the reinsertion of local details is learned from training examples.

DBM is trained with the generative adversarial approach. In particular, pairs of corresponding CG and fine-grained molecular structures are used for the training. While the fine-grained configurations serve as the target distribution, the CG structures are treated as conditional variables for the generative process: The generator has to generate missing degrees of freedom based on the CG structure. In order to evaluate the performance of the generator, a discriminative network is used to compare the generated structures with the training examples. Specifically, the input for the discriminator consists of both, the CG and the fine-grained configuration. As such, the discriminator evaluates not only the quality of the generated fine-grained structure, but also its consistency with the given CG structure. 

The architecture of both models is based on CNNs. As the CNN architecture requires a regular discretization of 3D space, scaling to larger spatial structures is limited. Therefore, the generator is combined with an autoregressive approach that reconstructs the fine-grained structure incrementally, i.e. atom by atom. In addition, it is assumed that the placement of one atom relies only on short-range force field related features. In particular, DBM only learns local correlations while large-scale features are adapted from the CG structure. Therefore, only local information is required in each step, which makes the method scalable to larger system sizes. Moreover, it can be hypothesized that such local environments strongly overlap across different state points and across chemical space. As such, the local environment approach is a key feature for the generalizability of DBM. 

Molecular graphs are generally undirected and can be cyclic or acyclic. For the graph traversal, which defines the order of reconstruction, the depth-first algorithm is used. In particular, the algorithm starts by sampling the variables with no parents from a prior distribution and generates subsequent variables based on the atoms generated in the previous steps. However, such forward sampling only yields accurate results if the underlying graph structure has a topological order, i.e. a graph traversal in which each node is visited only after all of its dependencies are explored. As such, accurate sampling of molecular structures calls for more feedback than a simple forward sampling strategy allows. This is especially true for condensed-phase systems, where great care has to be taken to avoid steric clashes. To this end, a variant of Gibbs sampling is applied, which subsequently refines the initial molecular structures by iteratively resampling the atom positions. Each further iteration still updates one atom at a time, but uses the knowledge of all other atoms.

Given the potential energy function of the system, the target distribution for the desired molecular structures is already known up to a normalization constant, i.e. the partition function. This knowledge can be incorporated in the training of DBM to improve its performance and to monitor the training process. Specifically, the potential energy $U$ of generated structures is utilized as an additional term $\mathcal{C}_{\text{pot}}$ in the cost function of the generator. As such, $\mathcal{C}_{\text{pot}}$ acts as a regularizer that effectively narrows down the functional space of the generator by penalizing structures with high potential energy.

\section{Performance and Transferability of DBM: Reverse-mapping of Syndiotactic Polystyrene}

In chapter 5, the performance and transferability of the new reverse-mapping method DBM is evaluated. To this end, DBM is applied to a challenging condensed-phase molecular system of syndiotactic polystyrene (sPS). CG representations are obtained by a projection of AA data onto the CG resolution, where each sPS monomer is represented by two beads. In addition to DBM, a baseline backmapping method based on geometric rules and energy minimization (EM) is applied.

The general ability of DBM to reproduce a reference AA distribution from CG configurations is probed first. To this end, DBM is applied to high-temperature data of the polymeric system. DBM yields well-equilibrated configurations for this particular state point in terms of structural and energetic properties. The baseline method, on the other hand, over-stabilizes the system and therefore does not reproduce the specific state point accurately. 
  
To probe the temperature transferability, training of DBM is fixed to melt configurations obtained at a high temperature and afterwards transferred to crystalline structures at lower temperatures. Again, DBM reproduces structural and energetic distributions of the reference system with remarkable accuracy. A higher-order investigation, facilitated by the Sketch-map (SM) algorithm, highlights the structural accuracy. The transferability of the baseline method to the crystalline phase is limited. In particular, MD simulations starting from backmapped configurations of the baseline method in the crystalline phase get stuck in local minima. Therefore, further human intervention would be required to achieve proper equilibration.

Finally, the chemical transferability of DBM is probed. To this end, training of DBM is fixed to liquids of octane and cumene. Afterwards, the model is transferred to the more complex sPS system without retraining. The performance of such chemically-transferred models varies in terms of bonded interactions: While the learned local correlations from octane and cumene allow for an accurate reconstruction of phenyl groups, reconstructed polymer backbones display discrepancies compared to the reference system. On the other hand, distributions of Lennard-Jones energies and pair correlation functions indicate that non-bonded features are reproduced with high accuracy. In addition, the SM algorithm is used to obtain a two-dimensional projection of the configuration space. It is observed that the reference and backmapped ensembles cover similar areas in this projected configuration space, but relative statistical weights of reference and backmapped microstates display discrepancies.
    
In summary, the ML-based method DBM is able to generate equilibrated AA molecular configurations based on CG structures. It is a well suited tool to automate backmapping processes as it learns the AA reconstruction from training data and therefore requires little human intervention. Moreover, avoiding unnecessary equilibrations of reverse-mapped structures will help to establish a tighter and more consistent link between models at different scales. In addition, DBM displays remarkable transferability features that can be linked to the applied local environment approach. In particular, the transferability across different state points can be rationalized in terms of a scale-separation: The backmapped structure is composed of the local correlations learned by DBM and the large-scale properties of the CG structure. It is hypothesized that most of the temperature dependence is carried by the CG structure. Local features on the other hand are assumed to be less temperature sensitive, because associated covalent interactions operate on energy scales significantly larger than $k_B T$. As such, local correlations separate from larger scales and therefore, can be transferred from the melt to the crystalline phase. Beside the transferability across state points, the advantages of the local environment approach of DBM are further highlighted by the encouraging performance of chemically-transferred models. In particular, it is demonstrated that small-scale features can be shared between different molecules, which allows DBM to interpolate across parts of chemical space. However, the limits of the generalization are shown as well by the limited quality of the reconstructed carbon backbone. It can be hypothesized that a bottleneck for the accuracy arises from missing features in the training set. Specifically, local environments of backbone carbons connecting monomers are not included in the training examples. As such, it can be expected that an increasing number of building blocks systematically improves the transferability across chemical space. In conclusion, DBM offers the perspective to recycle learned local correlations across different state points and across chemical space. This can be useful for future applications in MM, as DBM can be trained on data that is straightforward to obtain, such as liquids of small molecules or polymer melts with a small system size, but can be transferred to more challenging tasks afterwards, for example to study polymer crystallization or large systems of complex molecules. 
%For example, samples of small molecules can serve as training data for a model that is ultimately deployed on more complex systems. This enables the backmapping of complex molecular structures without necessarily simulating the specific fine-grained system in the first place.
 
\section{Backmapping as a Quality Measure for Coarse-grained Models}

In chapter 6, backmapping is applied to assess the quality of structure-based CG models at the AA resolution. In particular, three different models for Tris-Meta-Biphenyl-Triazine (TMBT) are parameterized that differ in their bonded interactions. While all CG models reproduce structural properties at the CG resolution targeted in the parameterization with remarkable accuracy, important cross-correlations between CG variables are not captured sufficiently. This is demonstrated by reverse-mapping of CG structures from two different sources: (1) An in-distribution test set denotes AA MD simulation data that is projected onto the CG resolution. One part of this data set is used to train DBM, while the other part is used to obtain the baseline accuracy of the backmapping method. (2) Snapshots obtained by MD simulations based on the CG force fields are denoted as generalization test sets. To assess the quality of the deployed CG models, backmapped in-distribution and backmapped generalization test sets are compared revealing significant discrepancies between the AA and CG ensembles. 

DBM is able to reproduce AA pair correlation functions for the in-distribution test set with remarkable accuracy. However, application to the generalization test sets yields AA structures that contain steric clashes, i.e. non-bonded atoms that are too close to each other. To rationalize the deterioration in performance for the generalization test sets, consider the following requirements that a backmapping scheme has to fulfill: (1) Reconstructed AA details have to be consistent with the underlying CG structure and (2) backmapped structures have to agree with the Boltzmann distribution. It is demonstrated that the generalization test sets contain CG conformations that prohibit reconstructing AA details that fulfill both requirements simultaneously. In particular, DBM generates AA structures that are consistent with the CG structure but therefore contain unavoidable steric clashes. To underpin the results, a second method that relies on EM is applied for the backmapping. The EM-based method is more robust and displays a similar performance for both test sets. However, the EM-based method yields pair correlation functions that are overly peaked compared to the AA reference, which is reasonable because of the involved relaxation. More importantly, it is observed that the EM-based method generates structures with low potential energy but violates the consistency criteria, i.e. backmapped structures are shifted away from the underlying CG structure. 

The reintroduced details enable force computations based on the AA force field. However, the coarse-to-fine mapping is not unique, as a single CG structure corresponds to an ensemble of AA microstates. Therefore, computed AA forces are projected onto the CG resolution to enable a more stringent comparison. The force distribution for the backmapped in-distribution test set obtained with DBM matches the AA reference distribution remarkably well. On the other hand, force distributions obtained for the generalization test sets display long tails towards large forces. Computing the Jenson-Shannon (JS) divergence between force distributions of reference AA and backmapped configurations yields a clear ranking for the quality of the different CG models. On the other hand, force distributions obtained with the EM-based backmapping method are not insightful, since the involved relaxation yields indistinguishable force distributions that are shifted towards significantly smaller forces.

Assessing the quality of CG models at the AA resolution can be beneficial for the development of new CG force field parameterization strategies. For example, force distributions could be used to evaluate the CG ensemble in terms of the AA force field, such that CG configurations that yield large AA forces can be suppressed. An evident starting point for this strategy is the multiscale force-matching approach, where the parameters of the CG force field are tuned such that the CG potential approximates the average net AA forces. Note that the force-matching functional is evaluated in the AA ensemble, i.e. it only contains structural information regarding cross-correlations observed in the AA model. However, the CG model is in general not guaranteed to reproduce cross-correlations between different degrees of freedom perfectly. A force evaluation in terms of the CG ensemble can reveal inconsistencies of the cross-correlations and has therefore the potential to improve the force-matching strategy.
 
\section{Morphing of Local Statistics: Mapping Through a Resolution Bottleneck}

In chapter 7, DBM is applied to adjust local, structural properties of molecular configurations. The method aims at improving the quality of structures obtained with chemically-specific top-down models that capture the correct large-scale behavior of a target system, but yield less faithful representations on a local scale. In order to correct local discrepancies, molecular structures are projected onto a lower resolution, i.e. a resolution bottleneck, and DBM is used to reinsert degrees of freedom. Importantly, DBM is trained solely on structures of the target distribution. Afterwards, DBM is transferred to configurations obtained with the top-down model. As such, local details learned from the target distribution are inserted into the top-down structures, which is referred to as morphing of local properties. 

%The impact of the morphing can be controlled by the resolution of the bottleneck. In particular, the number of degrees of freedom in the resolution bottleneck is reduced by a factor $s$. The reinsertion of details becomes less restricted by the representation at the resolution bottleneck for larger values of $s$, i.e. larger variations are enabled. However, larger values of $s$ also increase the number of degrees of freedom that DBM has to reinsert. Therefore, choosing the value for $s$ is a trade-off between the complexity and the impact of the morphing.

The morphing approach is tested for Kuhn scale matched Kremer-Grest (KG) sPS melts. The sPS melts obtained with the KG model display similar large-scale properties, such as the mean square end-to-end distance, as the higher resolution and solely structure-based model by Fritz \emph{et al.} However, Kuhn scale matching does not account for local properties below the Kuhn scale. As such, local structural distributions of both models differ, which is demonstrated by a projection of melt structures obtained with the Fritz model onto the resolution of the KG model. The impact of the morphing is evaluated in terms of structural distributions, pair correlation functions and free energy surfaces computed in SM coordinates. While morphing has no significant impact on large-scale characteristics, it is able to reconstruct local properties of the Fritz distribution with remarkable accuracy.

In addition, morphing of tetracosane (TCS) liquids obtained with the Martini model is performed. As a target system, AA simulations with the GROMOS-96 force field are used, which are projected onto the Martini resolution. While the morphing model is not able to correct local features sufficiently, the significant discrepancies between local properties of the Martini and the GROMOS configurations have to be emphasized. As such, it can be hypothesized that the distributions do not match at the resolution bottleneck. 

The general goal of this project is to introduce a two-step backmapping scheme for top-down CG models. In a first step, local statistics of a CG structure are corrected before it is processed by a backmapping algorithm in a second step. To investigate the impact of the morphing on the quality of backmapped structures, DBM is trained to increase the resolution of sPS melts to the level of the original Fritz model and TCS liquids to the AA level. Only a minor impact of the morphing on the quality of backmapped structures is observed for both systems. In particular, backmapping of KG structures and CG Fritz structures already yield similar distributions of local structural features without morphing. This can be rationalized by the robust transferability of DBM: It can be hypothesized that strong local interactions at the higher resolution yield local correlations that separate from larger scales. Therefore, local correlations learned by DBM transfer well across the CG configuration space. As such, backmapping of KG structures yield similar local structural properties compared to backmapping of CG Fritz structures. On the other hand, small differences in the distributions of structural properties are observed between backmapped Martini and backmapped CG GROMOS TCS liquids. However, the morphing of TCS liquids has a limited accuracy, such that it is not able yo improve the quality of the backmapped liquids significantly. Future work to improve the morphing capability of the model can focus on a hierarchical approach, where local features are successively adjusted on multiple scales.

%Local properties of the Martini and the GROMOS configurations differ significantly. The morphing model is not able to correct local features sufficiently.

\section{Temporal Coherent Backmapping of Molecular Trajectories}

In chapter 8, a new ML-based method for temporal coherent backmapping of molecular trajectories is introduced. In particular, temporal coherent backmapping refers to reproducing shifts of atomic positions between consecutive frames that are comparable to the AA reference system. The proposed method aims at both, generating well-equilibrated molecular structures for each individual frame, while maintaining temporal coherence within a series of frames. To this end, a variational autoencoder (VAE) is trained to reinsert atomistic details conditioned on the current CG \textit{and} the previous AA frame. The approach is applied to two popular biomolecular systems: Alanine dipeptide (ADP) and the miniprotein chignolin (CLN). 

The baseline accuracy of the ML model is evaluated regarding its ability to reproduce structural and dynamic properties of reference AA trajectories. To this end, the method is applied to AA trajectories projected onto the CG resolution, which are referred to as in-distribution test set. Excellent structural similarity between the reference system and the backmapped in-distribution test set is observed in terms of potential energy distributions and free energy surfaces computed with respect to collective variables. In order to analyze the dynamics of backmapped trajectories, MSMs are constructed to identify slow processes and their associated timescales. The implied timescales of the backmapped trajectories agree remarkably well with the dynamics observed for the AA reference system. To evaluate temporal coherence between consecutive frames, intra-frame velocity distributions are computed. The ML model is able to reproduce intra-frame velocities with excellent accuracy. The advantage of incorporating the previous AA simulation frame is highlighted by a comparison with a frame-based backmapping scheme, which yields velocity distributions that differ significantly from the reference.

In addition, the method is applied to analyze the dynamics of trajectories obtained in a CG simulation. To this end, CG simulations are performed based on approximate force fields generated by CGSchNet, which is a ML based method for CG force field parameterization. While structural properties of the reference AA system are reproduced with remarkable accuracy upon backmapping of the CgSchNet trajectories, dynamic properties differ. Timescale ratios of slow transitions between metastable states are recovered for ADP, but deviate from the AA reference for CLN. Moreover, intra-frame velocities of the backmapped CGSchNet trajectories are significantly larger compared to the AA reference. These findings are reasonable for CG simulation data, since averaging over degrees of freedom effectively smoothens the energy landscape and therefore enables a faster exploration of phase space. As such, CG simulations display faster dynamics compared to AA simulations. Moreover, timescales for transitions between metastable states are typically not rescaled uniformly yielding timescale ratios that differ from the AA reference system \cite{fritz2011multiscale}. 

In summary, the proposed method is able of reverse-mapping CG trajectories such that (1) each reconstructed frame has a high statistical weight, (2) each frame is a valid reconstruction of the given CG structure, i.e. atomistic degrees of freedom are reinserted along the CG variables and (3) consecutive frames are temporally coherent. As such, a tool to analyze the dynamics of a CG simulation at AA resolution is proposed, which is of relevance for a thorough analysis of dynamic properties that require atomistic details, such as the dynamic structure factor \cite{chen2008comparison, arbe2012neutron}.

%Multiple routes towards improving the proposed method are possible: (1) Examples of sparsely populated regions in configuration space could be emphasized during training to improve sampling of configuration space. This could be realized by accompanying training samples with thermodynamic or dynamical path weights. (2) To improve the temporal coherence, an autoregressive training protocol could be applied. To this end, a recurrent neural network approach could be applied to add information of multiple consecutive frames to the gradients used during backpropagation. (3) To further encourage the ML model to incorporate knowledge of previous states, the training loss could be augmented with a reconstruction error of properties that are directly linked to such information, for example intra-frame velocities. 

\section{Outlook}

Future work pertaining the integration of generative ML approaches into the MM framework can proceed along multiple avenues of research. A promising route is to apply DBM to hierarchical modeling, where a particular system is described by a nested sequence of CG models \cite{fritz2009hierarchical, zhang2015communication, zhang2019hierarchical}. Starting from the lowest resolution, CG structures are successively backmapped to higher resolutions until the AA level is reached. While hierarchical modeling is used routinely, it requires a significant amount of human effort, as force fields for each level of resolution have to be parameterized. Therefore, DBM could be a great advantage for hierarchical modeling, as the ML-based model learns the reverse-mapping from training data and does not necessarily require force field parameterizations. As such, it can be used to automate the process. 

Another direction for future research is to focus on the transferability of DBM, which is explored in Chpt. \ref{bm_ps}. Further insights into the limits of chemical transferability can be gained by systematically increasing the number of building blocks included in the training and analyze their impact on the quality of unseen molecules. In general, a model trained on a large and diverse data set has the potential to provide a general-purpose backmapping tool for a wide range of chemical systems.

In addition, parameterization strategies for CG force fields could benefit from DBM. As outlined in Chpt. \ref{bm_as_quality_measure}, a new force matching approach based on an evaluation of forces at the AA resolution can be used to improve the accuracy of cross-correlations between CG variables. A thorough investigation of the proposed method requires to implement an automated scheme that iteratively performs a CG simulation, backmaps the obtained CG configurations and updates the force field parameters taking the atomistic forces into account.

Moreover, future work to improve the methodology of DBM can focus on two aspects: (1) The conventional CNN architecture used for DBM in this work is not rotationally equivariant and therefore, the model has to learn rotational symmetries. Can rotational equivariant network architectures be used to improve the accuracy of DBM and reduce the computational effort for the backmapping task? (2) The order of reconstruction relies on a depth-first-search of the molecular graph. However, this might not be the best strategy for reconstruction. Can DBM therefore learn the order of reconstruction, such that artifacts upon backmapping can be reduced?

%Finally, beside further applications of DBM, MM can benefit from generative models in various ways. For example, temporal coherent backmapping, as introduced in Chpt. \ref{temp_coherent_bm}, of CG trajectories can be utilized for the analysis of fast and local processes on a higher resolution. Moreover, applying generative models to learn the fine-to-coarse mapping could improve CG representations.
In conclusion, MM can benefit from generative ML models in various ways. It is firmly believed that a tighter integration of ML approaches into the research field of computational chemistry will lead to significant advances in the future.

%This approach is especially efficient for polymeric systems, as large-scale properties of polymers can often be described by universal laws that incorporate chemistry-specific information into a few parameters. As such, computational efficient models, such as the KG model, can be applied to obtain the large-scale behavior of the system.

%Generative models are further distinguished between those that access the model probability distribution $p_{\Theta}(\mathbf{x})$ \textit{explicitly} through some functional form and those that define it \textit{implicitly} through a sampler. The former approach has the benefit that maximizing the likelihood $\mathcal{L}$ is straightforward, as the probability distribution can be accessed directly. However, explicit models require to assume some functional form for the probability $p_{\Theta}(\mathbf{x})$, which often becomes a bottleneck for the expressive power of the model. Therefore, the design of an explicit model is often a trade-off between the complexity of the true data distribution and tractability. On the other hand, implicit models do not require direct access of the likelihood function but define a stochastic procedure to generate new samples. 

%The various variants of explicit DGMs fall into two categories: Models that carefully construct a tractable functional form for the likelihood $\mathcal{L}$ and models that use a tractable approximation.\cite{goodfellow2020generative} Two major examples for tractable explicit models are autoregressive models and normalizing flow models. Examples for explicit models that require approximation are the variational auto encoder and the Boltzmann machine. The most prominent member of implicit generative models is the generative adversarial approach.


%Despite being a complex, non-convex optimization problem, simple methods such as
%stochastic gradient descent (SGD) are able to recover good solutions that minimize the training error.
%More surprisingly, the networks learned this way exhibit good generalization behavior, even when
%the number of parameters is significantly larger than the amount of training data

%The capabilities of deep learning models are often examined under notions of expressivity and capacity: their ability to learn a function of some complexity from a given set of examples. It has been shown that deep learning models are capable of high expressivity, and are hence able to learn any function under certain architectural constraints. However, classical measures of machine learning model expressivity (such as Vapnikâ€“Chervonenkis (VC) dimension [4], Rademacher complexity [5], etc.), which successfully characterize the behavior of many machine learning algorithms, fail to explain the generalization abilities of DNNs. Since DNNs are typically over-parameterized models with substantially less training data than model parameters, they are expected to overfit the training data and obtain poor generalization as a consequence [6]. However, this is not the case in practice. Thus, a specific line of work has been dedicated to study the generalization of these networks.

%Recently, DNNs have been deployed in a conditional framework as well \cite{isola2017image}. In particular, labels or cartoons of objects have been used as a conditional input for the network in order to generate a corresponding high-resolution image. Is it therefore possible to take advantage of DNNs for the reverse-mapping of molecular structures?

%Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.


%Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.
