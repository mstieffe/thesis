% Chapter Template

\chapter{Methology of DeepBM: Adversarial Backmapping of Condensed-Phase Molecular Structures}
\label{methology}

In this chapter, \textit{DeepBM} is introduced: A new method to tackle the backmapping problem for molecular structures based on ML. The method utilizes DNNs to learn a mapping from a coarse-grained representation to a higher resolution, i.e. it learns to reintroduce missing degrees of freedom. Unlike other backmapping schemes, deepBM aims at directly predicting equilibrated molecular structures that resemble the Boltzmann distribution. Importantly, deepBM does not rely on further energy minimization for relaxation and MD simulations for equilibration. Moreover, deepBM is designed for condensed-phase molecular systems and scales linearly with the number of fine-grained particles.

The method is based on DGMs with a convolutional network architecture. The model is trained with the generative adversarial approach and the coarse-grained structure is treated as a conditional variable for the generative process. To this end, training data is generated consisting of pairs of corresponding coarse-grained and fine-grained molecular structures. The convolutional architecture requires a regular discretization of 3D space, which prohibits scaling to larger spatial structures. Therefore, the generator is combined with an autoregressive approach that reconstructs the fine-grained structure incrementaly, i.e. atom by atom. In order to make the model scalable to arbitrary system sizes, each step uses only local information guided by the coarse-grained representation.

\section{Notation and Problem Formulation}

A backmapping scheme reintroduces details along the coarse-grained degrees of freedom. This amounts to generate coordinates $\mathbf{r} \in \mathbb{R}^{3n}$ for the $n$ atoms in the system, as described in Sec. \ref{theory_backmapping}. To this end, the backmapping function $\phi$ is necessarily a function of the coordinates $\mathbf{R} \in \mathbb{R}^{3N}$ of the $N$ coarse-grained beads. However, additional information to characterize the specific chemistry of both, the coarse-grained and the target fine-grained structure, can be provided as well in order to improve the quality and transferability of the mapping. 

Formally, let $\mathcal{A} = \{\mathbf{A}_{I} = (\mathbf{R}_I, \mathbf{C}_I) | I = 1, \dots, N \}$ denote a snapshot of the coarse-grained system consisting of $N$ beads. Each bead has position $\mathbf{R}_I \in \mathbb{R}^3$ and an associated type $\mathbf{C}_I  \in \mathbb{R}^F$. The type $\mathbf{C}_I$ is expressed as a $F$ dimensional feature vector and reflects various chemistry specific attributes, such as the bead mass, the connectivity or associated force field parameters. Similarly, let $\mathpzc{a} = \{\mathbf{a}_{I} = (\mathbf{r}_i, \mathbf{c}_i) | i = 1, \dots, n \}$ denote an atomistic snapshot of the system consisting of $n$ atoms, where each atom $\mathbf{a}_i$ has position $\mathbf{r}_i \in \mathbb{R}^3$ and type $\mathbf{c}_i \in \mathbb{R}^f$. The joint distribution of coarse- and fine-grained snapshots is denoted with $\mathcal{X}$. 

DeepBM is a DGM designed to infer the conditional probability 

\begin{equation}
 p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n)
\end{equation}

from training data $\mathcal{D} = \{(\mathcal{A}_j, \mathpzc{a}_j)\}$ that consists of pairs of corresponding atomistic and coarse-grained snapshots. The conditional probability implied by the model 

\begin{equation}
p_{\Theta}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n) , 
\end{equation}

where $\Theta$ are the model parameters, is not inferred explicitly, which is a hard problem in high-dimensional spaces (see Sec. \ref{}), but implicitly defined through a sampler $\phi_{\Theta}$ 

\begin{equation}
 \phi_{\Theta}: \mathbb{R}^{3N}, \mathbb{R}^{F}, \mathbb{R}^{f} \rightarrow \mathbb{R}^{3n} 
\end{equation}

that generates a set of coordinates $\phi_{\Theta}(\mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n) = \mathbf{r}_1, \dots, \mathbf{r}_n$, such that 

\begin{equation}
p_{\Theta}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n) \approx p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n). 
\end{equation}


%The training data $\mathcal{D} = \{(\mathcal{A}_j, \mathpzc{a}_j)\}$ consists of pairs of corresponding atomistic and coarse-grained snapshots. 
%Typically, the fine-grained structure $\mathbf{r} = (\mathbf{r}_1, \dots, \mathbf{r}_n)$ is sampled from the Boltzman distribution $p_{\mathbf{c}}(\mathbf{r}) \propto \text{exp}[ - U_{\mathbf{c}}(\mathbf{r})/(k_B T) ]$, where the superscript $\mathbf{c} = {\mathbf{c}_1, \dots, \mathbf{c}_n}$ denots the chemistry specific information of all the atoms. The corresponding coarse-grained structures $\mathbf{R} = (\mathbf{R}_1, \dots, \mathbf{R}_N)$ are obtained upon application of the coarse-grained mapping $\mathbf{R} = M_{\mathbf{C}}(\mathbf{r})$, 

\section{Autoregressive Reconstruction}

Sampling from $p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n)$ directly poses various challenges: (1) The number of coarse-grained beads and atoms is fixed, i.e. the model is only applicable to systems of the same size as the original system used for training. (2) Similarly, the ordering of the atoms and beads is not allowed to change. (3) The model becomes chemistry specific, i.e. it can only be deployed for molecules used during training and chemical transferability can not be achieved. (4) Most importantly, the number of particles in a molecular system is typically large. A method that generates all the coordinates at once would have to solve a unreasonable high-dimensional problem. As a consequence, applications would be limited to rather small system sizes, which would especially hinder the usage for condensed-phase systems.

In order to circumvent these limitations, $p_{\mathcal{X}}$ is factorized in terms of atomic contributions. More precisely, the generation of one specific atom becomes conditional on both, the coarse-grained beads as well as all the atoms previously reconstructed. To this end, the generative model $\phi_{\Theta}$ is trained to generate and refine atom coordinates sequentially. 

\subsection{Initial Structure}

The first step of the proposed algorithm is to generate an initial structure based on the factorization

\begin{equation}
 p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{c}_1, \dots, \mathbf{c}_n, \mathbf{A}_1, \dots, \mathbf{A}_N) =
 \prod_{i = 1}^{n}  p_{\mathcal{X}}(\mathbf{r}_{S(i)} | \mathbf{r}_{S(1)}, \dots, \mathbf{r}_{S(i-1)}, \mathbf{c}_{S(1)}, \dots, \mathbf{c}_{S(i)},  \mathbf{A}_1, \dots, \mathbf{A}_N),
\end{equation}

where $S$ sorts the atoms in the order of reconstruction and $\mathbf{r}_{S(1)}, \dots, \mathbf{r}_{S(i-1)}$ denotes the atoms that have already been generated. Eq. \ref{} allows to split a complex, high-dimensional problem into a sequence of rather simple tasks, namely to learn the conditionals $p_{\mathcal{X}}(\mathbf{r}_{S(i)} | \mathbf{r}_{S(1)}, \dots, \mathbf{r}_{S(i-1)}, \mathbf{c}_{S(1)}, \dots, \mathbf{c}_{S(i)},  \mathbf{A}_1, \dots, \mathbf{A}_N)$. 

\section{Representation of Molecular Structures}

\section{Conditional GAN}

\section{Potential Energy as Regularizer}
