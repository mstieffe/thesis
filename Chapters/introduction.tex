\chapter{Introduction} 
\label{introduction} 

%Scientists create models as purposeful simplifications of phenomena that are too complex to be observed experimentally or to be solved analytically. Ancient greeks like Plato already built geometric models to describe the motion of heavenly bodies. Nowadays, scientists use models to study phenomena on a wide range of different scales ranging from quantum-mechanical effects to cosmic expansion. 

The exploration of molecular processes is fundamental to a wide range of modern research areas, such as polymer science \cite{rubinstein2003polymer}, drug design \cite{anderson2003process} or folding dynamics of proteins \cite{dobson2003protein}. Novel algorithms and high-performance computing have made computational chemistry an important tool to gain further insights into the molecular nature of matter \cite{lewars2011computational}. In particular, the significance of physics-based models has to be highlighted, which are purposeful simplifications of molecular systems that are too complex to be observed experimentally or to be solved analytically. Molecular modeling facilitates calculations and predictions about the structure, thermodynamics and dynamics of molecular systems \cite{leach2001molecular}. 

On the other hand, processing of complex and high-dimensional data has become a hallmark of modern machine learning (ML). In the past decades, ML has emerged as a prominent research field that has a transformative impact on many domains, such as computer vision \cite{voulodimos2018deep}, speech recognition \cite{nassif2019speech} or medical image analysis \cite{fatima2017survey}. In its core, ML algorithms construct statistical models from data without relying on explicit program instructions. As such, the recent success of ML models is further fueled by the availability of large data sets. Recently, ML is gaining significant attention in many fields of modern science as well, especially particle physics and computational chemistry \cite{noe2020machine, vamathevan2019applications, radovic2018machine}. 

This thesis explores the advantages of integrating ML methods into molecular simulation frameworks, especially for problems that are difficult to solve from a pure physics-based perspective. ML is already applied in the computational chemistry community frequently, for example to construct molecular potentials \cite{behler2016perspective} or for the analysis of simulation data \cite{mardt2018vampnets}. Here, the emphasis is on generative tasks, i.e. deploying ML models to learn the complex dependencies between particles in order to synthesize realistic molecular structures. In particular, generative ML algorithms originally designed for computer vision are applied to increase the resolution of coarse-grained molecular systems. However, before the concept and goal of this thesis are outlined in more detail, the scientific context of this work has to be established first.

The theoretical underpinning for molecular models is given by statistical mechanics, which successfully explains macroscopic properties of matter in terms of microscopic degrees of freedom. However, while the fundamental principles to describe the motions of microscopic particles are well known, i.e. quantum-mechanics or Newton's equation of motion, the enormous number of microscopic degrees of freedom makes analytical solutions for most molecular systems intractable. In addition, resolving the microscopic state of a molecular system experimentally displays resolution limits: Modern microscopy techniques, such as cryo-EM \cite{adrian1984cryo, renaud2018cryo} or X-ray crystallography \cite{kendrew1958three}, achieve a spatial resolution of a few angstrom. However, a thorough understanding of molecular processes, such as protein folding, additionally requires a high temporal resolution \cite{takahashi2010spatio}. Microscopy techniques with high temporal resolution, like PALM \cite{betzig2006imaging} or LLSM \cite{chen2014lattice}, typically yield a lower spatial resolution. While longer exposure times or shorter wavelengths could be used to increase the spatiotemporal resolution in the diffraction experiments, the induced radiation damage prevents applications to biological systems \cite{holton2009beginner}. 

A possible remedy is offered by computer models based on experimental observations and/or analytical approximations. Simulations of the model can be used to study the behavior of the system and to predict its properties. The resolution limit of such computer models is theoretically only bound by computational effort. The most fundamental description of matter is a quantum-mechanical description that includes electronic degrees of freedom. However, models at this level of detail are computationally very demanding. As an example, a popular method is density functional theory deploying B3LYP functionals \cite{raghavachari2000perspective, tirado2008performance}, which scales as $\mathcal{O}(N^3)$, where $N$ is the number of atoms in the model system \cite{schuch2009computational}. The computational cost can be reduced significantly, when the molecular resolution is reduced to the level of single atoms that are treated as hard spheres. In this approximation, the effect of electrons is modeled as a potential energy surface representing the quantum ground-state. Such models are routinely implemented by molecular dynamics (MD) simulations that numerically integrate Newton's equation of motion. The deployed atomic interactions are often empirical and aim at correctly modeling structural, thermodynamic and/or dynamic properties of a target system \cite{mackerell2004empirical}. The computational effort of such classical models is dominated by long-range interactions, such as van der Waals and electrostatic interactions. Typically particle mesh Ewald summation is used to reduce the computational cost to $\mathcal{O}\big(N \text{log}(N)\big)$ \cite{darden1993particle}.

Rapid fluctuations of the atoms typically require an integration time step in the range of femtoseconds \cite{leimkuhler2015molecular}. However, timescales of relevant biological processes, such as protein folding or binding, can be in the order of microseconds up to seconds \cite{plattner2015protein, paul2017protein}. Therefore, an extremely large number of integration steps is required, such that even dedicated hardware and specialized software reach their limits: Current state-of-the-art integration systems achieve hundreds of nanoseconds up to tens of microseconds of simulation data per day for molecular systems that contain a few thousands of atoms \cite{shaw2014anton, eastman2017openmm}. 

%Therefore, a coarser description of matter is often required in order to push the limits of accessible length and time scales in the computer simulation.
To push the limits of accessible length- and timescales in the computer simulation, a coarser description of matter is routinely used. To this end, coarse-grained (CG) variables are deployed that represent an average over atomistic degrees of freedom. The lower resolution of CG systems reduces the computational effort of the simulation and allows for larger integration time steps \cite{marrink2010comment, nielsen2003coarse}. In addition, dynamics of the CG system are typically accelerated due to "softer" interactions between CG sites \cite{depa2005speed, fritz2011multiscale}. As such, CG models enable a faster exploration of configuration space. 

The particular resolution of a molecular model depends on the length- and timescales of the phenomena of interest. However, some phenomena display a wide range of relevant scales and therefore can not be captured by a single model. This is especially true for soft-matter systems, such as polymers, where processes on multiple scales can be linked and interwoven \cite{praprotnik2008multiscale, peter2010multiscale, peter2009multiscale}. In particular, local interactions can impact large scale conformational changes. Consequently, molecular modeling of soft-matter systems requires a methodology that captures the interplay of processes that are potentially linked to various different scales.

A solution is offered by multiscale modeling, where models of different resolutions are combined to address phenomena at multiple scales \cite{ayton2007multiscale, voth2008coarse, peter2010multiscale}. At the lower end, CG molecular models are deployed to study the large scale behavior of the system. However, a tight and consistent link between models of different resolutions requires to accomplish both mapping directions. In particular, a reverse-mapping to reintroduce details is required for the following reasons: (1) To rigorously analyze the simulation results on a local scale \cite{brocos2012multiscale, pandey2014multiscale, deshmukh2016water, pezeshkian2020backmapping}, (2) to enable a direct comparison to experimental data, for example obtained with spectroscopic methods \cite{hess2006long}, (3) to serve as starting point for further high-resolution simulations \cite{shimizu2018reconstruction, pandey2014multiscale}, or (4) to assess the stability and accuracy of the obtained CG structures \cite{shimizu2018reconstruction}.

The fine-to-coarse mapping of molecular configurations is typically a straight-forward computation, such as the center-of-mass calculation for a set of atoms. However, reverse-mapping is more challenging, as new degrees of freedom have to generated taking all their dependencies into account. In particular, a reverse-mapping scheme has to fulfill the following requirements: (1) The reintroduced details have to be consistent with the CG conformation, i.e. coarse-graining of the reverse-mapped structure has to yield the original CG structure. (2) The generated microstates must have high statistical weight and should ideally follow the Boltzmann distribution. (3) In addition, the mapping should not be unique, as the reduced resolution implies that a single CG structure corresponds to an ensemble of atomistic microstates.

Reverse-mapping is widely used in the molecular modeling community and several approaches to reintroduce details exist. Most reverse-mapping schemes follow the same strategy: At first, an initial atomistic structure is generated that fulfills the consistency condition. Two major approaches exist for this step: (1) Generic approaches place atoms close to their corresponding CG site, either randomly or based on some geometric rules \cite{rzepiela2010reconstruction, wassenaar2014going}. (2) Fragment-based schemes rely on a presampled library of atomistic fragments that are projected onto the CG conformation \cite{peter2009multiscale, zhang2019hierarchical, hess2006long, brasiello2012multiscale}. In both cases, energy minimization to relax the initial structure is required. Subsequently, MD simulations are performed to recover the correct statistical weights of the reinserted degrees of freedom. 

The computational effort for the subsequent energy minimization and equilibration procedures of such reverse-mapping schemes can become significant. As such, applications to large systems or high-throughput simulations are still limited. In addition, poorly initialized structures can get trapped into local minima with high energy barriers. Therefore, human intervention is frequently required for the reverse-mapping of more complex molecular structures and hence, hinder the automation of such processes. 

ML has shown its ability to detect and reproduce complex dependencies in a wide range of different domains. In particular, deep neural networks (DNNs) have received considerable attention in the field of computer vision \cite{voulodimos2018deep}. For example, generative models based on DNNs are able to synthesize photorealistic images of complex objects, such as human faces or animals \cite{zhang2017stackgan, karras2017progressive, brock2018large}. In its core, the great success of DNNs can be linked to a multiscale approach: Multiple layers are arranged subsequently and each layer transforms its input into a more abstract and composite representation. As such, DNNs represent data with multiple levels of abstraction. %Unlike traditional approaches, DNNs learn relevant pattern from the training examples instead of relying on handcrafted features. 

%Despite the great success of DNNs in terms of generative tasks, training of generative models is challenging, as it requires to judge the quality of generated samples. 
%The major route to train generative models is to maximize the data likelihood. However, directly assessing the data likelihood is typically based on approximations or computational models that provide a tractable functional form for the likelihood, which might limit the expressivity of the model. To circumvent these limitations, generative adversarial networks (GANs) have been developed \cite{creswell2018generative, gui2021review}. In its core, a GAN consists of two competing models trained in a game: A generator produces synthetic samples by transforming samples from a prior distribution. A second model, denoted as discriminator, is trained to distinguish between synthetic samples from the generator and real samples from a target distribution. As such, the discriminator provides a distance measure for the target distribution and the distribution of synthetic samples. As the generators objective is to minimize this distance, it is indirectly pushed towards reproducing the target distribution. Recently, generative DNNs have been deployed in a conditional framework \cite{isola2017image}. In particular, labels or cartoons of objects have been used as a conditional input for both models in order to generate a corresponding high-resolution image. Is it possible to take advantage of GANs for the reverse-mapping of molecular structures by incorporating the CG configuration as a conditional input?

Recently, generative DNNs have been deployed in a conditional framework \cite{isola2017image}. In particular, labels or cartoons of objects have been used as a conditional variable for the model in order to generate a corresponding high-resolution image. Generating a high-resolution image from a low-resolution representation has striking similarity to the reverse-mapping task of molecular structures. Is it therefore possible to take advantage of DNNs for MM?

This thesis answers this question with a resounding yes and demonstrates ML-based reverse-mapping. However, many challenges have to be solved to successfully accomplish this task. For example, how to define a training objective for the reverse-mapping? How can molecular structures be represented? How to avoid memory issues for large, high-dimensional configurations? All of these questions will be addressed in this work ultimately leading to the development of deepbackmap (DBM), a DNN-based approach for the reverse-mapping of condensed-phase molecular structures. In order to fulfill the consistency criteria, the CG variables are used as a conditional input for the ML model. Unlike other backmapping schemes, DBM aims at directly predicting equilibrated molecular structures resembling the Boltzmann distribution. Therefore, no further energy minimization or MD simulations are required. In addition, DBM requires little human intervention, since the reinsertion of local details is learned from training data.

%In particular, it will be demonstrated that a ML model conditioned on the CG variables can generate Boltzmann-distributed all-atom reconstructions.
%and demonstrates that ML algorithms can fulfill all the requirements for the reverse-mapping task. In particular, a ML model is trained to reproduce 
\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{./Figures/intro/intro2.pdf}
  \caption{Flowchart of this thesis' chapters. The theoretical foundation of this work is given by chapter 2 and 3. The core of this thesis forms chapter 4, where the methodology of deepbackmap is introduced. Applications of DBM and its variants can be found in chapters 5-8.}
  \label{FIG:chapter_flowchart}
\end{figure}

A flowchart for the structure of this thesis can be found in Fig.~\ref{FIG:chapter_flowchart}. The first two main chapters establish the theoretical foundation for this work. In particular, chapter 2 reviews the multiscale modeling approach, including statistical mechanics, molecular dynamics, coarse-graining and the challenges of reverse-mapping. Chapter 3 gives an introduction to ML with an emphasis on DNNs and generative models. The core of this thesis forms chapter 4, where the methodology of DBM is introduced and important concepts for the ML-based reverse-mapping task are outlined. In subsequent chapters, DBM and other ML-based techniques are applied to multiscale simulations: The general performance and transferability of DBM is evaluated in chapter 5 at the example of a challenging condensed-phase polymeric system that consists of polystyrene molecules. In chapter 6, DBM is applied to adjust local properties of molecular structures in order to resemble a target distribution more closely. Chapter 7 deploys reverse-mapping to assess the quality of CG models at the atomistic resolution. In chapter 8, a ML-based scheme inspired by DBM is applied to the reverse-mapping of molecular trajectories aiming at temporal coherence between subsequent frames. Finally, the thesis is concluded in chapter 9, where the highlights of this work are reviewed and future research questions are posed.
