% Chapter Template

\chapter{Methology of DeepBM: Adversarial Backmapping of Condensed-Phase Molecular Structures}
\label{methology}

In this chapter, \textit{DeepBM} is introduced: A new method to tackle the backmapping problem for molecular structures based on ML. The method utilizes DNNs to learn a mapping from a coarse-grained representation to a higher resolution, i.e. it learns to reintroduce missing degrees of freedom. Unlike other backmapping schemes, deepBM aims at directly predicting equilibrated molecular structures that resemble the Boltzmann distribution. Importantly, deepBM does not rely on further energy minimization for relaxation and MD simulations for equilibration. Moreover, deepBM is designed for condensed-phase molecular systems and scales linearly with the number of fine-grained particles.

The method is based on DGMs with a convolutional network architecture. The model is trained with the generative adversarial approach and the coarse-grained structure is treated as a conditional variable for the generative process. To this end, training data is generated consisting of pairs of corresponding coarse-grained and fine-grained molecular structures. The convolutional architecture requires a regular discretization of 3D space, which prohibits scaling to larger spatial structures. Therefore, the generator is combined with an autoregressive approach that reconstructs the fine-grained structure incrementaly, i.e. atom by atom. In order to make the model scalable to arbitrary system sizes, each step uses only local information guided by the coarse-grained representation.

\section{Notation and Problem Formulation}

Backmapping is the reintroduction of details along the coarse-grained degrees of freedom. More specifically, new coordinates $\mathbf{r} \in \mathbb{R}^{3n}$ for the $n$ atoms in the system have to be generated by the backmapping algorithm, as described in Sec. \ref{theory_backmapping}. To this end, the backmapping function $\phi$ is necessarily a function of the coordinates $\mathbf{R} \in \mathbb{R}^{3N}$ of the $N$ coarse-grained beads. However, additional information to characterize the specific chemistry of both, the coarse-grained and the target fine-grained structure, can be provided as well in order to improve the quality and transferability of the mapping. 

Formally, let $\mathcal{A} = \{\mathbf{A}_{I} = (\mathbf{R}_I, \mathbf{C}_I) | I = 1, \dots, N \}$ denote a snapshot of the coarse-grained system consisting of $N$ beads. Each bead has position $\mathbf{R}_I \in \mathbb{R}^3$ and an associated type $\mathbf{C}_I  \in \mathbb{R}^F$. The type $\mathbf{C}_I$ is expressed as a $F$ dimensional feature vector and reflects various chemistry specific attributes, such as the bead mass, the connectivity or associated force field parameters. Similarly, let $\mathpzc{a} = \{\mathbf{a}_{I} = (\mathbf{r}_i, \mathbf{c}_i) | i = 1, \dots, n \}$ denote an atomistic snapshot of the system consisting of $n$ atoms, where each atom $\mathbf{a}_i$ has position $\mathbf{r}_i \in \mathbb{R}^3$ and type $\mathbf{c}_i \in \mathbb{R}^f$. Each coarse-grained bead $\mathbf{A}_I$ has an associated set of atoms $\psi_{I} \subset \{\mathbf{a}_i | i=1, \dots, n\}$. Conversely, each atom $\mathbf{a}_i$ is associated with a coarse-grained bead $\mathbf{A}_{\Psi(i)}$. The joint distribution of coarse- and fine-grained snapshots is denoted with $\mathcal{X}$. In the following, a sequence of ordered objects $(\mathbf{x}_1, \dots, \mathbf{x}_k)$ is represented as $\mathbf{x}_{1}^{k}$, where the subscript and superscript denote the indices for the start and end point of the sequence, respectively.

DeepBM is a DGM designed to infer the conditional probability $p_{\mathcal{X}}(\mathbf{r}_1^n | \mathbf{A}_1^N, \mathbf{c}_1^n)$ from training data $\mathcal{D} = \{(\mathcal{A}_j, \mathpzc{a}_j)\}$ that consists of pairs of corresponding atomistic and coarse-grained snapshots. The conditional probability of the model $p_{\Theta}(\mathbf{r}_1^n | \mathbf{A}_1^N, \mathbf{c}_1^n)$, where $\Theta$ are the model parameters, is not inferred explicitly, but implicitly defined through a sampler $\phi_{\Theta}$. More specifically, the sampler  

\begin{equation}
 \phi_{\Theta}: \mathbb{R}^{3N}, \mathbb{R}^{F}, \mathbb{R}^{f} \rightarrow \mathbb{R}^{3n} 
\end{equation}

generates a set of coordinates $\phi_{\Theta}(\mathbf{A}_1^N, \mathbf{c}_1^n) = \mathbf{r}_1^n$. The overall goal is to tune the parameters $\Theta$ of $\phi_{\Theta}$ such that $p_{\Theta} \approx p_{\mathcal{X}}$.

%\begin{equation}
% p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n)
%\end{equation}

%\begin{equation}
%p_{\Theta}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n) \approx p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, %\dots, \mathbf{c}_n). 
%\end{equation}


%The training data $\mathcal{D} = \{(\mathcal{A}_j, \mathpzc{a}_j)\}$ consists of pairs of corresponding atomistic and coarse-grained snapshots. 
%Typically, the fine-grained structure $\mathbf{r} = (\mathbf{r}_1, \dots, \mathbf{r}_n)$ is sampled from the Boltzman distribution $p_{\mathbf{c}}(\mathbf{r}) \propto \text{exp}[ - U_{\mathbf{c}}(\mathbf{r})/(k_B T) ]$, where the superscript $\mathbf{c} = {\mathbf{c}_1, \dots, \mathbf{c}_n}$ denots the chemistry specific information of all the atoms. The corresponding coarse-grained structures $\mathbf{R} = (\mathbf{R}_1, \dots, \mathbf{R}_N)$ are obtained upon application of the coarse-grained mapping $\mathbf{R} = M_{\mathbf{C}}(\mathbf{r})$, 

\section{Autoregressive Reconstruction}

Sampling from $p_{\mathcal{X}}(\mathbf{r}_1^n | \mathbf{A}_1^N, \mathbf{c}_1^n)$ directly poses significant challenges. At first, the complexity of the sampling problem rises with the number of particles $n$. Conversely, the size of molecular systems studied with computer simulations is typically large. As a consequence, a sampler $\phi_{\Theta}$ designated to generate all coordinates at once has to solve an unreasonable high-dimensional problem. Such an one-shot approach is ultimately limited to rather small system sizes. 

Moreover, direct sampling restricts the transferability of the trained model. As an example, the number of coarse-grained beads and atoms is fixed, i.e. the model is only applicable to systems of the same size. This implies that the data required for training needs to be as high-dimensional as the target system. Such a strategy is questionable, as the purpose of most MS approaches is to extend the accesable system size. Additionally, the model becomes chemistry specific, as the trained sampler expects to generate the same kind molecules it was trained on. Therefore, it is not possible transfer the learned correlation across chemical space. 

The proposed method deepBM circumvents these limitations by factorizing $p_{\mathcal{X}}$ in terms of atomic contributions. More precisely, the generation of one specific atom becomes conditional on both, the coarse-grained beads as well as all the atoms previously reconstructed. Such a factorization can be obtained by applying the chain rule for probabilities

\begin{equation}
\label{probability_factorization}
 p_{\mathcal{X}}(\mathbf{r}_1^n | \mathbf{A}_1^N, \mathbf{c}_1^n) =
 \prod_{i = 1}^{n}  p_{\mathcal{X}}(\mathbf{r}_{S(i)} | \mathbf{r}_{S(1)}^{S(i-1)}, \mathbf{c}_{S(1)}^{S(i)},  \mathbf{A}_1^N),
\end{equation}

where $S$ sorts the atoms in the order of reconstruction and $\mathbf{r}_{S(1)}^{S(i-1)}$ denotes the atoms that have already been generated. Specifically, $S(i)$ denotes the atom index at the $i$th position in the ordering. Eq. \ref{probability_factorization} allows to split a complex, high-dimensional problem into a sequence of rather simple tasks, namely to learn the conditionals $p_{\mathcal{X}}(\mathbf{r}_{S(i)} | \mathbf{r}_{S(1)}^{S(i-1)}, \mathbf{c}_{S(1)}^{S(i)},  \mathbf{A}_1^N)$. To this end, the generative model $\phi_{\Theta}$ is trained to generate and refine atom coordinates sequentially. The dependence on earlier predictions of $\phi_{\Theta}$ makes the method \textit{autoregressive}. Note, that the modular reconstruction increases the flexibility of the model and offers a perspective to release the aforementioned restrictions.

\subsection{Ordering of Molecular Graphs}

The factorization proposed in Eq. \ref{probability_factorization} requires a strict ordering $S$ of the particles. In general, the ordering $S$ is not unique. In the current implementation, a molecule is selected randomly and completely reconstructed before the next molecule is chosen. Therefore, the sorting $S$ of the whole system is a random concatenation of the sorting for each individual molecule. 

In order to sort the particles of each molecule, the molecular structure can be represented as a graph. More precisely, particles and bonds are mapped to the nodes and edges of the graph. Note, that molecular graphs are generally undirected and can be cyclic or acyclic.\cite{david2020molecular} The graph representation allows to define the sorting of the particles as a graph traversal. Starting from a root node, the following strategies can be applied for traversing:

\begin{itemize}
 \item \textit{depth-first-search}: Each branch of the graph is explored as far as possible before backtracking.
 \item \textit{breadth-first-search}: All nodes at the present depth are explored before moving on to the nodes at the next depth level.
 \item \textit{random}: If the current node has multiple unexplored edges, the subsequent node is chosen randomly. 
\end{itemize}

Practice has shown that an ordering based on the depth-first-search yields the best performance regarding the quality of reconstructed molecules. More specifically, deepBM sorts the atoms depending on both, the coarse-grained and the atomistic molecular topology: In an outer loop, the coarse-grained molecular graph is explored yielding a sorting $S_{CG}(I)$ for the beads $\mathbf{A}_I$. In an inner loop, deepBM generates atom positions for each atom $\mathbf{a}_i$ in the fragment $\psi_{I}$ according to an ordering $S_{\psi_{I}}(i)$.

%In general, a molecular graph is an undirected graph that can be cyclic or acyclic. The ordering defined for the given graph structure basically converts the undirected graph into a directed graph. If the ordering allows to traverse the molecular graph such that each node is only visited after all of its dependencies are visited, it is called a \textit{topological sort}. Note, that a topological sort is only possible if the graph has no cycles.

\subsection{Initial Structure with Forward Sampling}

The first step of the proposed algorithm is to generate an initial structure. To this end, \textit{forward sampling} is used based on the factorization in Eq.\ref{probability_factorization}.\cite{koller2009probabilistic} The algorithm starts by sampling the variables with no parents from a prior distribution, i.e. the atom position $\mathbf{r}_{S(0)}$ for the first atom in the ordering $S$. Subsequent variables $\mathbf{r}_{S(i)}$ are generated by sampling from the conditional probability distributions $p_{\Theta}(\mathbf{r}_{S(i)} | \mathbf{r}_{S(1)}^{S(i-1)}, \mathbf{c}_{S(1)}^{S(i)},  \mathbf{A}_1^N)$ given the atoms generated in the previous steps. 

Forward sampling yields accurate results if the underlying graph structure allows to define a \textit{topological ordering}, i.e. a graph traversal in which each node is visited only after all of its dependencies are explored.\cite{koller2009probabilistic} Note, that a topological ordering exists only for directed acyclic graphs, which is generally not the case for molecular graphs. As a consequence, forward sampling applied for molecular graphs can yield structures with low statistical weight. Intuitively, forward sampling without topological order requires to sample some variables for which crucial information might be missing. In other words, it is not possible to find the optimal position for an atom without knowing its environment. This issue becomes especially apparent when the underlying graph contains cyclic structures, such as phenyl rings. Moreover, the autoregressive approach accumulates the errors, as misplaced atoms affect the placement of subsequent atoms.

\subsection{Refinement with Gibbs Sampling}

As outlined above, accurate sampling of molecular structures calls for more feedback than a simple forward sampling strategy allows. This is especially true for condensed-phase systems, where great care has to be taken to avoid steric clashes. To this end, a variant of \textit{Gibbs sampling} is applied, which subsequently refines the initial molecular structures.\cite{geman1984stochastic} 

Gibbs sampling is a Markov chain Monte Carlo algorithm. As such, it constructs a Markov Chain that eventually converges towards the target distribution. Gibbs sampling starts from an initial structure $\mathbf{r}_1^{n^{(0)}}$ and resamples each component iteratively along $S$. Importantly, each further iteration still updates a single component at a time, but each component is conditioned on \textit{all} other components: The component $\mathbf{r}_{S(i)}^{^{(k+1)}}$ is conditioned on the values $\mathbf{r}_{S(1)}^{S(i-1)^{(k+1)}}$ of already updated components at the current step $k+1$ up to $S(i)$ and thereafter, the values $\mathbf{r}_{S(i+1)}^{S(n)^{(k)}}$ from the previous step $k$ are used. More precisely, $\mathbf{r}_{S(i)}^{^{(k+1)}}$ is sampled according to $p_{\Theta}(\mathbf{r}_{S(i)}^{^{(k+1)}} | \mathbf{r}_{S(1)}^{S(i-1)^{(k+1)}}, \mathbf{r}_{S(i+1)}^{S(n)^{(k)}}, \mathbf{c}_{S(1)}^{S(n)},  \mathbf{A}_1^N)$. Experiments confirmed that such Gibbs sampling leads to a good approximation of the target distribution $p_{\mathcal{X}}$, even with a small number of iterations.

\section{Representation of Molecular Structures}

Learning of complex, high-dimensional and higher-order dependencies in generative models is a hallmark of computer vision. One of the most successful learning algorithms for processing image content are deep CNNs.\cite{fukushima1983neocognitron, lecun1989backpropagation, lecun1998gradient, rawat2017deep, voulodimos2018deep} Their success relies on their ability to exploit spatial and temporal correlations. Other key attributes of CNNs are automatic feature extraction, hierarchical learning and weight sharing.\cite{khan2020survey}

In order to leverage modern CNNs for the backmapping task, an explicit spatial discretization of ambient space is required. Similar to pixels in a two dimensional image, the three dimensional molecular structure has to be mapped onto a voxel-based representation.\cite{wu20153d} To this end, atoms and beads are represented as smooth densities, $\gamma$ and $\Gamma$, respectively. More specifically, Gaussian distributions are used to model particle densities, such that an atom $i$ at position $\mathbf{r}_i$ is represented as

\begin{equation}
  \gamma_i = \text{exp} \Big( - \frac{(\mathbf{x} - \mathbf{r}_i)^2}{2\sigma^2} \Big), 
\end{equation}

where $\mathbf{x}$ is a spatial location in Cartesian coordinates, expressed on a discretized grid due to voxelization, and $\sigma$ is the Gaussian width, which is treated as a hyper parameter. The same concept is used to represent coarse-grained beads.

\subsection{Local Environment}

The proposed voxel-based representation is well suited for deploying CNNs. However, it does not adapt well to large molecular structures, as the computational cost scales exponentially with the grid size. To circumvent these limitations, the autoregressive approach is used to build-up larger structures incrementally, while restricting the receptive field of the CNN: Rather than representing the molecular structure as a whole, the model becomes conditional on \textit{local environments}, where the information is limited to a cutoff $r_{\text{cut}}$. Such a locality assumption makes the model scalable to arbitrary system sizes. 

Beside introducing a cutoff $r_{\text{cut}}$, the local environments are centered and aligned. This improves generalization, as translational and rotational degrees of freedom are removed. In other words, the ML algorithm does not have to learn the corresponding equivariance from (additional) training data. This is especially important for regular CNNs, which are by construction equivariant with respect to translations, but not with respect to rotations. Although promising progress has been achieved recently regarding the design of rotational equivariant networks, it is not straightforward to extend these approaches to generative models.\cite{cohen2016steerable, xie2018crystal, thomas2018tensor} It is therefore desirable to use the given molecular geometry to reduce the rotational degrees of freedom. Experiments confirm that the alignment of the local environment improves the performance of the model significantly.

Specifically, the $i$th local environment $\epsilon_i$ for an atom $\mathbf{a}_{s(i)}$ is centered around the current bead of interest $\mathbf{A}_{\Psi(s(i))}$, i.e. all atoms and beads are shifted around $\mathbf{R}_{\Psi(s(i))}$. To improve readability, the notation $\Psi_i \coloneqq \Psi(s(i))$ is used in the following to denote the index of the coarse-grained bead associated with $\mathbf{a}_{s(i)}$. The local environment contains the densities of all particles within a cubic environment of size $2 r_{\text{cut}}$. Further, the local environment is rotated to a local axis. To this end, the bond between consecutive coarse-grained beads $\Psi_i$ and $\Psi_i -1$ is aligned to the local $z$ axis by a rotation matrix $\mathbf{M}_{\Psi_i}$. This yields the definition for the local environment

\begin{align}
\label{simple_representation}
 \epsilon_i ( \mathbf{x} ) =& \sum_{j=1, j \neq i}^{n} \gamma_{s(j)}(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) \\
 +& \sum_{J=1}^{N} \Gamma_J(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) ,
\end{align}

which extends over the region $-r_{\text{cut}} < x_{\alpha} < r_{\text{cut}} $, where $\alpha$ runs over the three Cartesian coordinates. Note, that $\mathbf{x}$ is discretized over a regular grid. In practice, $r_{\text{cut}}$ is chosen such that several beads are present in each local environment. 

In the case of forward-sampling, an incomplete representation $\tilde{\epsilon}_{i} ( \mathbf{x} )$ has to be used, where all atoms $s(j)$ are excluded for which $j \geq i$, i.e.

\begin{align}
\label{simple_representation2}
 \tilde{\epsilon}_{i} ( \mathbf{x} ) =& \sum_{j=1}^{i-1} \gamma_{s(j)}(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) \\
 +& \sum_{J=1}^{N} \Gamma_J(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) .
\end{align}

Centering and alignment of $\epsilon_I$ and $\tilde{\epsilon}_{i}$ removes three translational and two rotational degrees of freedom. This leaves one rotational degree of freedom around the director axis, which the model is supposed to learn from the training data. For this reason, the training set is augmented by means of rotations around said axis.

%To this end, only local information is used to position the atoms by restricting the receptive field of the applied CNN to rather small local environments. the autoregressive approach is used to build-up larger structures incrementally. Fortunately, the autoregressive approach does not require to represent the molecular structure as a whole, but allows to focus on rather small \textit{local environments}. Such a locality assumption is fueled by the coarse-graining philosophy, since a well parameterized coarse-grained model should already describe the behaviour beyond the atomistic resolution correctly. Therefore, it is reasonable to deploy the model only locally and limit the information about the environment to a cutoff $r_{{text{cut}}$. large scale behaviour beyond its resolution limits. 

\subsection{Feature Embedding}

The input of CNNs is typically a two or three dimensional image composed of multiple \textit{feature channels}, i.e. each pixel or voxel is vector-valued. The different channels provide different views on the data. As an example, an RGB image contains three seperate channels: One feature channel for every primary color. Similarly, the input for deepBM is composed of multiple channels to encode the presence of atoms and beads of a certain type. In the most basic version, the representation given in Eq. \ref{simple_representation} is used directly. This yields a single feature channel encoding all other atoms and beads. However, this leads to clutter in most cases and, even more importantly, a single feature channel does not allow to distinguish different types of atoms and beads. The opposite extreme is to assign each atom or bead to a different feature channel. Such a representation is also not flawless, because the permutational invariance of the atoms and beads is lost, i.e. atoms and beads have to be presented in a fixed order. This reduces the ability of the model to generalize dramatically.

To improve the representation defined in Eq. \ref{simple_representation}, various feature channels can be created. The underlying idea is that each channel reflects a different attribute of the atoms and beads assigned to it. For example, an attribute can encode the chemical element or represent the set of force-field parameters associated with a specific atom type. Further, attributes can encode the functional form of the interaction to the current atom of interest. Interaction types distinguish between bond, bending angle, torsion or Lennard-Jones. In its core, such interaction attributes reflect the local structure of the molecular graph, as they represent short paths with one (bond), two (bending angle), three (torsion) or more (Lennard-Jones) edges originating from the current atom of interest. As such, deepBM is trained to place an atom that completes the given paths. Paths of the same length can be split up further into distinct feature channels in order to emphasize the difference of their associated force-field parameters. For example, a bending angle $C-C-C$ between carbon atoms might be treated differently than a bending angle $H-C-H$ between carbon and hydrogen atoms.

Formally, let $f \in \{1,2,\dots,N_F\}$ denote the index of the $N_F$ different feature channels. The activation function, $h_f(s(j);s(i))$, is defined to denote association of an atom $s(j)$ with a channel $f$

\begin{equation}
 h_f(s(j); s(i)) =
  \begin{cases}
      1,& \text{if atom } s(j) \text{ has feature }f \text{ (relative to } s(i) \text{)}\\
      0,              & \text{otherwise}.
  \end{cases}
\end{equation}

Note, that some attributes, such as the associated atom types, are static, i.e. they have no dependence on the current atom of interest $s(i)$. Other attributes, like the interaction channels, are defined relative to atom $s(i)$. Similarly, an activation function $H_f(J)$ is defined to encode static attributes of the coarse-grained beads, i.e. the bead types.

This yields the following featurized representation

\begin{align}
\label{featurized_representation_gibbs}
 \epsilon_i ( \mathbf{x}, f ) =& \sum_{j=1, j \neq i}^{n} \gamma_{s(j)}(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) h_f(s(j);s(i)) \\
 +& \sum_{J=1}^{N} \Gamma_J(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) H_f(J).
\end{align}

The featurized representation for the forward-sampling $\tilde{\epsilon}_{i}$ is constructed similarly.

\section{Conditional GAN}

The autoregressive approach turns the complex problem of generating molecular structures into a sequence of much simpler decisions. However, learning the local placement of the atoms is still a challenging task. Implementing a rule based decision algorithm, for example grounded on the geometry or energy of the structure, quickly becomes tedious and problem specific. Even more importantly, such  methods would not be able to reproduce the desired Boltzmann distribution. On the other hand, ML models have shown the ability to learn complex distributions, such as the higher order dependencies in generating human faces. Deploying ML models also avoids tedious rule based programming, as the model learns decisions from training data. Additionally, many ML models generalize to a great extend.

At this point, the engine of deepBM is introduced: A ML model to learn the placement of the atoms. The recent success of GANs in generating sharp, photo-realistic images has motivated the application of the adversarial training approach. As stated in Sec. \ref{ML_GAN}, a generator $g_{\Theta}$ maps samples $\mathbf{z} \in \mathbb{R}^d$ from a latent distribution $\mathcal{Z}$ into the ambient space $\mathbb{R}^D$. A second model, the discriminator $c_{\Psi}$, acts as a distance measure in ambient space $\mathbb{R}^D$ for the real distribution $\mathcal{X}$ and the distribution of synthetic samples $g_{\Theta}(\mathcal{Z})$. However, the standard GAN approach does not offer much control over the generative process, as the correlations between latent and data distribution are chosen arbitrarily. On the other hand, BM requires to condition the generative process on the coarse-grained structure and the autoregressive approach demands the information of prior atoms. To this end, both networks $g_{\Theta}$ and $c_{\Psi}$ are provided with auxiliary information to generate samples related to this additional input.  Such an approach is called a conditional generative adversarial network (cGAN).
 
In the present work, the conditional input for both networks consists of the local environment representation $\epsilon_i$ and the atom type $c_i$, denoted with $\mathbf{u}_i = (\epsilon_i, c_i)$. The CNN architecture of the critic network $c_{\Psi}$ requires that the prediction of the generator has a smooth density representation to perform adversarial training. At the same time, the position of the atom has to be ultimtely expressed as a point coordinate. Two options are available to generate both consistently: 

1) The generator $g_{\Theta}$ predicts a smooth-density representation $\hat{\gamma}_i \coloneqq g_{\Theta}(\mathbf{z}, \mathbf{u}_i)$, which is collapsed back to point coordinates $\hat{\mathbf{r}}_i$. To this end, a weighted average is computed, discretized over the voxel-grid

\begin{equation}
  \hat{\mathbf{r}}_i = \int d\mathbf{x} \hat{\gamma}_i (\mathbf{x}) \approx \sum_{m,k,l} x_{mkl} \hat{\gamma}_i (x_{mkl}) .
\end{equation}

2) The generator $g_{\Theta}$ directly predicts a point coordinate $\hat{\mathbf{r}}_i \coloneqq g_{\Theta}(\mathbf{z}, \mathbf{u}_i)$, which is mapped to a smooth density representation $\hat{\gamma}_i$. To this end, a Gaussian mapping is used

\begin{equation}
    \hat{\gamma}_i = \text{exp} \Big( - \frac{(\mathbf{x} - \hat{\mathbf{r}}_i)^2}{2\sigma^2} \Big).
\end{equation}

Experiments have shown that both versions perform equally well. If not stated otherwise, the first option is used in the following. In either case, both $\hat{\gamma}_i$ as well as $\hat{\mathbf{r}}_i$ are differentiable and thus can be easily incorporated in a cost function.

The critic is trained to distinguish between reference densities $\gamma_i$ and generated densities $\hat{\gamma}_i$, which are both related to the conditional input $\mathbf{u}_i$. The adversarial cost function for the critic can be written as

\begin{equation}
  \mathcal{C}_{\text{adv}}(c_{\Psi}) = \mathbb{E}_{i} [ c_{\Psi}(\mathbf{u}_i, \boldsymbol{\gamma}_i) - c_{\Psi}(\mathbf{u}_i, \hat{\boldsymbol{\gamma}}_i) ] .
\end{equation}

As stated in Sec. \ref{theory_wgan}, the critic has to be constrained to a 1-Lipschitz function. To this end, gradient penalty as expressed in Eq. \ref{gradient_penalty} is added to the total cost function $\mathcal{C}_{\text{tot}}$ of the critic

\begin{equation}
  \mathcal{C}_{\text{tot}}(c_{\Psi}) = \mathcal{C}_{\text{adv}}(c_{\Psi}) + \lambda_{\text{gp}} \mathcal{C}_{\text{gp}}(c_{\Psi}), 
\end{equation}

where $\lambda_{\text{gp}}$ denots the weight of the gradient penalty.

The adversarial cost function for the generator can be written as 

\begin{equation}
  \mathcal{C}_{\text{adv}}(g_{\Theta}) = \mathbb{E}_{i} [ c_{\Psi}(\mathbf{u}_i, g_{\Theta}(\mathbf{z}, \mathbf{u}_i)) ] .
\end{equation}

\subsection{Recurrent Training}

The autoregressive reconstruction of the molecular structure demands for an autoregressive training protocol. 

\section{Potential Energy as Regularizer}
