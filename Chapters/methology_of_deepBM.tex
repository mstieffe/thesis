% Chapter Template

\chapter{Methology of DeepBM: Adversarial Backmapping of Condensed-Phase Molecular Structures}
\label{methology}

In this chapter, \textit{DeepBM} is introduced: A new method to tackle the backmapping problem for molecular structures based on ML. The method utilizes DNNs to learn a mapping from a coarse-grained representation to a higher resolution, i.e. it learns to reintroduce missing degrees of freedom. Unlike other backmapping schemes, deepBM aims at directly predicting equilibrated molecular structures that resemble the Boltzmann distribution. Importantly, deepBM does not rely on further energy minimization for relaxation and MD simulations for equilibration. Moreover, deepBM is designed for condensed-phase molecular systems and scales linearly with the number of fine-grained particles.

The method is based on DGMs with a convolutional network architecture. The model is trained with the generative adversarial approach and the coarse-grained structure is treated as a conditional variable for the generative process. To this end, training data is generated consisting of pairs of corresponding coarse-grained and fine-grained molecular structures. The convolutional architecture requires a regular discretization of 3D space, which prohibits scaling to larger spatial structures. Therefore, the generator is combined with an autoregressive approach that reconstructs the fine-grained structure incrementaly, i.e. atom by atom. In order to make the model scalable to arbitrary system sizes, each step uses only local information guided by the coarse-grained representation.

\section{Notation and Problem Formulation}

Backmapping is the reintroduction of details along the coarse-grained degrees of freedom. More specifically, new coordinates $\mathbf{r} \in \mathbb{R}^{3n}$ for the $n$ atoms in the system have to be generated by the backmapping algorithm, as described in Sec. \ref{theory_backmapping}. To this end, the backmapping function $\phi$ is necessarily a function of the coordinates $\mathbf{R} \in \mathbb{R}^{3N}$ of the $N$ coarse-grained beads. However, additional information to characterize the specific chemistry of both, the coarse-grained and the target fine-grained structure, can be provided as well in order to improve the quality and transferability of the mapping. 

Formally, let $\mathcal{A} = \{\mathbf{A}_{I} = (\mathbf{R}_I, \mathbf{C}_I) | I = 1, \dots, N \}$ denote a snapshot of the coarse-grained system consisting of $N$ beads. Each bead has position $\mathbf{R}_I \in \mathbb{R}^3$ and an associated type $\mathbf{C}_I  \in \mathbb{R}^F$. The type $\mathbf{C}_I$ is expressed as a $F$ dimensional feature vector and reflects various chemistry specific attributes, such as the bead mass, the connectivity or associated force field parameters. Similarly, let $\mathpzc{a} = \{\mathbf{a}_{I} = (\mathbf{r}_i, \mathbf{c}_i) | i = 1, \dots, n \}$ denote an atomistic snapshot of the system consisting of $n$ atoms, where each atom $\mathbf{a}_i$ has position $\mathbf{r}_i \in \mathbb{R}^3$ and type $\mathbf{c}_i \in \mathbb{R}^f$. Each coarse-grained bead $\mathbf{A}_I$ has an associated set of atoms $\psi_{I} \subset \{\mathbf{a}_i | i=1, \dots, n\}$. The joint distribution of coarse- and fine-grained snapshots is denoted with $\mathcal{X}$. 

DeepBM is a DGM designed to infer the conditional probability 

\begin{equation}
 p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n)
\end{equation}

from training data $\mathcal{D} = \{(\mathcal{A}_j, \mathpzc{a}_j)\}$ that consists of pairs of corresponding atomistic and coarse-grained snapshots. The conditional probability of the model 

\begin{equation}
p_{\Theta}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n) , 
\end{equation}

where $\Theta$ are the model parameters, is not inferred explicitly, but implicitly defined through a sampler $\phi_{\Theta}$. More specifically, the sampler  

\begin{equation}
 \phi_{\Theta}: \mathbb{R}^{3N}, \mathbb{R}^{F}, \mathbb{R}^{f} \rightarrow \mathbb{R}^{3n} 
\end{equation}

generates a set of coordinates $\phi_{\Theta}(\mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n) = \mathbf{r}_1, \dots, \mathbf{r}_n$. The overall goal is to tune the parameters $\Theta$ of $\phi_{\Theta}$ such that $p_{\Theta} \approx p_{\mathcal{X}}$.

%\begin{equation}
%p_{\Theta}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n) \approx p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, %\dots, \mathbf{c}_n). 
%\end{equation}


%The training data $\mathcal{D} = \{(\mathcal{A}_j, \mathpzc{a}_j)\}$ consists of pairs of corresponding atomistic and coarse-grained snapshots. 
%Typically, the fine-grained structure $\mathbf{r} = (\mathbf{r}_1, \dots, \mathbf{r}_n)$ is sampled from the Boltzman distribution $p_{\mathbf{c}}(\mathbf{r}) \propto \text{exp}[ - U_{\mathbf{c}}(\mathbf{r})/(k_B T) ]$, where the superscript $\mathbf{c} = {\mathbf{c}_1, \dots, \mathbf{c}_n}$ denots the chemistry specific information of all the atoms. The corresponding coarse-grained structures $\mathbf{R} = (\mathbf{R}_1, \dots, \mathbf{R}_N)$ are obtained upon application of the coarse-grained mapping $\mathbf{R} = M_{\mathbf{C}}(\mathbf{r})$, 

\section{Autoregressive Reconstruction}

Sampling from $p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n)$ directly poses significant challenges. At first, the complexity of the sampling problem rises with the number of particles $n$. Conversely, the size of molecular systems studied with computer simulations is typically large. As a consequence, a sampler $\phi_{\Theta}$ designated to generate all coordinates at once has to solve an unreasonable high-dimensional problem. Such an one-shot approach is ultimately limited to rather small system sizes. 

Moreover, direct sampling restricts the transferability of the trained model. As an example, the number of coarse-grained beads and atoms is fixed, i.e. the model is only applicable to systems of the same size. This implies that the data required for training needs to be as high-dimensional as the target system. Such a strategy is questionable, as the purpose of most MS approaches is to extend the accesable system size. Additionally, the model becomes chemistry specific, as the trained sampler expects to generate the same kind molecules it was trained on. Therefore, it is not possible transfer the learned correlation across chemical space. 

The proposed method deepBM circumvents these limitations by factorizing $p_{\mathcal{X}}$ in terms of atomic contributions. More precisely, the generation of one specific atom becomes conditional on both, the coarse-grained beads as well as all the atoms previously reconstructed. Such a factorization can be obtained by applying the chain rule for probabilities

\begin{equation}
\label{probability_factorization}
 p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{c}_1, \dots, \mathbf{c}_n, \mathbf{A}_1, \dots, \mathbf{A}_N) =
 \prod_{i = 1}^{n}  p_{\mathcal{X}}(\mathbf{r}_{S(i)} | \mathbf{r}_{S(1)}, \dots, \mathbf{r}_{S(i-1)}, \mathbf{c}_{S(1)}, \dots, \mathbf{c}_{S(i)},  \mathbf{A}_1, \dots, \mathbf{A}_N),
\end{equation}

where $S$ sorts the atoms in the order of reconstruction and $\mathbf{r}_{S(1)}, \dots, \mathbf{r}_{S(i-1)}$ denotes the atoms that have already been generated. Eq. \ref{probability_factorization} allows to split a complex, high-dimensional problem into a sequence of rather simple tasks, namely to learn the conditionals $p_{\mathcal{X}}(\mathbf{r}_{S(i)} | \mathbf{r}_{S(1)}, \dots, \mathbf{r}_{S(i-1)}, \mathbf{c}_{S(1)}, \dots, \mathbf{c}_{S(i)},  \mathbf{A}_1, \dots, \mathbf{A}_N)$. To this end, the generative model $\phi_{\Theta}$ is trained to generate and refine atom coordinates sequentially. The dependence on earlier predictions of $\phi_{\Theta}$ makes the method \textit{autoregressive}. Note, that the modular reconstruction increases the flexibility of the model and offers a perspective to release the aforementioned resitrictions.

\subsection{Ordering of Molecular Graphs}

The factorization proposed in Eq. \ref{probability_factorization} requires a strict ordering $S$ of the particles. In general, the ordering $S$ is not unique. In the current implementation, a molecule is selected randomly and completely reconstructed before the next molecule is chosen. Therefore, the sorting $S$ of the whole system is a random concatenation of the sortings for each individual molecule. 

In order to sort the particles of each molecule, the molecular structure can be represented as a graph. More precisely, particles and bonds are mapped to the nodes and edges of the graph. Note, that molecular graphs are generally undirected and can be cyclic or acyclic. The graph representation allows to define the sorting of the particles as a graph traversal. Starting from a root node, the following strategies can be applied for traversing:

\begin{itemize}
 \item \textit{depth-first-search}: Each branch of the graph is explored as far as possible before backtracking.
 \item \textit{breadth-first-search}: All nodes at the present depth are explored before moving on to the nodes at the next depth level.
 \item \textit{random}: If the current node has multiple unexplored edges, the subsequent node is chosen randomly. 
\end{itemize}

Practice has shown that an ordering based on the depth-first-search yields the best performance regarding the quality of reconstructed molecules. More specifically, deepBM sorts the atoms depending on both, the coarse-grained and the atomistic molecular topology: In an outer loop, the coarse-grained molecular graph is explored yielding a sorting $S_{CG}(I)$ for the beads $\mathbf{A}_I$. In an inner loop, deepBM generates atom positions for each atom $\mathbf{a}_i$ in the fragment $\psi_{I}$ according to an ordering $S_{\psi_{I}}(i)$.

%In general, a molecular graph is an undirected graph that can be cyclic or acyclic. The ordering defined for the given graph structure basically converts the undirected graph into a directed graph. If the ordering allows to traverse the molecular graph such that each node is only visited after all of its dependencies are visited, it is called a \textit{topological sort}. Note, that a topological sort is only possible if the graph has no cycles.

\subsection{Initial Structure with Forward Sampling}

The first step of the proposed algorithm is to generate an initial structure. To this end, \textit{forward sampling} is used based on the factorization in Eq.\ref{probability_factorization}. The algorithm starts by sampling the variables with no parents from a prior distribution, i.e. the atom position for the first atom in the ordering $S$. Subsequent variables are generated by sampling from the conditional probability distributions given the atoms generated in the previous steps. 

Forward sampling yields accurate results if the underlying graph structure allows to define a topological ordering, i.e. a graph traversal in which each node is visited only after all of its dependencies are visited. Note, that a topological ordering exists only for directed acyclic graphs, which is generally not the case for molecular graphs. As a consequence, forward sampling applied for molecular graphs can yield structures with low statistical weight. Intuitively, forward sampling without topological order requires to sample variables without taking all of their dependencies into account. Therefore, the algorithm might get stuck, because previously sampled variables 

if the atom positions are sampled without topological order, atoms are placed without  the generative process can easily get stuck  

\subsection{Refinement with Gibbs Sampling}


\section{Representation of Molecular Structures}

\section{Conditional GAN}

\section{Potential Energy as Regularizer}
