% Chapter Template

\chapter{Methology of DeepBM: Adversarial Backmapping of Condensed-Phase Molecular Structures}
\label{methology}

In this chapter, \textit{DeepBM} is introduced: A new method to tackle the backmapping problem for molecular structures. The method deploys a ML model that learns the coarse-to-fine mapping, i.e. it learns to reintroduce missing degrees of freedom from training examples. Unlike other backmapping schemes, deepBM aims at directly predicting equilibrated molecular structures resembling the Boltzmann distribution. As such, the method does not rely on further energy minimization for relaxation and MD simulations for equilibration of the fine-grained structures. As illustrated in Fig. \ref{fig_bm_intro}, deepBM is designed for condensed-phase molecular systems.%, especially polymeric systems. 

\begin{figure}
  \centering
      \includegraphics[width=0.7\textwidth]{./Figures/methology/bm_intro.pdf}
  \caption{DeepBM generates Boltzmann-equilibrated atomistic structures conditional on the coarse-grained configuration using an adversarial network. It is applied to the backmapping of a condensed-phase molecular systems, such as polystyrene.\cite{stieffenhofer2020adversarial}}
  \label{fig_bm_intro}
\end{figure}

DeepBM is a DGM with CNN architecture trained with the generative adversarial approach. The training data consists of pairs of corresponding coarse-grained and fine-grained molecular structures, where the coarse-grained structure is treated as a conditional variable for the generative process. The CNN architecture requires a regular discretization of 3D space, which prohibits scaling to larger spatial structures. Therefore, the generator is combined with an autoregressive approach that reconstructs the fine-grained structure incrementally, i.e. atom by atom. In each step only local information is used for scalability.

This chapter presents content that has been previously published in the following research articles. The content is reproduced here with kind permission from the other authors and the corresponding Journals published this work.

\hfill \break
\noindent \textit{Marc Stieffenhofer, Michael Wand, Tristan Bereau}\\
\textbf{Adversarial reverse mapping of equilibrated condensed-phase molecular structures}\\
Machine Learning: Science and Technology, Volume 1, Number 4\\
DOI: 10.1088/2632-2153/abb6d4\\
$\copyright$ IOP Publishing Ltd, 2020
\hfill \break

\noindent \textit{Marc Stieffenhofer, Tristan Bereau, Michael Wand}\\
\textbf{Adversarial reverse mapping of condensed-phase molecular structures: Chemical transferability}\\
APL Materials 9, Volume 9, Number 3\\
DOI: 10.1063/5.0039102\\
$\copyright$ AIP Publishing LLC, 2021
\hfill \break

%The computational cost scales linearly with the number of fine-grained particles.
%The sequential reconstruction is thereby guided by the coarse-grained configuration.

\section{Notation and Problem Formulation}

Backmapping is the reintroduction of details along the coarse-grained degrees of freedom. More specifically, the backmapping function $\phi$ is required to generate new coordinates $\mathbf{r} \in \mathbb{R}^{3n}$ for the $n$ atoms in the system. As described in Sec. \ref{theory_backmapping}, $\phi$ is a function of the coordinates $\mathbf{R} \in \mathbb{R}^{3N}$ of the $N$ coarse-grained beads. However, deepBM incorporates additional information to improve the quality and transferability of the mapping. Here, the additional information  characterizes the specific chemistry of both, the coarse-grained as well as the target fine-grained structure.

Formally, let $\mathcal{A} = \{\mathbf{A}_{I} = (\mathbf{R}_I, \mathbf{C}_I) | I = 1, \dots, N \}$ denote a snapshot of the coarse-grained system consisting of $N$ beads. Each bead has position $\mathbf{R}_I \in \mathbb{R}^3$ and an associated type $\mathbf{C}_I  \in \mathbb{R}^T$. The type $\mathbf{C}_I$ is expressed as a $T$ dimensional one-hot vector, where $T$ is the number of bead types, and reflects various chemistry specific attributes, such as the bead mass, the connectivity or associated force field parameters. Similarly, let $\mathpzc{a} = \{\mathbf{a}_{i} = (\mathbf{r}_i, \mathbf{c}_i) | i = 1, \dots, n \}$ denote an atomistic snapshot of the system consisting of $n$ atoms. Each atom $\mathbf{a}_i$ has position $\mathbf{r}_i \in \mathbb{R}^3$ and type $\mathbf{c}_i \in \mathbb{R}^t$, where $t$ is the number of atom types and $\mathbf{c}_i$ is a one-hot vector. Each coarse-grained bead $\mathbf{A}_I$ has an associated set of atoms $\pi_I = \{ \mathbf{a}_j | j \in \psi_{I}\} \subset \mathpzc{a}$, where $\psi_{I}$ is the corresponding set of atom indices. Conversely, each atom $\mathbf{a}_i$ has an associated coarse-grained bead $\mathbf{A}_{\Psi_i}$. The joint distribution of coarse- and fine-grained snapshots is denoted with $\mathcal{X}$. In the following, a tuple $(\mathbf{x}_1, \dots, \mathbf{x}_k)$ is represented as $\mathbf{x}_{1}^{k}$, where the subscript and superscript denote the indices for the first and the last element of the sequence, respectively.

DeepBM is a DGM designed to infer the conditional probability $p_{\mathcal{X}}(\mathbf{r}_1^n | \mathbf{A}_1^N, \mathbf{c}_1^n)$ from training data $\mathcal{D} = \{(\mathcal{A}_j, \mathpzc{a}_j)\}$ that consists of pairs of corresponding atomistic and coarse-grained snapshots. The conditional probability of the model $p_{\Theta}(\mathbf{r}_1^n | \mathbf{A}_1^N, \mathbf{c}_1^n)$, where $\Theta$ are the model parameters, is not inferred explicitly, but implicitly defined through a sampler $\phi_{\Theta}$. More specifically, the sampler  

\begin{equation}
 \phi_{\Theta}: \mathbb{R}^{3N}, \mathbb{R}^{TN}, \mathbb{R}^{tn} \rightarrow \mathbb{R}^{3n} 
\end{equation}

generates a list of coordinates $\phi_{\Theta}(\mathbf{A}_1^N, \mathbf{c}_1^n) = \mathbf{r}_1^n$. The overall goal is to tune the parameters $\Theta$ of $\phi_{\Theta}$ such that $p_{\Theta} \approx p_{\mathcal{X}}$.

%\begin{equation}
% p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n)
%\end{equation}

%\begin{equation}
%p_{\Theta}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, \dots, \mathbf{c}_n) \approx p_{\mathcal{X}}(\mathbf{r}_1, \dots, \mathbf{r}_n | \mathbf{A}_1, \dots, \mathbf{A}_N, \mathbf{c}_1, %\dots, \mathbf{c}_n). 
%\end{equation}


%The training data $\mathcal{D} = \{(\mathcal{A}_j, \mathpzc{a}_j)\}$ consists of pairs of corresponding atomistic and coarse-grained snapshots. 
%Typically, the fine-grained structure $\mathbf{r} = (\mathbf{r}_1, \dots, \mathbf{r}_n)$ is sampled from the Boltzman distribution $p_{\mathbf{c}}(\mathbf{r}) \propto \text{exp}[ - U_{\mathbf{c}}(\mathbf{r})/(k_B T) ]$, where the superscript $\mathbf{c} = {\mathbf{c}_1, \dots, \mathbf{c}_n}$ denots the chemistry specific information of all the atoms. The corresponding coarse-grained structures $\mathbf{R} = (\mathbf{R}_1, \dots, \mathbf{R}_N)$ are obtained upon application of the coarse-grained mapping $\mathbf{R} = M_{\mathbf{C}}(\mathbf{r})$, 

\section{Autoregressive Reconstruction}

Sampling from $p_{\mathcal{X}}(\mathbf{r}_1^n | \mathbf{A}_1^N, \mathbf{c}_1^n)$ directly poses significant challenges. At first, the complexity of the sampling problem rises with the number of particles $n$. However, the size of molecular systems studied with computer simulations is typically large. As a consequence, a sampler $\phi_{\Theta}$ designated to generate all coordinates at once has to solve a problem with unreasonably large dimensionality. Such an one-shot approach is ultimately limited to rather small system sizes. 

Moreover, direct sampling restricts the transferability of the trained model. As an example, the number of coarse-grained beads and atoms is fixed in a one-shot model, i.e. the model is only applicable to systems of the same size. This implies that the data required for training needs to be as high-dimensional as the target system. Such a strategy is questionable, as the purpose of most MS approaches is to extend the accessible system size. A more progressive approach is to train the model on fine-grained samples with rather small system sizes, but deploy it on larger coarse-grained structures. In addition, chemical transferability is limited in a one-shot approach, as the trained sampler expects to generate the same kind molecules it was trained on. As such, transferring the learned correlation across chemical space is not straight-forward. 

\begin{figure}
  \centering
      \includegraphics[width=0.7\textwidth]{./Figures/methology/autoreg_gan.pdf}
  \caption{Adversarial autoregressive approach: The generator, $g_{\Theta}$, sequentially samples atom positions conditional on the coarse-grained structure and the existing atoms. A critic network, $c_{\Psi}$, estimates the discrepancy between reference and generated atoms.\cite{stieffenhofer2020adversarial}}
  \label{fig_autoreg_gan}
\end{figure}

The proposed method deepBM circumvents these limitations by factorizing $p_{\mathcal{X}}$ in terms of atomic contributions. More precisely, the generation of one specific atom becomes conditional on both, the coarse-grained beads as well as all the atoms previously reconstructed. Such a factorization can be obtained by applying the chain rule for probabilities

\begin{equation}
\label{probability_factorization}
 p_{\mathcal{X}}(\mathbf{r}_1^n | \mathbf{A}_1^N, \mathbf{c}_1^n) =
 \prod_{i = 1}^{n}  p_{\mathcal{X}}(\mathbf{r}_{s(i)} | \mathbf{r}_{s(1)}^{s(i-1)}, \mathbf{c}_{s(1)}^{s(i)},  \mathbf{A}_1^N),
\end{equation}

where $s$ sorts the atoms in the order of reconstruction and $\mathbf{r}_{s(1)}^{s(i-1)}$ denotes the atoms that have already been generated. Specifically, $s(i)$ denotes the atom index at the $i$th position in the ordering. Eq. \ref{probability_factorization} allows to split a complex, high-dimensional problem into a sequence of rather simple tasks, namely to learn the conditionals $p_{\mathcal{X}}(\mathbf{r}_{s(i)} | \mathbf{r}_{s(1)}^{s(i-1)}, \mathbf{c}_{s(1)}^{s(i)},  \mathbf{A}_1^N)$. Such a modular reconstruction increases the flexibility of the model and offers a perspective to release the aforementioned limitations. 

In this study, the conditionals are implicitly learned by a generative model $\phi_{\Theta}$, i.e. $\phi_{\Theta}$ is trained to generate and refine atom coordinates sequentially. The local placement of the atoms is thereby learned with an adversarial approach, as illustrated in Fig. \ref{fig_autoreg_gan}. The dependence on earlier predictions of $\phi_{\Theta}$ makes the method \textit{autoregressive}.

% Note, that the modular reconstruction increases the flexibility of the model and offers a perspective to release the aforementioned restrictions.

\subsection{Ordering of Molecular Graphs}

The factorization proposed in Eq. \ref{probability_factorization} requires a strict ordering $s$ of the particles. However, the ordering $s$ is generally not unique and has to be defined artificially. Here, the order is defined on multiple levels: The ordering of molecules, as well as the ordering of beads and atoms within a molecule. While the ordering of the molecules is less important for the performance of the model, the traversal through the molecular structure has to be chosen carefully. 

\begin{figure}
  \centering
      \includegraphics[width=1.0\textwidth]{./Figures/methology/graph_traversal.pdf}
  \caption{Three different options to traverse a graph: a) depth-first search, b) breadth-first search, c) random search}
  \label{graph_traversal}
\end{figure}

The algorithm deepBM iterates through the sequence of molecules, which is arbitrarily chosen. Each molecule is completely reconstructed before the next molecule is visited. In order to sort the particles within each molecule, the molecular structure is represented as a graph. Specifically, particles and bonds are mapped to the nodes and edges of the graph. As such, the sorting of the particles is described as a graph traversal. Note, that molecular graphs are generally undirected and can be cyclic or acyclic.\cite{david2020molecular} Therefore, the molecular graph has no specific sorting. Here, three different strategies are available to traverse the graph. In each strategy, a root node is selected from which the traversal origins. If the structure is linear, the ends are typically chosen. From there on, the subsequent nodes are selected according to one of the following search-algorithms:

\begin{itemize}
 \item \textit{depth-first-search}: Each branch of the graph is explored as far as possible before backtracking. See Fig. \ref{graph_traversal} a).
 \item \textit{breadth-first-search}: All nodes at the present depth are explored before moving on to the nodes at the next depth level. See Fig. \ref{graph_traversal} b).
 \item \textit{random}: The subsequent node is chosen randomly. See Fig. \ref{graph_traversal} c).
\end{itemize}

Practice has shown that an ordering based on the depth-first-search yields the best performance regarding the quality of reconstructed molecules. 

DeepBM sorts the atoms depending on both, the coarse-grained as well as the atomistic molecular topology: In an outer loop, the coarse-grained molecular graph is explored yielding a sorting for the beads. Within each bead $\mathbf{A}_I$, deepBM iterates through the fragment $\psi_{I}$ before visiting subsequent beads.

Note that it is possible to let deepBM learn the order of reconstruction itself. However, while such an approach is feasible for small molecules, it poses significant computational and conceptual challenges for large molecular structures. In particular, learning the ordering of a molecule requires a cost-function that allows to backpropagate the error signal for every step in order to find the best reconstruction strategy. This becomes intractable for large molecules, since unrolling of the recursive approach requires to store a copy of the model in memory for each step (see Sec. \ref{ML_NN_DL}). Moreover, obtaining a suitable representation is more complicated (see Sec. \ref{DBM:representation}), as it requires to represent the molecular structure to its full extent in every step or automatically adapt a local environment to the current focus of interest.

%In general, a molecular graph is an undirected graph that can be cyclic or acyclic. The ordering defined for the given graph structure basically converts the undirected graph into a directed graph. If the ordering allows to traverse the molecular graph such that each node is only visited after all of its dependencies are visited, it is called a \textit{topological sort}. Note, that a topological sort is only possible if the graph has no cycles.

\subsection{Initial Structure with Forward Sampling}

The first step of the proposed algorithm is to generate an initial structure. To this end, \textit{forward sampling} is applied based on the factorization in Eq.\ref{probability_factorization}.\cite{koller2009probabilistic} The algorithm starts by sampling the variables with no parents from a prior distribution, i.e. the atom position $\mathbf{r}_{s(0)}$ for the first atom in the ordering $s$. Note that even this first atom position is not arbitrary, since trasnlational symmetry is lost by conditioning on the coarse-grained structure. Subsequent variables $\mathbf{r}_{s(i)}$ are generated by sampling from the conditional probability distributions $p_{\Theta}(\mathbf{r}_{s(i)} | \mathbf{r}_{s(1)}^{s(i-1)}, \mathbf{c}_{s(1)}^{s(i)},  \mathbf{A}_1^N)$ given the atoms generated in the previous steps. 

Forward sampling yields accurate results if the underlying graph structure has a \textit{topological order}, i.e. a graph traversal in which each node is visited only after all of its dependencies are explored.\cite{koller2009probabilistic} Note, that a topological order exists only for directed acyclic graphs, which is generally not the case for molecular graphs. As a consequence, forward sampling applied to molecular graphs can yield structures with low statistical weight, as it requires to sample some variables for which crucial information might be missing. In other words, it is not possible to find the optimal position for an atom without knowing its environment. This issue becomes especially apparent when the underlying graph contains cyclic structures, such as the phenyl rings in polystyrene. In general, the autoregressive approach is prone to accumulate the errors even without cyclic structures, since misplaced atoms can always affect the placement of subsequent atoms.

\subsection{Refinement with Gibbs Sampling}
\label{DBM:representation}

As outlined above, accurate sampling of molecular structures calls for more feedback than a simple forward sampling strategy allows. This is especially true for condensed-phase systems, where great care has to be taken to avoid steric clashes. To this end, a variant of \textit{Gibbs sampling} is applied, which subsequently refines the initial molecular structures.\cite{geman1984stochastic} 

Gibbs sampling is a Markov chain Monte Carlo algorithm. As such, it constructs a Markov Chain that eventually converges towards the target distribution. Gibbs sampling starts from an initial structure $\mathbf{r}_1^{n^{(0)}}$ and resamples each component iteratively along $S$. Importantly, each further iteration still updates a single component at a time, but each component is conditioned on \textit{all} other components: The component $\mathbf{r}_{s(i)}^{^{(k+1)}}$ is conditioned on the values $\mathbf{r}_{s(1)}^{s(i-1)^{(k+1)}}$ of already updated components at the current step $k+1$ up to $s(i)$ and thereafter, the values $\mathbf{r}_{s(i+1)}^{s(n)^{(k)}}$ from the previous step $k$ are used. More precisely, $\mathbf{r}_{s(i)}^{^{(k+1)}}$ is sampled according to $p_{\Theta}(\mathbf{r}_{s(i)}^{^{(k+1)}} | \mathbf{r}_{s(1)}^{s(i-1)^{(k+1)}}, \mathbf{r}_{s(i+1)}^{s(n)^{(k)}}, \mathbf{c}_{s(1)}^{s(n)},  \mathbf{A}_1^N)$. Experiments confirmed that such Gibbs sampling leads to a good approximation of the target distribution $p_{\mathcal{X}}$, even with a small number of iterations.

\section{Representation of Molecular Structures}
\label{DBM:representation}

Learning of complex, high-dimensional and higher-order dependencies in generative models is a hallmark of computer vision. As outlined in Sec. \ref{theory_ML_CNN}, one of the most successful learning algorithms for processing image content are deep CNNs.\cite{fukushima1983neocognitron, lecun1989backpropagation, lecun1998gradient, rawat2017deep, voulodimos2018deep} Their success relies on their ability to exploit spatial and temporal correlations. Other key attributes of CNNs are automatic feature extraction, hierarchical learning and weight sharing.\cite{khan2020survey}

In order to leverage modern CNNs for the backmapping task, an explicit spatial discretization of ambient space is required. Similar to pixels in a two dimensional image, the three dimensional molecular structure has to be mapped onto a voxel-based representation.\cite{wu20153d} To this end, atoms and beads are represented as smooth densities, $\gamma$ and $\Gamma$, respectively. More specifically, Gaussian distributions are used to model particle densities: An atom $\mathbf{a}_i$ at position $\mathbf{r}_i$ is represented as

\begin{equation}
\label{DBM:density_representation}
  \gamma_i = \text{exp} \Big( - \frac{(\mathbf{x} - \mathbf{r}_i)^2}{2\sigma^2} \Big), 
\end{equation}

where $\mathbf{x}$ is a spatial location in Cartesian coordinates, expressed on a discretized grid due to voxelization, and $\sigma$ is the Gaussian width, which is treated as a hyper parameter. The same concept is used to represent coarse-grained beads.

\subsection{Local Environment}
\label{DBM:loc_env}

The proposed voxel-based representation is well suited for deploying CNNs. However, it does not adapt well to large molecular structures, as the computational cost scales with the cubic grid size. To circumvent these limitations, the autoregressive approach is used to build-up larger structures incrementally, while restricting the receptive field of the CNN: Rather than representing the molecular structure as a whole, the model becomes conditional on \textit{local environments}, where the information is limited to a cutoff $r_{\text{cut}}$. Such a locality assumption makes the model scalable to larger system sizes, i.e. the computational cost scales linearly with the number of fine-grained particles.

Beside introducing a cutoff $r_{\text{cut}}$, the local environments are centered and aligned. This improves generalization, as translational and rotational degrees of freedom are removed. In other words, the ML algorithm does not have to learn the corresponding equivariance from (additional) training data. Note that the regular CNNs deployed in this thesis are equivariant with respect to translations by construction, but not with respect to rotations. Although promising progress has been achieved recently regarding the design of rotational equivariant networks, it is not straightforward to extend these approaches to generative models.\cite{cohen2016steerable, xie2018crystal, thomas2018tensor} It is therefore beneficial to make use of the given molecular geometry to reduce the rotational degrees of freedom. Experiments confirm that the alignment of the local environment improves the performance of the model significantly.

Specifically, the local environment $\epsilon_i$ for an atom $\mathbf{a}_{i}$ is centered around the current bead of interest $\mathbf{A}_{\Psi_i}$, i.e. all atoms and beads are shifted around $\mathbf{R}_{\Psi_i}$. The local environment contains the densities of all particles within a cubic environment of size $2 r_{\text{cut}}$. Further, the local environment is rotated to a local axis. To this end, the bond between consecutive coarse-grained beads $\Psi_i$ and $\Psi_i -1$ is aligned to the local $z$ axis by a rotation matrix $\mathbf{M}_{\Psi_i}$. This yields the definition for the local environment

\begin{align}
\label{simple_representation}
 \epsilon_i ( \mathbf{x} ) =& \sum_{j=1, j \neq i}^{n} \gamma_{j}(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) \\
 +& \sum_{J=1}^{N} \Gamma_J(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) ,
\end{align}

which extends over the region $-r_{\text{cut}} < x_{\alpha} < r_{\text{cut}} $, where $\alpha$ runs over the three Cartesian coordinates. Note that $\mathbf{x}$ is discretized over a regular grid. In practice, $r_{\text{cut}}$ is chosen such that several beads are present in each local environment. 

In the case of forward-sampling an incomplete representation $\tilde{\epsilon}_{i} ( \mathbf{x} )$ has to be used. $\tilde{\epsilon}_{i} ( \mathbf{x} )$ excludes all atoms $\mathbf{a}_{s(j)}$ for which $j \geq s^{-1}(i)$, where $s^{-1}(i)$ denotes the order for the atom $\mathbf{a}_i$, i.e.

\begin{align}
\label{simple_representation2}
 \tilde{\epsilon}_{i} ( \mathbf{x} ) =& \sum_{j=1}^{ s^{-1}(i)-1} \gamma_{s(j)}(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) \\
 +& \sum_{J=1}^{N} \Gamma_J(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) .
\end{align}

Centering and alignment of $\epsilon_i$ and $\tilde{\epsilon}_{i}$ removes three translational and two rotational degrees of freedom. This leaves one rotational degree of freedom around the director axis, which the model is supposed to learn from the training data. For this reason, the training set is augmented by means of rotations around said axis.

%To this end, only local information is used to position the atoms by restricting the receptive field of the applied CNN to rather small local environments. the autoregressive approach is used to build-up larger structures incrementally. Fortunately, the autoregressive approach does not require to represent the molecular structure as a whole, but allows to focus on rather small \textit{local environments}. Such a locality assumption is fueled by the coarse-graining philosophy, since a well parameterized coarse-grained model should already describe the behaviour beyond the atomistic resolution correctly. Therefore, it is reasonable to deploy the model only locally and limit the information about the environment to a cutoff $r_{{text{cut}}$. large scale behaviour beyond its resolution limits. 
%To improve readability, the notation $\Psi_i \coloneqq \Psi(s(i))$ is used in the following to denote the index of the coarse-grained bead associated with $\mathbf{a}_{s(i)}$.

\subsection{Feature Embedding}

The input of CNNs is typically a two or three dimensional image composed of multiple \textit{feature channels}, i.e. each pixel or voxel is vector-valued. The different channels provide different views on the data. As an example, an RGB image contains three separate channels: One feature channel for every primary color. Similarly, the input for deepBM is composed of multiple channels to encode the presence of atoms and beads of a certain type. In the most basic version, the representation given in Eq. \ref{simple_representation} is used directly. This yields a single feature channel encoding all other atoms and beads. However, this leads to clutter in most cases and, even more importantly, a single feature channel does not allow to distinguish different types of atoms and beads. The opposite extreme is to assign each atom or bead to a different feature channel. Such a representation is also not flawless, because the permutational invariance of the atoms and beads is lost, i.e. atoms and beads have to be presented in a fixed order. This reduces the ability of the model to generalize dramatically.

\begin{figure}
  \centering
      \includegraphics[width=1.0\textwidth]{./Figures/methology/representation.pdf}
  \caption{Representation and conditional input. Existing atoms and coarse-grained beads are split into separate channels according to their atom/bead type. In addition, the atomic information is split in terms of intra- and intermolecular interactions. All channels are voxelized and used in addition to a noise sample as input for the generator network, $g_{\Theta}$.\cite{stieffenhofer2020adversarial}}
  \label{fig_representation}
\end{figure}

As shown in Fig. \ref{fig_representation}, various feature channels are created to improve the representation defined in Eq. \ref{simple_representation}. The underlying idea is that each channel reflects a different attribute of the atoms and beads assigned to it. For example, an attribute can encode the chemical element or represent the set of force-field parameters associated with a specific atom type. Further, attributes can encode the functional form of the interaction to the current atom of interest. Interaction types distinguish between bond, bending angle, torsion or Lennard-Jones. In its core, such interaction attributes reflect the local structure of the molecular graph, as they represent short paths with one (bond), two (bending angle), three (torsion) or more (Lennard-Jones) edges originating from the current atom of interest. As such, deepBM is trained to place an atom that completes the given paths. Paths of the same length can be split up further into distinct feature channels in order to emphasize the difference of their associated force-field parameters. For example, a bending angle $C-C-C$ between carbon atoms might be treated differently than a bending angle $H-C-H$ between carbon and hydrogen atoms.

Formally, let $f \in \{1,2,\dots,N_f\}$ denote the index of the $N_f$ different feature channels. The activation function, $h_f(\mathbf{a}_j; \mathbf{a}_i)$, is defined to denote association of an atom $\mathbf{a}_j$ with a channel $f$

\begin{equation}
 h_f(\mathbf{a}_j; \mathbf{a}_i) =
  \begin{cases}
      1,& \text{if atom } \mathbf{a}_j \text{ has feature }f \text{ (relative to } \mathbf{a}_i \text{)}\\
      0,              & \text{otherwise}.
  \end{cases}
\end{equation}

Note, that some attributes, such as the associated atom types, are static, i.e. they have no dependence on the current atom of interest $\mathbf{a}_i$. Other attributes, like the interaction channels, are defined relative to atom $\mathbf{a}_i$. Similarly, an activation function $H_f(\mathbf{A}_J)$ is defined to encode static attributes of the coarse-grained beads, i.e. the bead types.

This yields the following featurized representation

\begin{align}
\label{featurized_representation_gibbs}
 \epsilon_i ( \mathbf{x}, f ) =& \sum_{j=1, j \neq i}^{n} \gamma_{j}(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) h_f(\mathbf{a}_j; \mathbf{a}_i) \\
 +& \sum_{J=1}^{N} \Gamma_J(\mathbf{M}_{\Psi_i} (\mathbf{x} - \mathbf{R}_{\Psi_i})) H_f(\mathbf{A}_J).
\end{align}

The featurized representation for the forward-sampling $\tilde{\epsilon}_{i}$ is constructed similarly.

\section{Conditional GAN}
\label{DBM:cGAN}

The autoregressive approach turns the complex problem of generating molecular structures into a sequence of much simpler decisions. However, learning the local placement of the atoms is still a challenging task. Implementing a rule based decision algorithm, for example grounded on the geometry or energy of the structure, quickly becomes tedious and problem specific. Even more importantly, such  methods would not be able to reproduce the desired Boltzmann distribution. On the other hand, ML models have shown the ability to learn complex distributions, such as the higher order dependencies in generating human faces.\cite{karras2017progressive} Deploying ML models also avoids tedious rule based programming, as the model learns decisions from training data. 

At this point, the engine of deepBM is introduced: A ML model to learn the local placement of the atoms. The recent success of GANs in generating sharp, photo-realistic images has motivated the application of the adversarial training approach for this task. As stated in Sec. \ref{ML_GAN}, a generator $g_{\Theta}$ maps samples $\mathbf{z} \in \mathbb{R}^d$ from a latent distribution $\mathcal{Z}$ into the ambient space $\mathbb{R}^D$. A second model, the discriminator $c_{\Psi}$, acts as a distance measure in ambient space $\mathbb{R}^D$ for the real distribution $\mathcal{X}$ and the distribution of synthetic samples $g_{\Theta}(\mathcal{Z})$. However, the standard GAN approach does not offer much control over the generative process, as the correlations between latent and data distribution are chosen arbitrarily. On the other hand, BM requires to condition the generative process on the coarse-grained structure and the autoregressive approach demands the information of prior atoms. To this end, both networks $g_{\Theta}$ and $c_{\Psi}$ are provided with auxiliary information to generate samples related to this additional input. Such an approach is called a \textit{conditional generative adversarial network} (cGAN). In the present work, the conditional input for both networks consists of the local environment representation $\epsilon_i$ and the atom type $c_i$, denoted with $\mathbf{u}_i = (\epsilon_i, c_i)$. 

\subsection{Densities and Coordinates}

The CNN architecture of the critic network $c_{\Psi}$ requires that the prediction of the generator $g_{\Theta}$ has a smooth density representation to perform adversarial training. At the same time, the position of the atom has to be expressed ultimately as a point coordinate. Two options are available to generate both consistently: 

1) The generator $g_{\Theta}$ predicts a smooth-density representation $\hat{\gamma}_i \coloneqq g_{\Theta}(\mathbf{z}, \mathbf{u}_i)$, which is collapsed back to point coordinates $\hat{\mathbf{r}}_i$. To this end, a weighted average is computed, discretized over the voxel-grid

\begin{equation}
  \hat{\mathbf{r}}_i = \int d\mathbf{x} \hat{\gamma}_i (\mathbf{x}) \approx \sum_{m,k,l} x_{mkl} \hat{\gamma}_i (x_{mkl}) .
\end{equation}

2) The generator $g_{\Theta}$ directly predicts a point coordinate $\hat{\mathbf{r}}_i \coloneqq g_{\Theta}(\mathbf{z}, \mathbf{u}_i)$, which is mapped to a smooth density representation $\hat{\gamma}_i$. To this end, a Gaussian mapping is used

\begin{equation}
    \hat{\gamma}_i = \text{exp} \Big( - \frac{(\mathbf{x} - \hat{\mathbf{r}}_i)^2}{2\sigma^2} \Big).
\end{equation}

Experiments have shown that both versions perform equally well. If not stated otherwise, the first option is used in the following. In either case, both $\hat{\gamma}_i$ as well as $\hat{\mathbf{r}}_i$ are differentiable and thus can be easily incorporated in a cost function.

\subsection{Adversarial Cost-Function for Training on Sequences}

Training of the networks is based on the WGAN protocol described in Sec. \ref{theory_wgan}, which is extended to incorporate conditional information. However, optimal positioning of an atom is only possible if the previous atoms are placed correctly. In other words, the autoregressive reconstruction of the molecular structure is prone to accumulate errors. Therefore, the training protocol of the network has to take the autoregressive nature of the approach into account. More specifically, the generator has to be penalized for its actions in the past if they hinder the correct placement of the current atom. To this end, backpropagation of the error signal is applied to an entire sequence of generated atom positions.

\begin{figure}
  \centering
      \includegraphics[width=1.0\textwidth]{./Figures/methology/autoregressive.pdf}
  \caption{Autoregressive training. Starting from an atomistic configuration taken from training data (black) the predicted atoms (red) will be added to the local environment description for predicting the next atom in the sequence.\cite{stieffenhofer2020adversarial}}
  \label{fig_autoregressive}
\end{figure}

For simplicity and practical reasons, the training sequences $\omega_I$ contain the indices $\psi_{I}$ of atoms corresponding to a single coarse-grained bead $\mathbf{A}_I$. Unlike $\psi_{I}$, the sequence $\omega_I$ is ordered according to $s$, i.e. $s(\omega_{I_k}) < s(\omega_{I_l})$ for $k < l$. The autoregressive adversarial cost-function $\mathcal{C}_{\text{ar}}$ the generator $g_{\Theta}$ aims to minimize is expressed as

\begin{equation}
  \underset{\Theta}{\text{min}} \; \mathcal{C}_{\text{ar}}(g_{\Theta}) =  \underset{\Theta}{\text{min}} \; \mathbb{E}_{I} \Big[ \frac{1}{|\omega_I|} \sum_{i \in \omega_I} c_{\Psi}(\mathbf{u}_i, g_{\Theta}(\mathbf{z}, \mathbf{u}_i)) \Big] ,
\end{equation} 

where $|\omega_I|$ is the number of atoms in the sequence. The critic $c_{\Psi}$ is trained to minimize

\begin{equation}
  \underset{\Psi}{\text{min}} \; \mathcal{C}_{\text{ar}} (c_{\Psi}) =  \underset{\Theta}{\text{min}} \; \mathbb{E}_{I} \Big[ \frac{1}{|\omega_I|} \sum_{i \in \omega_I} c_{\Psi}(\mathbf{u}_i, \boldsymbol{\gamma}_i) - c_{\Psi}(\mathbf{u}_i, g_{\Theta}(\mathbf{z}, \mathbf{u}_i)) + \lambda_{\text{gp}} \mathcal{C}_{\text{gp}}(\bar{\mathbf{u}_i} , \bar{\boldsymbol{\gamma}}_i)  \Big],
\end{equation} 

where $\mathcal{C}_{\text{gp}}$ is the gradient penalty to enforce the 1-Lipschitz continuity of the critic, as explained in Sec. \ref{theory_wgan}. The prefactor $\lambda_{\text{gp}}$ scales the weight of the gradient penalty and is set to $\lambda_{\text{gp}} = 10$ in all experiments. The density $\bar{\boldsymbol{\gamma}}_i$ is interpolated linearly between pairs of points $\boldsymbol{\gamma}_i$ and $g_{\Theta}(\mathbf{z}, \mathbf{u}_i)$. 

As illustrated in Fig. \ref{fig_autoregressive}, the local environments presented to the network during training are composed of atoms taken from the training data as well as already generated atoms: The initial local environment for the first atom in a sequence $\omega_I$ is constructed from training data. After each step, the generated density $\hat{\gamma}_i$ is added to the local environment representation for the next atom in the sequence, until all atoms in the sequence are generated. As such, the computational graph for the sequence generation consists of $|\omega_I|$ copies of the generator. The critic judges each step and the error signals of all $|\omega_I|$ steps are accumulated. After a sequence is completed, the accumulated cost is backpropagated through the unrolled network. This approach allows to take dependencies among the different steps into account.

%Hydrogens are typically treated separately, such that all non-hydrogens of a system are placed first in order to avoid that misplaced hydrogens hinder $g_{\Theta}$ to find suitable positions for the heavy atoms.

\section{Potential Energy as Regularizer}
\label{SEC:DBM_regularization}

Ideally, the adversarial cost is already sufficient to drive the generator towards reproducing the desired Boltzmann distribution. However, training of a GAN is notoriously unstable and the parameters of a GAN easily diverge. As a result, GANs have a number of failure modes, such as mode-collapse or failure to converge (see Sec. \ref{theory_ML_GAN_limitations}). Various forms of regularization have been deployed to address those issues and improve generalization, including gradient penalties, weight normalization or architectural methods.\cite{kurach2018gan, lee2020regularization}

Unlike data sets commonly used in the ML community, the target distribution $p_{\mathcal{X}}(\mathbf{x}) \propto \text{exp} \Big[ \frac{U(\mathbf{x})}{k_b T} \Big]$ for the desired molecular structures is already known up to a normalization constant, i.e. the partition function. This knowledge can be incorporated in the training of deepBM to improve its performance and to monitor the training process. Specifically, the potential energy $U$ of generated structures is utilized as an additional term $\mathcal{C}_{\text{pot}}$ in the cost function of the generator. As such, $\mathcal{C}_{\text{pot}}$ acts as a regularizer that effectively narrows down the functional space of the generator by penalizing structures with high potential energy. In bayesian terms, $\mathcal{C}_{\text{pot}}$ incorporates prior believe about the model into the training, as it helps steering the optimization towards generating structures with high Boltzmann weight. It thereby effectively accelerates convergence and helps to improve accuracy.

$\mathcal{C}_{\text{pot}}$ depends on the set of atoms $\pi_{I}$ corresponding to a coarse-grained bead $\mathbf{A}_I$ as well as reference atoms $N_I$ from the local neighborhood of different beads, i.e. $N_I = \{\mathbf{a}_j | \mathbf{a}_j \in \pi_J, J \neq I, |\mathbf{R}_J - \mathbf{R}_I| < r_{\text{cut}}\}$. In the following, $e_t$ denotes the potential energy of specific intra-molecular and intermolecular interactions, as described in Sec. \ref{molecular_forcefield}. Specifically, $t$ runs over the interaction types: intra-molecular bond, angle, and dihedral, and non-bonded Lennard-Jones. In this study, two different cost functions based on the potential energy are used.

The first prior cost function $\mathcal{C}^{(1)}_{\text{pot}}$ aims at minimizing the potential energy of generated structures,

\begin{equation}
  \mathcal{C}^{(1)}_{\text{pot}}(\hat{\pi}_{I}, N_I) = \sum_{t} \lambda_t e_t(\hat{\pi}_{I}, N_I) ,
\end{equation} 

where $\hat{\pi}_{I}$ denotes atoms positioned by the generator $g_{\Theta}$ and $\lambda_t$ scales the different energy terms.

The second prior cost function $\mathcal{C}^{(2)}_{\text{pot}}$ penalizes discrepancies between potential energies of generated and reference structures,

\begin{equation}
  \mathcal{C}^{(2)}_{\text{pot}}(\hat{\pi}_{I}, \pi_I, N_I) = \sum_{t} \lambda_t | e_t(\hat{\pi}_{I}, N_I) - e_t(\pi_{I}, N_I) | .
\end{equation} 

Overall, the following cost function is minimized by the generator 

\begin{equation}
    \underset{\Theta}{\text{min}} \; \mathcal{C}_{\text{tot}}(g_{\Theta}) =  \underset{\Theta}{\text{min}} \; \mathbb{E}_{I} \Big[ \frac{1}{|\omega_I|} \sum_{i \in \omega_I} c_{\Psi}(\mathbf{u}_i, g_{\Theta}(\mathbf{z}, \mathbf{u}_i)) + \lambda_{\text{pot}} \mathcal{C}_{\text{pot}}(\hat{\pi}_{I}, \pi_I, N_I) \Big] .
\end{equation}

Note that $\mathcal{C}_{\text{pot}}$ might be in a conflict with the adversarial cost function. While the purely data driven adversarial cost ultimately aims at reproducing the target distribution, the prior cost function encourages the generator to produce structures with a certain potential energy. In an extreme case, where optimization is solely based on the prior cost function, the generator is likely to collapse. Therefore, $\mathcal{C}_{\text{pot}}$ aims at supporting the adversarial optimization, which might suffer from resolution limits of the voxel representation, for fine tuning of the generated structures. As such, the prior cost function is scaled with an appropriately low weight $\lambda_{\text{pot}}$, such that it provides a significant contribution to the total cost only for high-energy structures. Moreover, training typically starts with $\lambda_{\text{prior}} = 0.0$ and is increased slowly during the course of optimization.
