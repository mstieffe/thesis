
\chapter{Multiscale Modeling} % Main chapter title

\label{theory_ms} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\cite{peter2010multiscale}, \cite{peter2009multiscale}

% -----------------------------------
%   Statistical Mechanics
% -----------------------------------

\section{Thermodynamics and Statiscal Mechanics}

%\subsection{Microstates vs Macrostates}
\textit{Classical thermodynamics} describes the behaviour of bulk, macroscopic systems in terms of a few macroscopic quantities, such as the total internal energy $E$, the total volume $V$, and the number of particles $N$. Typically, the system is considered at the \textit{thermodynamic equilibrium}, where its properties do not change with time and the actual state is history-independent. 

The basic concepts of classical thermodynamics were developed before the molecular nature of matter was generally accepted. Therefore, it is not surprising that classical thermodynamics is concerned with laws and relationships exclusively for macroscopic quantities without referencing to a more fundamental description on the molecular level. In fact, the laws of classical thermodynamics are based only on a few postulates.\cite{shell2015thermodynamics}

The molecular underpinning of thermodynamics was developed in the field of \textit{statistical mechanics}, where all the microscopic details of individual molecules are taken into account. For example, in the classical picture, a list of positions $\mathbf{r} \in \mathrm{R}^{3N}$ and momenta $\mathbf{p} \in \mathrm{R}^{3N}$ of the atoms are considered, whereas a quantum mechanical description uses quantum states. In the following, the classical picture is used for simplicity and a microscopic state $m = (\mathbf{r}, \mathbf{p})$ is characterized by its $6N$ degrees of freedom, i.e. a point in the $6N$ dimensional \textit{phase space}.

\subsection{The Mircrocanonical Ensemble and the Principle of Equal a Priori Probabilities}

In statistical mechanics, macroscopic quantities measured at equilibrium are described as the average behaviour of many particles. For an isolated system, i.e. a system that can not exchange energy or particles with its surrounding at fixed volume, a macrostate is completely specified by $(E, V, N)$, which remain constant throughout molecular motion.\cite{shell2015thermodynamics} Note that the temperature $T$ and pressure $P$ are not necessary to specify the macroscopic state, as their values can be derived once the vales for $(E, V, N)$ are set. For each macrostate $(E, V, N)$, a collection of possible microstates can be found, i.e. a surface in the phase space of $N$ atoms with constant total energy $E$ at a volume $V$. This collection of microstates together with their associated probabilities is called the \textit{microcanonical ensemble}.

The positions and velocities of the atoms constantly vary under the influence of their mutual interactions. Therefore, the microstate changes constantly even if the macrostate stays fixed. The likelihood that a microstate will be visited by the system is denoted with $p_m$ and the microstate probabilities do not change with time at equilibrium. A cornerstone of statistical mechanics is the statement that the system has no preference for a certain microstate and hence, each microstate is equally likelily.\cite{shell2015thermodynamics} This fundamental rule is called the \textit{principle of equal a priori probabilities}. It allows to write the likelihood in the canonical ensemble as

\begin{equation}
  p_m = \begin{cases}
    \frac{1}{\Omega(E,V,N)} \;\;\;\; & \text{if} \; E_m \neq E \\
    0 \;\;\;\; & \text{if} \; E_m \neq E
  \end{cases} ,
\end{equation}

where $\Omega(E,V,N)$, called the \textit{density of states}, is a function describing the number of accessible microstates for particular $(E,N,V)$.\cite{shell2015thermodynamics}

\subsection{Different views on the Entropy}

A central theme common for thermodynamics, statistical mechanics as well as information theory is the concept of \textit{entropy}.
In classical thermodynamics, the entropy $S$ is regarded as a non-conserved state-function that emerges naturally for systems in equilibrium.\cite{shell2015thermodynamics} It is a function

\begin{equation}
  S = S(E,V,N)
\end{equation}

dependent on the macroscopic quantities $E$, $V$ and $N$. Allowing for heat, volume or mass transfer, the system can change its equilibrium macrostate to another macrostate. This is called a \textit{thermodynamic process}. Historically, entropy was introduced to explain why some thermodynamic processes are irreversible, i.e. the process occurs spontaneously in one direction whereas the reverse does not, although both directions obey the conversation of energy.\cite{simon1997physical} The reason for this is that thermodynamic systems tend to progress towards states with increasing entropy. This is stated in the second law of thermodynamics: The entropy of an isolated system can not decrease as it always evolves to an equilibrium state where the entropy is highest.\cite{shell2015thermodynamics} 

While the specific form of the entropy function is different for every system, all entropy functions have some shared properties. A very important one is the total differential

\begin{equation}
  dS = \frac{1}{T}dE + \frac{P}{T}dV - \frac{\mu}{T}dN ,
\end{equation}

which relates the temperature $T$, the pressure $P$ and the chemical potential $\mu$ to derivatives of the same function and therefore defines a relationships between these variables.\cite{shell2015thermodynamics}

The above definition for the entropy $S$ is exclusively based on macroscopic properties. Boltzmann was the first who gave a definition for the entropy based on microscopic considerations and therefore introduced a connection of thermodynamics to the molecular nature of matter.\cite{shell2015thermodynamics} His famous formula reads

\begin{equation}
\label{boltzmann_entropy}
  S = k_B ln( \Omega(E,V,N) ) ,
\end{equation}

where $k_B$ is proportionality constant, called \textit{Boltzmann's constant}. Eq. \ref{boltzmann_entropy} links the entropy $S$ to the number of accessible microstates for given macrostate. Therefore, the second law of thermodynamics can be interpreted as the tendency of a system to evolve to a state that maximizes the number of accessible microstates.

Based on Boltzmann's equation, Gibbs introduced a more general form of the entropy 

\begin{equation}
\label{gibbs_entropy}
  S = k_B \sum p_m ln( p_m )  ,
\end{equation}

which is equivalent, up to the Boltzmann constant, to the definition of the information entropy by Shannon. Note, that upon application of the principle of equal a priori probailities, i.e. $p_m = \frac{1}{\Omega(E,V,N)}$, the entropy is maximized and Gibbs formulation of the entropy recovers Boltzmann's equation. 

%In this regard, the entropy can interpreted as a measure of uncertainty. Moreover, according to Jayne, the thermodynamic entropy can be regarded as an application of Shannon's information theory: The entropy $S$ expresses the missing information needed to determine the specific microstate of a system that remains uncommunicated due to a description of the system solely in terms of macroscopic quantities. From this point of view, the principle of equal a priori probabilities expresses our ignorance about the microscopic details of the system as it chooses the distribution that maximizes the entropy. 

\subsection{The Canonical Ensemble and the Boltzmann Distribution}

Central to the previous considerations was the ensemble for an system in isolation, i.e. the microcanonical ensemble for fixed $(E,V,N)$. In the following, the \textit{canonical ensemble} is introduced describing a system that is not isolated but at constant temperature. To this end, the system is considered to be in thermal contact with an infinitely large heat bath with a fixed temperature $T$. Therefore, the fixed macroscopic quanties of the system are $(T,V,N)$, while the total energy $E$ is allowed to fluctuate. The composite of the system and the heat bath is again considered to be an isolated system. However, summing over all the microstates of the heat bath allows to derive the probabilities for the microstates $m$ of the system of interest. Importantly, these microstate probabilities are no longer equal but depend on their total energy $E_m$. More specifically, the microstate probabilities can be written as

\begin{equation}
\label{boltzmann_dstr}
  p_m = \frac{e^{- \frac{E_m}{k_B T}}}{Z} ,
\end{equation}

where the normalization constant 

\begin{equation}
  Z = \sum_{\text{all }\, m \, \text{at}\, V,N} e^{- \frac{E_m}{k_B T}}
\end{equation}

is called the \textit{canonical partition function} and the probability distribution in Eq. \ref{boltzmann_dstr} is refered to as \textit{Boltzmann distribution}.\cite{shell2015thermodynamics} Note, that similarly to the microcanonical distribution, the Boltzmann distribution maximizes the entropy for a given macrocospic state $(T,V,N)$. In general, the canonical ensemble is more useful than the microcanonical ensemble in practice, since in most cases systems in thermal equilibrium with their surroundings are considered.

\subsection{Thermodynamic Limit and Statistical Equivalence of Ensembles}

The canonical approach provides an alternative, in addition to the microcanonical approach, to determine the behaviour of a system at a microscopic level. While there are rigorously no fluctuations in the energy in the microcanonical ensemble, energy fluctuates in the canonical ensemble but the temperature is rigorously constant. However, in the \textit{thermodynamic limit}, i.e. when the number of particles and the volume of the system go to inifinity $N \rightarrow \infty$, $V \rightarrow \infty$ while the particle density is held fixed $\frac{N}{V} = \text{constant}$, the differences in macroscopic properties for both ensembles vanish.\cite{shell2015thermodynamics} 

This can be seen clearly considering the distribution of the energy in the canonical ensemble. Using Eq. \ref{boltzmann_dstr} and \ref{boltzmann_entropy} the propability for a specific energy $E$ can be written as

\begin{equation}
  p(E) \propto \Omega(E,V,N) e^{- \frac{E_m}{k_B T}} = e^{\frac{1}{k_B} ( S(E,V,N) - \frac{E_m}{T} ) } .
\end{equation}

This equation shows that two competing terms have to be considered for the probability of energy levels: The first term is the entropy $S$, which is a concave, increasing function of $E$.\cite{shell2015thermodynamics} The second term $-\frac{E}{T}$ decreases linearly with the energy $E$. Therefore, the probability distribution for the energy levels has a maximum at an intermidiate energy $E^*$. Both terms, $S$ and $E$, are extensive quantities, i.e. they scale as $N$. Since the competing terms are within the exponential, the probability distribution becomes sharply peaked at the maximum $p(E^*)$. Therefore, $p(E^*)$ becomes the most dominant term and the impact of microstates with different energies $E \neq E^*$ vanishes.\cite{shell2015thermodynamics}

Another way to view this, is that fluctuations in the total energy $E$ become extremely small in the thermodynamic limit. The variance of the total energy $\sigma_E^2 = <E^2> - <E>^2$ can be linked to the heat capacity, which is an extensive quantity. Therefore, the relative magnitude of energy fluctuations scales as

\begin{eqnarray}
  \frac{\sqrt{\sigma_E^2}}{E} \propto N^{-1/2} .
\end{eqnarray}

As a consequence, a macroscopic systems appear to have constant energies.\cite{shell2015thermodynamics}

\subsection{Information-theoretic View on Statistical Mechanics}

In 1957 Jaynes published two papers emphazising the correspondence between information theory and statistical mechanics.\cite{jaynes1957information,jaynes1957information2} According to Jaynes, Gibbs' entropy in statistical mechanics and Shannon's information entropy are identical except for the Boltzmann constant.\cite{jaynes1957information} Consequently, statistical mechanics can be viewed from the perpespective of information theory and the seek for microscopic distributions can be treated as an inference problem.\cite{jaynes1957information} 

Jaynes adapted a bayesian perpective (see Sec. \ref{bayes_vs_frequentist}) and treated the testable information, i.e. the macroscopic observables, as prior information.\cite{jaynes1957information} Obviously, information is missing that is needed to determine the specific microstate of a system that remains uncommunicated due to a description of the system solely in terms of macroscopic quantities. Consequently, finding the microscopic probability distribution amounts to assign probabilities to microstates that avoid bias while still aggreeing with the observed macroscopic quantities.\cite{jaynes1957information} In other words, out of all possible distributions the one has to be chosen which is the least-informative to avoid arbitrary assumptions.

Shannon has showed, that the entropy written in Eq. \ref{gibbs_entropy} (without the Boltzmann constant) is a quantity that increases with increasing uncertainty.\cite{shannon1948mathematical} Consequently, the distribution that maximizes the entropy has to be found under the constraints of the given prior information. From this point of view, the principle of equal a priori probabilities is a consequence of our ignorance about the microscopic details of the system. In practice, the problem is solved introducing Lagrange multiplier to impose the constraints. For the microcanonical ensemble, the constraint renders to a zero probability for all microstates with different total energy than the observed one. In the canonical ensemble, the constraint is a fixed expectation value for the energy.\cite{jaynes1957information} In both cases, the well known results from statistical mechanics are reproduced, i.e. the uniform distribution for the microcanonical ensemble and the boltzmann distribution for the canonical ensemble.

% -----------------------------------
%   Molecular Dynamics Simulation
% -----------------------------------

\section{Molecular Dynamics Simulation: Sampling from the Thermodynamic Ensembles}

In many cases, it is possible to express the basic laws of nature in terms of relatively simple equations. Unfortunately, solving such equations analytically, such as the equations of motion for more than two interacting bodies, becomes impossible in many cases. However, it is central to the statistical mechanics view on thermodynamics to consider systems with an extremely large number of interacting particles. 
Fortunately, Computer simulations can circumvent these issues. The two main branches of computer simulation techniques for molecular systems are molecular dynamics (MD) and Monte Carlo (MC), but a whole range of hybrid techniques exists as well. In the following, the focus is set on MD simulations.

\textit{Molecular Dynamics} (MD) simulation numerically integrats Newtons equations of motion allowing to analyze the physical movement of atoms or molecules and predict bulk properties. The starting point for every MD simulation is to set up a model for the molecular interactions, i.e. the forcefield, and to select initial positions and velocities for the particles. Afterwards, the system is evolved in time using descrete timesteps where the particle positions and velocities are repeatly updated. To this end, the forces have to be calculated at every step. However, the algorithm and parameters for the numerical intergration have to be taken with great care in order to minimize the effects of cumulative errors.\cite{frenkel2001understanding} After the system is equilibrated, i.e. it has evolved for a sufficient amount of time such that macroscopic proties do not change anymore, snapshots or whole trajectories of the system can be extracted and the actual measurement of the observable quantity of interest can be performed. In this regard, MD simulation builts a bridge between theory and experiment, as it allows to test a model and compare it with experimental results.\cite{allen2004introduction}

The following introducting to MD is based on the book by Frenkel and Smit and the lecture notes of M. Scott Shell, which are excellent sources providing the interested reader with more detailed explanations on this topic.\cite{shell2019lecturenotes, frenkel2001understanding}

\subsection{Molecular Force Fields}
\label{molecular_forcefield}

Calculating forces acting on the particles is crucial to evolve the system in time. To this end, a definition for the potential energy function is required describing the interactions between the particles. These potentials can be defined on vary different levels of resolution. In the following, the classical description is used, which ignores the motion of the electrons and focuses solely on the motion of the nuclei. However, MD simulations including electronic degrees of freedom are also possible.\cite{marx2000ab, lemkul2016empirical} In this regard, the classical description approximates the effect of electrons as a potential energy surface representing the quantum ground-state.\cite{shell2019lecturenotes} This description is reasonable in cases where the Born-Oppenheimer approximation is valid and the electronic structure is not of interest.\cite{shell2019lecturenotes} Furthermore, bond breaking or forming is prohebitted.\cite{shell2019lecturenotes}

Typically, the potential $U (\mathbf{r})$ is divided into a term representing the bonded interactions and a term for the nonbonded interactions \cite{shell2019lecturenotes}

\begin{equation}
  U = U_{\text{bonded}} + U_{\text{nonbonded}} .
\end{equation}

Most of the forcefields used in classic MD are emperical and are obtained by fitting simple analytic functions against experimental data or detailed electronic calculations, such as density-functional theory (DFT). 

\subsubsection{Bonded Interactions}

Bonded interactions are associated with chemical bonds, bond angles and bond dihedrals. A typical choice is to deploy harmonic and cosine potentials to model the bonded interactions, such as

\begin{equation}
  U_{\text{bonded}} = \sum_{\text{bonds}} a(d - d_0)^2 + \sum_{\text{angles}} b(\Phi - \Phi_0)^2 + \sum_{\text{dihedrals}} \Big( \sum_{n} c_n cos(\omega)^n \Big) ,
\end{equation}

where $d$ and $d_0$ are the calculated and the equilibrium bond length respectively, $\Phi$ and $\Phi_0$ the calculated and equilibrium bond angles and $\omega$ is the calculated dihedral angle.\cite{shell2019lecturenotes} The parameters $a$, $b$ and $c_n$ are the strength for the harmonic and the cosine series potential repectively. 

\subsubsection{Nonbonded Interactions}

The nonbonded potential is associated with van der Waals attraction, pauli repulsion and electrostatic interactions.\cite{shell2019lecturenotes} The van der Waals attraction arises due to the correlation between instantaneous dipoles between electron clouds of the atoms. The Pauli repulsion occurs upon the overlap of electron clouds and is a consequence of the Pauli principle, which forbids any two electrons from having the same quantum numbers. Both, the van der Waals attraction and the Pauli repulsion, are often combined into a single expression, such as the Lennard-Jones potential. The electrostatic forced arise due to partial or formal charges of the atoms and are taken into account through Coulomb's law. In combination, a typical nonbonded potential is modeled as

\begin{equation}
  U_{\text{nonbonded}} = \sum_{\text{pairs}} \Big( \underbrace{ 4 \epsilon \bigg[ \big( \frac{r_{ij}}{\sigma} \big)^{-12} - \big( \frac{r_{ij}}{\sigma} \big)^{-6} \bigg] }_{\text{Lennard-Jones}} + \underbrace{ \frac{q_i q_j}{4 \pi \epsilon_0 r_{ij}} }_{\text{Coulomb}} \Big) , 
\end{equation}

where $r_{ij}$ is the pairwise distance, $q_i$ and $q_j$ are the net (partial) charges, $\epsilon$ and $\sigma$ are the Lennard-Jones parameters, which depend on the particular atom types and $\epsilon_0$ is the electric permitivity in vacuum.\cite{shell2019lecturenotes} 

Obviously, calculating the nonbonded interactions is computationally more expensive compared to the bonded interactions, since the number of terms in the pairwise atomic sum scales as $N^2$, while the other scale as $N$. To reduce the computational overhead, the Lennard-Jones potential is often truncated as its contribution becomes minimal for large distances. To this end, a cutoff distance of $r_c \approx 2.5 \sigma$ is typically used where the energy is only a few percent of the minimum energy ($U_{\text{LJ}}(r_c) \approx -0.016 \epsilon$).\cite{shell2019lecturenotes} In addition, it is common practice to shift to potential to avoid discontinuities, i.e. subtracting the value of the potential at the cutoff. However, the truncated contributions can become significant for the total energy and pressure of the system. To this end, a correction to the total potential can be introduced, which is derived analytically for isotropic systems.\cite{shell2019lecturenotes, frenkel2001understanding}  On the other hand, the long-range Coulomb interaction need a special treadment, as a a a tail correction can not be derived directly.\cite{shell2019lecturenotes} Therefore, Ewald summation is typically used to reduce the computational effort. In this case, the potential is split into a short-range and long-range contribution. The short-range contributions are computed in real space, while the long-range contributions are computed in Fourier space. 

\subsection{Numerical Integration}
\label{numerical_integration}

The time evolution of the molecular system is described by Newton's equations of motion: Given the potential energy function $U(\mathbf{r})$ the forces $m \frac{d^2 \mathbf{r}}{d t^2}$ acting on the $N$ atoms are computed as

\begin{equation}
\label{newtons_eq_motion}
  m \frac{d^2 \mathbf{r}}{d t^2} = - \frac{d U (\mathbf{r})}{d \mathbf{r}} .
\end{equation}

Starting from initial positions and velocities, the time-evolution of the system traces a path in phase space, called trajectory. This trajectory is a set of states compatible with the starting condition. For classical systems, the phase space trajectory lies on a surface of constant energy, as Newton's equations conserve energy.\cite{shell2019lecturenotes} For ergodic systems, the trajectory will eventually visit all points in phase space compatible with the given total energy, whereas systems that are non-ergodic have areas in phase space that are inacessible. Obviously, it is not possible to sample all states of the trajectory. However, MD simulations aim at sampling the accessible phase space representatively.

Note that Eq. \ref{newtons_eq_motion} is a set of $3N$ second-order, nonlinear, coupled partial differential equations, which can not be solved analytically.\cite{shell2019lecturenotes} Therefore, numerical integration is used to evolve the system in time. Here, the basic idea is to introduce a small timestep $\delta t$ and find the positions of the atoms at consecutive time steps, i.e. 

\begin{equation}
  \mathbf{r}(0), \mathbf{r}(\delta t), \mathbf{r}(2 \delta t), ..
\end{equation}

Many different algorithms exist to progate the system forward in time. As an example, the Verlet algorithm is explained in the following.

\subsubsection{Verlet Algorithm}

Using a Taylor expansion, the position at time $t + \delta t$ can be written as

\begin{equation}
\label{verlet_taylor1}
  \mathbf{r}(t + \delta t) = \mathbf{r}(t) + \frac{d \mathbf{r}(t)}{d t} \delta t + \frac{d^2 \mathbf{r}(t)}{d t^2} \frac{ \delta t^2}{2} + \frac{d^3 \mathbf{r}(t)}{d t^3} \frac{ \delta t^3}{6} + \mathcal{O}(\delta t^4) .
\end{equation}

Similarly, the position at the previous time step can be written as

\begin{equation}
\label{verlet_taylor2}
  \mathbf{r}(t - \delta t) = \mathbf{r}(t) - \frac{d \mathbf{r}(t)}{d t} \delta t + \frac{d^2 \mathbf{r}(t)}{d t^2} \frac{ \delta t^2}{2} - \frac{d^3 \mathbf{r}(t)}{d t^3} \frac{ \delta t^3}{6} + \mathcal{O}(\delta t^4) .
\end{equation}

Adding Eq. \ref{verlet_taylor1} and \ref{verlet_taylor2} and rearranging leads to

\begin{equation}
\label{verlet}
  \mathbf{r}(t + \delta t) = 2 \mathbf{r}(t) - \mathbf{r}(t - \delta t) + \frac{d^2 \mathbf{r}(t)}{d t^2} \frac{ \delta t^2}{2} + \mathcal{O}(\delta t^4) .
\end{equation}

Eq. \ref{verlet} allows to compute the positions at the next time step from the positions at the two previous timesteps and the forces, which can be calculated using Eq. \ref{newtons_eq_motion}. The error of this algorithm is of order $\mathcal{O}(\delta t^4)$ and therefore decreases with the time step.

Note that the Verlet algorithm has two important features: It is time-reversible and symplectic, i.e. volume in phase space is preserved.\cite{allen2004introduction} Those features are crucial to maintain correct statistical sampling and stability, as those are properties of the true Hamiltonian dynamics.\cite{shell2019lecturenotes} Algorithms that do not preserve the volume in phase space can dramatically expand the initial volume, such that it eventually covers areas of phase space that are not compatible with the starting condition and might violate energy conservation. Although reversiblity and the conservation of phase space volume does not automatically guarantee that there is no drift of the total energy on a long time scale, it is at least a reasonable requirement.\cite{frenkel2001understanding} In practice, the Verlet algorithm does not exactly conserve the total energy, but it exhibits little long energy drifts.\cite{frenkel2001understanding} Note, that there a many different algorithm that can be derived from the Verlet algorithm yielding identical trajectories, such as the leap frog or the velocity Verlet algorithm.\cite{frenkel2001understanding}

\subsection{Thermostats and Barostats: Controlling Temperature and Pressure}

The numerical integration scheme described in Sec. \ref{numerical_integration} allows to sample from the microcanonical ensemble, i.e. maintain a constant total energy. However, it is often desirable to sample from different ensembles, such as the canonical ensemble (NVT) or the isothermal-isobaric ensemble (NPT). Modifications on the MD algorithm that allow to control the temperature or pressure are called thermostat or barostat algorithms respectively. They are important for many reasons, for example to match experimental conditions, study temperature dependent processes or enhance the efficiency of conformational search.\cite{Huenenberger2005} Several methods exist to control temperature and pressure during the simulation. In the following, some of the popular techniques are introduced.

\subsubsection{Velocity Rescaling}

The simplest approach to control the temperature is the velocity rescaling algorithm. The temeprature is related to the kinetic energy and can be estimated as

\begin{equation}
 T = \frac{2 <K>}{k_B n_{DOF}} ,
\end{equation}

where $K$ is the kinetic energy and $n_{DOF}$ are the degrees of freedom.\cite{shell2019lecturenotes} Therefore, the velocities can be rescaled at each time step to fix the temperature to a desired value. Despite its simplicity, this algorithm does not reproduce the correct thermodynamic properties of the canonical ensemble, since the fluctuations in the kinetic energy are not captured.\cite{shell2019lecturenotes} 

\subsubsection{Anderson Thermostat}

The Anderson thermostat introduces random collision of the molecules with an imaginary heat bath at the desrired temperature. To this end, particles are chosen at random and their velocities are sampled randomly from the Maxwell-Boltzmann distribution:

\begin{equation}
 p(\vec{v}) = \Big( \frac{m}{2 \pi k_B T} \Big)^{3/2} \text{exp} \Big[- \frac{m |\vec{v}|^2}{2 k_B T} \Big]
\end{equation}

Although this approach generates the correct canonical ensemble propabliities, the molecular kinetics are not reproduced correctly, because the random collisions decorrelate the system.\cite{shell2019lecturenotes}

\subsubsection{Nosé-Hoover Thermostat}

Nosé augmented the Hamiltonian with two extra degrees of freedom representing an imaginary heat bath:

\begin{equation}
\label{nose_hamiltonian}
 H_{\text{Nosé}} = \sum_i^N \Big( \frac{p_i^2}{2 m_i s^2} \Big) + U(\mathbf{r}) + \frac{p_s^2}{2Q} + (3N + 1) \frac{ln(s)}{k_b T}
\end{equation}

Here, $s$ is the position and $p_s$ is the momentum of the heat bath.\cite{frenkel2001understanding} The parameter $Q$ is an effective mass associated with $s$, i.e. $p_s = Q \frac{ds}{dt}$ and its magnitude determines the coupling between the heat bath and the original system. It has to be chosen carefully by the user, as it influences the temperature fluctuations.\cite{Huenenberger2005} Using the Lagrangian, it can be shown that the particles are coupled to the heat bath by scaling the momenta:

\begin{equation}
 p_i = m_i v_i \times s
\end{equation}

The Hamiltonien in Eq. \ref{nose_hamiltonian} can then be used to derive the equations of motions for the extended system, i.e. for both, the heat bath and the original system. Note, that this approach is deterministic as no stochastic element is present. This thermostat generates the correct thermodynamics for the canonical ensemble.\cite{shell2019lecturenotes}

However, scaling of the particle momenta using the position of the imaginary heat bath also implies scaling of the timescale in the extended system.\cite{shell2019lecturenotes} Since the position $s$ is variable, the implied timescale also changes making it difficult to implement the Nose thermostat. To solve this issue, Hoover proposed an alternative by replacing the heat bath momentum $p_s$ with a friction coefficient $\xi = \frac{d ln(s)}{dt}$:

\begin{equation}
 H_{\text{Nosé-Hoover}} = \sum_i^N \Big( \frac{p_i^2}{2 m_i s^2} \Big) + U(\mathbf{r}) + \frac{\xi^2 Q}{2} + 3N k_B T ln(s)
\end{equation}

This approach is known as the Nosé-Hoover thermostat and the modified Hamiltonian yields equations of motion that no longer require a scaling of the time step but still enables to correctly generate a canonical ensemble through MD simulation.\cite{shell2019lecturenotes}

\subsubsection{Parinello-Rahman Barostat}

Similarly to thermostats, barostats are used to maintain constant pressure during the simulation. Again, several different techniques exists and in the following the popular Parinello-Rahman barostat is introduced as an example. 

In its core, the Parinello-Rahman barostat is similar to the Nose-Hoover thermostat, but this time an imaginary pressure bath is couples to the original system instead of a heat bath. The resulting Hamiltonian is 

\begin{equation}
 H_{\text{Parinello-Rahman}} = \sum_i^N \Big( \frac{p_i^2}{2 m_i} \Big) + U(\mathbf{r}) + \sum_j \mathbf{P}_jj V + \sum_{j,k} \frac{1}{2} \mathbf{W}_{jk} \Big( \frac{d b_{jk}}{dt} \Big) .
\end{equation}

Here, $\mathbf{b}$ is a matrix containing the box vectors and $V$ is the volume of the simulation box, $\mathbf{P}$ is the instantenous pressure tensor and $\mathbf{W}$ is the mass parameter matrix.\cite{abraham2014gromacs} The vector of the simulatiin box $b$ is coupled to the pressure bath with the relationships

\begin{equation}
 \frac{db^2}{dt^2} = V \mathbf{W}^{-1 }\mathbf{b}^{'-1}(\mathbf{P}-\mathbf{P}_{ref}) ,
\end{equation}

where $\mathbf{P}_{ref}$ is the reference pressure.\cite{abraham2014gromacs}

Typically, The Nosé-Hoover thermostat and the Parinello-Rahman barostat are used simultaneously yielding a Hamiltonien that includes both, a coupling to a heat bath and a pressure bath. Therefore, the resulting equations of motions allow to sample from the isothermal-isobaric ensemble (NPT).

% -----------------------------------
%   Coarse-Graining
% -----------------------------------

\section{Coarse-Graining}

Coarse-Graining is the process of building a simplified model of a complex system. To this end, a coarse-grained model represents the system at a lower level of resolution, i.e. it reduces the number of degrees of freedom. Obviously, the goal is to keep essential features while ignoring or averaging over less important details. In other words, the simplified model still has to maintain the correct physical behaviour. The benefits of coarse-grained models are twofold: Firstly, the reduced representation allows to access longer length an time scales, as the computational cost is reduced as well. Secondly, coarse-graining helps to put the essential features driving the emergend phenomena of interest into the spot light, as disturbing and unnecessary details are removed.

\subsection{Representation}

The basis of a coarse-grained model is the representation of the particles captured in the system. The most fundamental model we could apply is a quantum mechanical description. In this regard, a classical atomistic description is already a coarse-grained model based on ab initio considerations. However, coarse-graining typically refers to an even lower resolution description, where the coarse-grained sites, often called beads, represent multiple atoms. Typically, the beads are associated with specific types reflecting the physiochemical properties of the corresponding group of atoms. Moreover, bonds between those CG beads are introduced to capture the molecular topology. The choice of mapping from the atomistic to the lower resolution representation is crucial and has to be done with great care, as the essential features of the original system still have to be captured by the coarse-grained representation. However, in many cases the mapping is based on the chemical intuition of the user, but more systematic methods have been developed recently.\cite{chakraborty2018encoding, giulini2020information}

It is often required to not only specify the representation but also define a concrete mapping for the coordinates, i.e. a function $\mathbf{M}$ of the atomistic coordinates $\mathbf{r}$ to the coordinates of the CG beads $\mathbf{R}$. Typically, a linear mapping is chosen,

\begin{equation}
  \mathbf{R}_I = \sum_{i \in \phi(I)} c_{iI} \mathbf{r}_i ,
\end{equation}

where $I$ and $i$ are the indices of the CG bead and atom respectively, $\phi(I)$ is the set of indices of atoms the CG bead $I$ corresponds to and $c_{iI}$ are coefficients of the mapping. In many cases, the coordinate mapping is chosen such that it reflects the center of mass geometry of the group of atoms. 

\subsection{Top-down vs Bottom-up}

Once the representation of the coarse-grained model is defined, the forcefield has to be determined. For this task, a wide range of different schemes have been developed and two schools of thoughts have been established, refered to as bottom-up and top-down approaches. Note, that while a distinction between those two approaches is instructive, a clear classification is not always possible as many coarse-graining schemes incorporate aspects both ideas.

The bottom-up approach utilizes a more detailed model and aims at reproducing the thermodynamic and structural properties of the higher resolution system as closely as possible. In general, the choice of the underlying fine-grained model is not bound to a specific resolution. A common choice is to use the classical atomistic model as a basis. In this case, the accuracy of the coarse-grained model depends on the quality of the fine-grained model, as the atomistic model itself is a approximation of the quantum mechanical description. Once the high-resolution model is chosen, statistical mechanics provides a framework to rigorously derive the forcefield for the coarse-grained system. In theory, the many-body potential of mean force (PMF, see below) is central for this derivation. However, practical application of the PMF is provides significant challenges as it is extremely costly to evaluate for high dimensional systems.

While the spirit of bottom-up approaches is related to inductive reasoning, i.e. generalizing from something that is more specific, the top-down approach can be associated with deduction, i.e. applying general rules to derive a conclusion. Top-down coarse-graining starts with universal physical principles or experimentally observed phenomenas that the model aims to reproduce. In general, top-down models are used to study the consequences of the rules incorporated in the model. In this regard, top-down models are often not chemically-specific, as the set of rules yielding a specific phenomena might not be unique. Furthermore, it is often not sufficient to link a single phenomena to a specific chemical compound, as a full range of properties have to be reproduced to characterize the compound with high confidence. In this sense, top-down models are often under-constrained.

%While a coarse-grained model implies the existence of more fundamental, higher resolution model, the question of how the coarse-grained model is built upon this fine-grained 

\cite{noid2013perspective}

\cite{noid2008multiscale}

\subsection{Consistency Criteria and the Many-Body Potential of Mean Force}

Deriving the coarse-grained potential $U(\mathbf{R})$ for the coarse-grained model is the most challenging task. In the case of bottom-up coarse-graining, the potential $U(\mathbf{R})$ can theoretically be derived exactly from the fine-grained potential $u(\mathbf{r})$ and the mapping $\mathbf{M}(\mathbf{r})$. The underlying criteria for the derivation is called consistency criteria and states that the equilibrium joint probability density $p_{CG} (\mathbf{R}, \mathbf{P})$ in phase space of the coarse-grained coordinates $\mathbf{R}$ and momenta $\mathbf{P}$ have to match the implied atomistic probability density $p_{AA} (\mathbf{R}, \mathbf{P})$.\cite{noid2008multiscale} For simplicity, the following considerations are restricted to the configuration space, i.e. exluding momenta, such that the consistency criteria can be written as

\begin{equation}
\label{consistency1}
  p_{CG} (\mathbf{R}) = p_{AA} (\mathbf{R}) ,
\end{equation}

where $p_{CG} (\mathbf{R})$ is is the equilibrium probability density for a configuration $\mathbf{R}$ in the canonical ensemble of the coarse-grained model

\begin{equation}
\label{consistency2}
  p_{CG} (\mathbf{R}) \propto \text{exp} \Big[ - \frac{U(\mathbf{R})}{k_B T} \Big]
\end{equation}

and $p_{AA} (\mathbf{R})$ is the equilibrium probability density for a coarse-grained configuration $\mathbf{R}$ implied by the mapping $\mathbf{M}(\mathbf{r})$ expressed in terms of the fine-grained model

\begin{equation}
\label{consistency3}
  p_{AA} (\mathbf{R}) \propto \mathcal{Z}(\mathbf{R}) := \int  \text{exp} \Big[ - \frac{u(\mathbf{r})}{k_B T} \Big] \delta(\mathbf{M}(\mathbf{r}) - \mathbf{R}) d\mathbf{r} .
\end{equation}

Plugging Eq. \ref{consistency2} and \ref{consistency3} into \ref{consistency1} and reordering yields

\begin{equation}
\label{mbpmf}
 U(\mathbf{R}) = - k_B T \; \text{ln} (\mathcal{Z}(\mathbf{R})) + \text{const} .
\end{equation}

Eq. \ref{mbpmf} defines the many-body PMF as a projection of the free energy function onto the coarse-grained degrees of freedom. It assigns a weight to each coarse-grained configuration associated with the sum of all the Boltzmann weights for the corresponding atomistic configurations. Recalling the free energy  $F$

\begin{equation}
  F = U - TS = - k_B T \; \text{ln}(\mathcal{Z}) ,
\end{equation}

where $U$ is the internal energy, makes it clear that the many-body PMF is not a regular potential, as it contains both, energetic as well as entropic contributions. Moreover, as the name suggests, this potential generates the average atomistic forces associated with the atomistic configurations that map to the specific coarse-grained configuration. 

Importantly, $U(\mathbf{R})$ typically introduces many-body interactions that can not be expressed with simple classical force fields as outlined in Sec. \ref{molecular_forcefield}. This is a direct consequence for integrating over degrees of freedom. Additionally, it is extremely costly to evaluate and becomes infeasible to compute for most systems. Therefore, a wide range of techniques have been developed for a tractable approximation of the many-body PMF. 

\cite{frenkel2001understanding}, \cite{noid2008multiscale}

\subsection{Overview of Bottom-Up Techniques and Top-Down Force Fields}

\subsubsection{Direct Boltzmann Inversion}

The most simple approach to obtain an approximate coarse-grained potential is direct Boltzmann inversion (DBI). This approach aims at reproducing certain structural distributions computed from atomistic reference data that is mapped onto the coarse-grained degrees of freedom. The distribution functions for given interactions $\xi$ are denoted with $p_{\xi}(x)$, where $x$ is a scalar variable, such as pairwise distances, angles or dihedrals. The goal is then to find the corresponding potential $U_{\xi}$ that yields the desired distribution. Based on the assumption, that the distribution functions for different mechanical variables $x$ factorize, the probability distributions can be written as Boltzmann factors

\begin{equation}
  p_{\xi}(x) \propto \text{exp} \Big[ \frac{- U_{\xi}(x)}{k_B T} \Big] .
\end{equation}

In addition, the corresponding volume elements for each distribution has to be taken into account, i.e. the Jacobian element $J_{\xi}(x)$. The potential can then directly computed as 

\begin{equation}
  U_{\xi} (x) = -k_b T \text{ln} \Big( \frac{p_{\xi}(x)}{J_{\xi}(x)} \Big) .
\end{equation}

Note, that factorizing the probability distributions is a very severe approximation. Therefore, this approach yields accurate results only for interactions that can be regarded as isolated in the coarse-grained model. However, in cases where the coupling between the interactions can not be ignored, DBI yields inaccurate potentials that do not corretly reproduce the sturctural distributions as important cross-correlations are not taken into account.

\cite{tschop1998simulation}

\subsubsection{Iterative Boltzmann Inversion}

To improve the potentials derived with DBI, an iterative scheme can be applied. This scheme is called iterative Boltzmann inversion (IBI) and consists of the following steps: (1) Initial potentials are derived using DBI. (2) Use the derived coarse-grained potentials in a MD simulations to compute the corresponding structural distributions $p_{\xi}(x|U)$. (3) Update the coarse-grained potentials via

\begin{equation}
  U_{\xi, \text{new}} (x) = U_{\xi, \text{old}} (x) - k_B T \text{ln} \Big( \frac{p_{\xi}(x)}{p_{\xi}(x|U)} \Big) .
\end{equation}

Steps (2) and (3) are repeated until the potentials converge. Despite its simplicity, IBI has become very popular and is used especially for complex liquids and polymers. While IBI still treads every interaction $\xi$ seperatly, it implicitly accounts for correlations between the interactions through the iterative scheme. However, convergence of the potentials is not guaranteed.

\cite{reith2003deriving}

\subsubsection{Multiscale Coarse-Graining}

\cite{noid2008multiscale}

\subsubsection{Relative Entropy}

\cite{shell2008relative}, \cite{chaimovich2010relative}, \cite{chaimovich2011coarse}

\subsubsection{Martini Force Field}

\cite{marrink2007martini}, \cite{marrink2013perspective}

\subsubsection{Kremer-Grest Polymer Model}

\cite{kremer1990dynamics}, \cite{grest1986molecular}, \cite{faller1999local}, \cite{everaers2020kremer}

% -----------------------------------
%   Backmapping
% -----------------------------------

\section{Backmapping}

\subsection{The Challenges of Reintroducing Degrees of Freedom}

\cite{tschop1998simulation}

\subsubsection{Consistency with Coarse-Grained Mapping}

\subsubsection{One-to-Many Mapping}

\subsection{Overview of Existing Approaches}

\subsubsection{Generic Approaches}

\cite{rzepiela2010reconstruction}, \cite{wassenaar2014going}

\subsubsection{Fragment-based Approaches}

\cite{peter2009multiscale}, \cite{zhang2019hierarchical}, \cite{hess2006long}, \cite{brasiello2012multiscale}

\subsubsection{Machine Laarning Approaches}

\cite{wang2019coarse}, \cite{li2020backmapping}
