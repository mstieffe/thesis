
\chapter{Multiscale Modeling} % Main chapter title

Condensed matter can be studied at various levels of resolution ranging from a classical thermodynamic description of macroscopic quantities to a fundamental, quantum mechanical description that takes electronic degrees of freedom into account. Ideally, all emergend phenomenas of matter should be treated with ab initio methods, i.e. methods based on first principles. However, even performed on modern supercomputers, ab initio molecular dynamics simulations quickly reach their limits and are currently restricted to systems involving a few thousands of atoms.\cite{lahnsteiner2016room, paquet2018computational} Therefore, it is often necessarry to resign to a coarser description of matter in order to push the limits of length and time scales reachable with computer simulations. 

In general, the actual choice of resolution depents on the length and time scale of the phenomena of interest. The applied model has to be detailed enough to capture locally relevant length scales, while still providing the capability to study the phenomena in its whole extent. Often, it is not possible to capture all the relevant scales in a single model. This is especially true for soft matter systems, where processes occuring on atomistic length and time scales can impact meso- to macroscopic changes. The reason for this is a rather low characteristic energy scale of soft matter systems that is in the order of magnitude of the thermal energy $k_b T$.\cite{praprotnik2008multiscale, peter2010multiscale, peter2009multiscale} Therefore, entropic contributions to the free energy due to large scale conformational and structural changes can be in the same order of magnitude as local interactions. Thus, soft matter systems are charaterized by large thermal fluctuations. Consequently, a thorough exploration of soft matter systems demands for methods that allow to capture the interplay of processes that are inherently linked to various different scales.

Multiscale Modeling (MM) offers a solution to study phenomenas that require a description on multiple levels of resolution. It refers to methods that include multiple models of various resolution providing the ability to address the phenomena at different length and time scales. The models can be linked or combined following different strategies: In the sequential approach, models are treated seperatly and information is passed between them without directly influencing each other, wheres hybrid methods provide a direct interaction between models to use different resolutions simultanousely.\cite{ayton2007multiscale, voth2008coarse, peter2010multiscale} Alternatively, the resolution of single molecules can be changed adaptively during the course of the simulation.\cite{praprotnik2005adaptive, praprotnik2007macromolecule}

This chapter gives an introduction to MM and is organized as follows: At first, basics of thermodynamics and statistical mechanics are recalled followed by a review of Molecular Dynamics simulations. Afterwards, a section about coarse-graining outlines strategies to reduce the resolution. Finally, the inverse problem, i.e. increasing the resolution, is introduced to motivate the main theme of this thesis.

\label{theory_ms} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}



% -----------------------------------
%   Statistical Mechanics
% -----------------------------------

\section{Thermodynamics and Statiscal Mechanics}

%\subsection{Microstates vs Macrostates}
\textit{Classical thermodynamics} describes the behaviour of bulk, macroscopic systems in terms of a few macroscopic quantities, such as the total internal energy $E$, the total volume $V$, and the number of particles $N$. Typically, the system is considered at the \textit{thermodynamic equilibrium}, where its properties do not change with time and the actual state is history-independent. 

The basic concepts of classical thermodynamics were developed before the molecular nature of matter was generally accepted. Therefore, it is not surprising that classical thermodynamics is concerned with laws and relationships exclusively for macroscopic quantities without referencing to a more fundamental description on the molecular level. In fact, the laws of classical thermodynamics are based only on a few postulates.\cite{shell2015thermodynamics}

The molecular underpinning of thermodynamics was developed in the field of \textit{statistical mechanics}, where all the microscopic details of individual molecules are taken into account. For example, in the classical picture, a list of positions $\mathbf{r} \in \mathrm{R}^{3N}$ and momenta $\mathbf{p} \in \mathrm{R}^{3N}$ of the atoms are considered, whereas a quantum mechanical description uses quantum states. In the following, the classical picture is used for simplicity and a microscopic state $m = (\mathbf{r}, \mathbf{p})$ is characterized by its $6N$ degrees of freedom, i.e. a point in the $6N$ dimensional \textit{phase space}.

\subsection{The Mircrocanonical Ensemble and the Principle of Equal a Priori Probabilities}

In statistical mechanics, macroscopic quantities measured at equilibrium are described as the average behaviour of many particles. For an isolated system, i.e. a system that can not exchange energy or particles with its surrounding at fixed volume, a macrostate is completely specified by $(E, V, N)$, which remain constant throughout molecular motion.\cite{shell2015thermodynamics} Note that the temperature $T$ and pressure $P$ are not necessary to specify the macroscopic state, as their values can be derived once the vales for $(E, V, N)$ are set. For each macrostate $(E, V, N)$, a collection of possible microstates can be found, i.e. a surface in the phase space of $N$ atoms with constant total energy $E$ at a volume $V$. This collection of microstates together with their associated probabilities is called the \textit{microcanonical ensemble}.

The positions and velocities of the atoms constantly vary under the influence of their mutual interactions. Therefore, the microstate changes constantly even if the macrostate stays fixed. The likelihood that a microstate will be visited by the system is denoted with $p_m$ and the microstate probabilities do not change with time at equilibrium. A cornerstone of statistical mechanics is the statement that the system has no preference for a certain microstate and hence, each microstate is equally likelily.\cite{shell2015thermodynamics} This fundamental rule is called the \textit{principle of equal a priori probabilities}. It allows to write the likelihood in the canonical ensemble as

\begin{equation}
  p_m = \begin{cases}
    \frac{1}{\Omega(E,V,N)} \;\;\;\; & \text{if} \; E_m \neq E \\
    0 \;\;\;\; & \text{if} \; E_m \neq E
  \end{cases} ,
\end{equation}

where $\Omega(E,V,N)$, called the \textit{density of states}, is a function describing the number of accessible microstates for particular $(E,N,V)$.\cite{shell2015thermodynamics}

\subsection{Different views on the Entropy}

A central theme common for thermodynamics, statistical mechanics as well as information theory is the concept of \textit{entropy}.
In classical thermodynamics, the entropy $S$ is regarded as a non-conserved state-function that emerges naturally for systems in equilibrium.\cite{shell2015thermodynamics} It is a function

\begin{equation}
  S = S(E,V,N)
\end{equation}

dependent on the macroscopic quantities $E$, $V$ and $N$. Allowing for heat, volume or mass transfer, the system can change its equilibrium macrostate to another macrostate. This is called a \textit{thermodynamic process}. Historically, entropy was introduced to explain why some thermodynamic processes are irreversible, i.e. the process occurs spontaneously in one direction whereas the reverse does not, although both directions obey the conversation of energy.\cite{simon1997physical} The reason for this is that thermodynamic systems tend to progress towards states with increasing entropy. This is stated in the second law of thermodynamics: The entropy of an isolated system can not decrease as it always evolves to an equilibrium state where the entropy is highest.\cite{shell2015thermodynamics} 

While the specific form of the entropy function is different for every system, all entropy functions have some shared properties. A very important one is the total differential

\begin{equation}
  dS = \frac{1}{T}dE + \frac{P}{T}dV - \frac{\mu}{T}dN ,
\end{equation}

which relates the temperature $T$, the pressure $P$ and the chemical potential $\mu$ to derivatives of the same function and therefore defines a relationships between these variables.\cite{shell2015thermodynamics}

The above definition for the entropy $S$ is exclusively based on macroscopic properties. Boltzmann was the first who gave a definition for the entropy based on microscopic considerations and therefore introduced a connection of thermodynamics to the molecular nature of matter.\cite{shell2015thermodynamics} His famous formula reads

\begin{equation}
\label{boltzmann_entropy}
  S = k_B ln( \Omega(E,V,N) ) ,
\end{equation}

where $k_B$ is proportionality constant, called \textit{Boltzmann's constant}. Eq. \ref{boltzmann_entropy} links the entropy $S$ to the number of accessible microstates for given macrostate. Therefore, the second law of thermodynamics can be interpreted as the tendency of a system to evolve to a state that maximizes the number of accessible microstates.

Based on Boltzmann's equation, Gibbs introduced a more general form of the entropy 

\begin{equation}
\label{gibbs_entropy}
  S = k_B \sum p_m ln( p_m )  ,
\end{equation}

which is equivalent, up to the Boltzmann constant, to the definition of the information entropy by Shannon. Note, that upon application of the principle of equal a priori probailities, i.e. $p_m = \frac{1}{\Omega(E,V,N)}$, the entropy is maximized and Gibbs formulation of the entropy recovers Boltzmann's equation. 

%In this regard, the entropy can interpreted as a measure of uncertainty. Moreover, according to Jayne, the thermodynamic entropy can be regarded as an application of Shannon's information theory: The entropy $S$ expresses the missing information needed to determine the specific microstate of a system that remains uncommunicated due to a description of the system solely in terms of macroscopic quantities. From this point of view, the principle of equal a priori probabilities expresses our ignorance about the microscopic details of the system as it chooses the distribution that maximizes the entropy. 

\subsection{The Canonical Ensemble and the Boltzmann Distribution}

Central to the previous considerations was the ensemble for an system in isolation, i.e. the microcanonical ensemble for fixed $(E,V,N)$. In the following, the \textit{canonical ensemble} is introduced describing a system that is not isolated but at constant temperature. To this end, the system is considered to be in thermal contact with an infinitely large heat bath with a fixed temperature $T$. Therefore, the fixed macroscopic quanties of the system are $(T,V,N)$, while the total energy $E$ is allowed to fluctuate. The composite of the system and the heat bath is again considered to be an isolated system. However, summing over all the microstates of the heat bath allows to derive the probabilities for the microstates $m$ of the system of interest. Importantly, these microstate probabilities are no longer equal but depend on their total energy $E_m$. More specifically, the microstate probabilities can be written as

\begin{equation}
\label{boltzmann_dstr}
  p_m = \frac{e^{- \frac{E_m}{k_B T}}}{Z} ,
\end{equation}

where the normalization constant 

\begin{equation}
  Z = \sum_{\text{all }\, m \, \text{at}\, V,N} e^{- \frac{E_m}{k_B T}}
\end{equation}

is called the \textit{canonical partition function} and the probability distribution in Eq. \ref{boltzmann_dstr} is refered to as \textit{Boltzmann distribution}.\cite{shell2015thermodynamics} Note, that similarly to the microcanonical distribution, the Boltzmann distribution maximizes the entropy for a given macrocospic state $(T,V,N)$. In general, the canonical ensemble is more useful than the microcanonical ensemble in practice, since in most cases systems in thermal equilibrium with their surroundings are considered.

\subsection{Thermodynamic Limit and Statistical Equivalence of Ensembles}

The canonical approach provides an alternative, in addition to the microcanonical approach, to determine the behaviour of a system at a microscopic level. While there are rigorously no fluctuations in the energy in the microcanonical ensemble, energy fluctuates in the canonical ensemble but the temperature is rigorously constant. However, in the \textit{thermodynamic limit}, i.e. when the number of particles and the volume of the system go to inifinity $N \rightarrow \infty$, $V \rightarrow \infty$ while the particle density is held fixed $\frac{N}{V} = \text{constant}$, the differences in macroscopic properties for both ensembles vanish.\cite{shell2015thermodynamics} 

This can be seen clearly considering the distribution of the energy in the canonical ensemble. Using Eq. \ref{boltzmann_dstr} and \ref{boltzmann_entropy} the propability for a specific energy $E$ can be written as

\begin{equation}
  p(E) \propto \Omega(E,V,N) e^{- \frac{E_m}{k_B T}} = e^{\frac{1}{k_B} ( S(E,V,N) - \frac{E_m}{T} ) } .
\end{equation}

This equation shows that two competing terms have to be considered for the probability of energy levels: The first term is the entropy $S$, which is a concave, increasing function of $E$.\cite{shell2015thermodynamics} The second term $-\frac{E}{T}$ decreases linearly with the energy $E$. Therefore, the probability distribution for the energy levels has a maximum at an intermidiate energy $E^*$. Both terms, $S$ and $E$, are extensive quantities, i.e. they scale as $N$. Since the competing terms are within the exponential, the probability distribution becomes sharply peaked at the maximum $p(E^*)$. Therefore, $p(E^*)$ becomes the most dominant term and the impact of microstates with different energies $E \neq E^*$ vanishes.\cite{shell2015thermodynamics}

Another way to view this, is that fluctuations in the total energy $E$ become extremely small in the thermodynamic limit. The variance of the total energy $\sigma_E^2 = <E^2> - <E>^2$ can be linked to the heat capacity, which is an extensive quantity. Therefore, the relative magnitude of energy fluctuations scales as

\begin{eqnarray}
  \frac{\sqrt{\sigma_E^2}}{E} \propto N^{-1/2} .
\end{eqnarray}

As a consequence, a macroscopic systems appear to have constant energies.\cite{shell2015thermodynamics}

\subsection{Information-theoretic View on Statistical Mechanics}

In 1957 Jaynes published two papers emphazising the correspondence between information theory and statistical mechanics.\cite{jaynes1957information,jaynes1957information2} According to Jaynes, Gibbs' entropy in statistical mechanics and Shannon's information entropy are identical except for the Boltzmann constant.\cite{jaynes1957information} Consequently, statistical mechanics can be viewed from the perpespective of information theory and the seek for microscopic distributions can be treated as an inference problem.\cite{jaynes1957information} 

Jaynes adapted a bayesian perpective (see Sec. \ref{bayes_vs_frequentist}) and treated the testable information, i.e. the macroscopic observables, as prior information.\cite{jaynes1957information} Obviously, information is missing that is needed to determine the specific microstate of a system that remains uncommunicated due to a description of the system solely in terms of macroscopic quantities. Consequently, finding the microscopic probability distribution amounts to assign probabilities to microstates that avoid bias while still aggreeing with the observed macroscopic quantities.\cite{jaynes1957information} In other words, out of all possible distributions the one has to be chosen which is the least-informative to avoid arbitrary assumptions.

Shannon has showed, that the entropy written in Eq. \ref{gibbs_entropy} (without the Boltzmann constant) is a quantity that increases with increasing uncertainty.\cite{shannon1948mathematical} Consequently, the distribution that maximizes the entropy has to be found under the constraints of the given prior information. From this point of view, the principle of equal a priori probabilities is a consequence of our ignorance about the microscopic details of the system. In practice, the problem is solved introducing Lagrange multiplier to impose the constraints. For the microcanonical ensemble, the constraint renders to a zero probability for all microstates with different total energy than the observed one. In the canonical ensemble, the constraint is a fixed expectation value for the energy.\cite{jaynes1957information} In both cases, the well known results from statistical mechanics are reproduced, i.e. the uniform distribution for the microcanonical ensemble and the boltzmann distribution for the canonical ensemble.

% -----------------------------------
%   Molecular Dynamics Simulation
% -----------------------------------

\section{Molecular Dynamics Simulation: Sampling from the Thermodynamic Ensembles}

In many cases, it is possible to express the basic laws of nature in terms of relatively simple equations. Unfortunately, solving such equations analytically, such as the equations of motion for more than two interacting bodies, becomes impossible in many cases. However, it is central to the statistical mechanics view on thermodynamics to consider systems with an extremely large number of interacting particles. 
Fortunately, Computer simulations can circumvent these issues. The two main branches of computer simulation techniques for molecular systems are molecular dynamics (MD) and Monte Carlo (MC), but a whole range of hybrid techniques exists as well. In the following, the focus is set on MD simulations.

\textit{Molecular Dynamics} (MD) simulation numerically integrats Newtons equations of motion allowing to analyze the physical movement of atoms or molecules and predict bulk properties. The starting point for every MD simulation is to set up a model for the molecular interactions, i.e. the forcefield, and to select initial positions and velocities for the particles. Afterwards, the system is evolved in time using descrete timesteps where the particle positions and velocities are repeatly updated. To this end, the forces have to be calculated at every step. However, the algorithm and parameters for the numerical intergration have to be taken with great care in order to minimize the effects of cumulative errors.\cite{frenkel2001understanding} After the system is equilibrated, i.e. it has evolved for a sufficient amount of time such that macroscopic proties do not change anymore, snapshots or whole trajectories of the system can be extracted and the actual measurement of the observable quantity of interest can be performed. In this regard, MD simulation builts a bridge between theory and experiment, as it allows to test a model and compare it with experimental results.\cite{allen2004introduction}

The following introducting to MD is based on the book by Frenkel and Smit and the lecture notes of M. Scott Shell, which are excellent sources providing the interested reader with more detailed explanations on this topic.\cite{shell2019lecturenotes, frenkel2001understanding}

\subsection{Molecular Force Fields}
\label{molecular_forcefield}

Calculating forces acting on the particles is crucial to evolve the system in time. To this end, a definition for the potential energy function is required describing the interactions between the particles. These potentials can be defined on vary different levels of resolution. In the following, the classical description is used, which ignores the motion of the electrons and focuses solely on the motion of the nuclei. However, MD simulations including electronic degrees of freedom are also possible.\cite{marx2000ab, lemkul2016empirical} In this regard, the classical description approximates the effect of electrons as a potential energy surface representing the quantum ground-state.\cite{shell2019lecturenotes} This description is reasonable in cases where the Born-Oppenheimer approximation is valid and the electronic structure is not of interest.\cite{shell2019lecturenotes} Furthermore, bond breaking or forming is prohebitted.\cite{shell2019lecturenotes}

Typically, the potential $U (\mathbf{r})$ is divided into a term representing the bonded interactions and a term for the nonbonded interactions \cite{shell2019lecturenotes}

\begin{equation}
  U = U_{\text{bonded}} + U_{\text{nonbonded}} .
\end{equation}

Most of the forcefields used in classic MD are emperical and are obtained by fitting simple analytic functions against experimental data or detailed electronic calculations, such as density-functional theory (DFT). 

\subsubsection{Bonded Interactions}

Bonded interactions are associated with chemical bonds, bond angles and bond dihedrals. A typical choice is to deploy harmonic and cosine potentials to model the bonded interactions, such as

\begin{equation}
  U_{\text{bonded}} = \sum_{\text{bonds}} a(d - d_0)^2 + \sum_{\text{angles}} b(\Phi - \Phi_0)^2 + \sum_{\text{dihedrals}} \Big( \sum_{n} c_n cos(\omega)^n \Big) ,
\end{equation}

where $d$ and $d_0$ are the calculated and the equilibrium bond length respectively, $\Phi$ and $\Phi_0$ the calculated and equilibrium bond angles and $\omega$ is the calculated dihedral angle.\cite{shell2019lecturenotes} The parameters $a$, $b$ and $c_n$ are the strength for the harmonic and the cosine series potential repectively. 

\subsubsection{Nonbonded Interactions}

The nonbonded potential is associated with van der Waals attraction, pauli repulsion and electrostatic interactions.\cite{shell2019lecturenotes} The van der Waals attraction arises due to the correlation between instantaneous dipoles between electron clouds of the atoms. The Pauli repulsion occurs upon the overlap of electron clouds and is a consequence of the Pauli principle, which forbids any two electrons from having the same quantum numbers. Both, the van der Waals attraction and the Pauli repulsion, are often combined into a single expression, such as the Lennard-Jones potential. The electrostatic forced arise due to partial or formal charges of the atoms and are taken into account through Coulomb's law. In combination, a typical nonbonded potential is modeled as

\begin{equation}
  U_{\text{nonbonded}} = \sum_{\text{pairs}} \Big( \underbrace{ 4 \epsilon \bigg[ \big( \frac{r_{ij}}{\sigma} \big)^{-12} - \big( \frac{r_{ij}}{\sigma} \big)^{-6} \bigg] }_{\text{Lennard-Jones}} + \underbrace{ \frac{q_i q_j}{4 \pi \epsilon_0 r_{ij}} }_{\text{Coulomb}} \Big) , 
\end{equation}

where $r_{ij}$ is the pairwise distance, $q_i$ and $q_j$ are the net (partial) charges, $\epsilon$ and $\sigma$ are the Lennard-Jones parameters, which depend on the particular atom types and $\epsilon_0$ is the electric permitivity in vacuum.\cite{shell2019lecturenotes} 

Obviously, calculating the nonbonded interactions is computationally more expensive compared to the bonded interactions, since the number of terms in the pairwise atomic sum scales as $N^2$, while the other scale as $N$. To reduce the computational overhead, the Lennard-Jones potential is often truncated as its contribution becomes minimal for large distances. To this end, a cutoff distance of $r_c \approx 2.5 \sigma$ is typically used where the energy is only a few percent of the minimum energy ($U_{\text{LJ}}(r_c) \approx -0.016 \epsilon$).\cite{shell2019lecturenotes} In addition, it is common practice to shift to potential to avoid discontinuities, i.e. subtracting the value of the potential at the cutoff. However, the truncated contributions can become significant for the total energy and pressure of the system. To this end, a correction to the total potential can be introduced, which is derived analytically for isotropic systems.\cite{shell2019lecturenotes, frenkel2001understanding}  On the other hand, the long-range Coulomb interaction need a special treadment, as a a a tail correction can not be derived directly.\cite{shell2019lecturenotes} Therefore, Ewald summation is typically used to reduce the computational effort. In this case, the potential is split into a short-range and long-range contribution. The short-range contributions are computed in real space, while the long-range contributions are computed in Fourier space. 

\subsection{Numerical Integration}
\label{numerical_integration}

The time evolution of the molecular system is described by Newton's equations of motion: Given the potential energy function $U(\mathbf{r})$ the forces $m \frac{d^2 \mathbf{r}}{d t^2}$ acting on the $N$ atoms are computed as

\begin{equation}
\label{newtons_eq_motion}
  m \frac{d^2 \mathbf{r}}{d t^2} = - \frac{d U (\mathbf{r})}{d \mathbf{r}} .
\end{equation}

Starting from initial positions and velocities, the time-evolution of the system traces a path in phase space, called trajectory. This trajectory is a set of states compatible with the starting condition. For classical systems, the phase space trajectory lies on a surface of constant energy, as Newton's equations conserve energy.\cite{shell2019lecturenotes} For ergodic systems, the trajectory will eventually visit all points in phase space compatible with the given total energy, whereas systems that are non-ergodic have areas in phase space that are inacessible. Obviously, it is not possible to sample all states of the trajectory. However, MD simulations aim at sampling the accessible phase space representatively.

Note that Eq. \ref{newtons_eq_motion} is a set of $3N$ second-order, nonlinear, coupled partial differential equations, which can not be solved analytically.\cite{shell2019lecturenotes} Therefore, numerical integration is used to evolve the system in time. Here, the basic idea is to introduce a small timestep $\delta t$ and find the positions of the atoms at consecutive time steps, i.e. 

\begin{equation}
  \mathbf{r}(0), \mathbf{r}(\delta t), \mathbf{r}(2 \delta t), ..
\end{equation}

Many different algorithms exist to progate the system forward in time. As an example, the Verlet algorithm is explained in the following.

\subsubsection{Verlet Algorithm}

Using a Taylor expansion, the position at time $t + \delta t$ can be written as

\begin{equation}
\label{verlet_taylor1}
  \mathbf{r}(t + \delta t) = \mathbf{r}(t) + \frac{d \mathbf{r}(t)}{d t} \delta t + \frac{d^2 \mathbf{r}(t)}{d t^2} \frac{ \delta t^2}{2} + \frac{d^3 \mathbf{r}(t)}{d t^3} \frac{ \delta t^3}{6} + \mathcal{O}(\delta t^4) .
\end{equation}

Similarly, the position at the previous time step can be written as

\begin{equation}
\label{verlet_taylor2}
  \mathbf{r}(t - \delta t) = \mathbf{r}(t) - \frac{d \mathbf{r}(t)}{d t} \delta t + \frac{d^2 \mathbf{r}(t)}{d t^2} \frac{ \delta t^2}{2} - \frac{d^3 \mathbf{r}(t)}{d t^3} \frac{ \delta t^3}{6} + \mathcal{O}(\delta t^4) .
\end{equation}

Adding Eq. \ref{verlet_taylor1} and \ref{verlet_taylor2} and rearranging leads to

\begin{equation}
\label{verlet}
  \mathbf{r}(t + \delta t) = 2 \mathbf{r}(t) - \mathbf{r}(t - \delta t) + \frac{d^2 \mathbf{r}(t)}{d t^2} \frac{ \delta t^2}{2} + \mathcal{O}(\delta t^4) .
\end{equation}

Eq. \ref{verlet} allows to compute the positions at the next time step from the positions at the two previous timesteps and the forces, which can be calculated using Eq. \ref{newtons_eq_motion}. The error of this algorithm is of order $\mathcal{O}(\delta t^4)$ and therefore decreases with the time step.

Note that the Verlet algorithm has two important features: It is time-reversible and symplectic, i.e. volume in phase space is preserved.\cite{allen2004introduction} Those features are crucial to maintain correct statistical sampling and stability, as those are properties of the true Hamiltonian dynamics.\cite{shell2019lecturenotes} Algorithms that do not preserve the volume in phase space can dramatically expand the initial volume, such that it eventually covers areas of phase space that are not compatible with the starting condition and might violate energy conservation. Although reversiblity and the conservation of phase space volume does not automatically guarantee that there is no drift of the total energy on a long time scale, it is at least a reasonable requirement.\cite{frenkel2001understanding} In practice, the Verlet algorithm does not exactly conserve the total energy, but it exhibits little long energy drifts.\cite{frenkel2001understanding} Note, that there a many different algorithm that can be derived from the Verlet algorithm yielding identical trajectories, such as the leap frog or the velocity Verlet algorithm.\cite{frenkel2001understanding}

\subsection{Thermostats and Barostats: Controlling Temperature and Pressure}

The numerical integration scheme described in Sec. \ref{numerical_integration} allows to sample from the microcanonical ensemble, i.e. maintain a constant total energy. However, it is often desirable to sample from different ensembles, such as the canonical ensemble (NVT) or the isothermal-isobaric ensemble (NPT). Modifications on the MD algorithm that allow to control the temperature or pressure are called thermostat or barostat algorithms respectively. They are important for many reasons, for example to match experimental conditions, study temperature dependent processes or enhance the efficiency of conformational search.\cite{Huenenberger2005} Several methods exist to control temperature and pressure during the simulation. In the following, some of the popular techniques are introduced.

\subsubsection{Velocity Rescaling}

The simplest approach to control the temperature is the velocity rescaling algorithm. The temeprature is related to the kinetic energy and can be estimated as

\begin{equation}
 T = \frac{2 <K>}{k_B n_{DOF}} ,
\end{equation}

where $K$ is the kinetic energy and $n_{DOF}$ are the degrees of freedom.\cite{shell2019lecturenotes} Therefore, the velocities can be rescaled at each time step to fix the temperature to a desired value. Despite its simplicity, this algorithm does not reproduce the correct thermodynamic properties of the canonical ensemble, since the fluctuations in the kinetic energy are not captured.\cite{shell2019lecturenotes} 

\subsubsection{Anderson Thermostat}

The Anderson thermostat introduces random collision of the molecules with an imaginary heat bath at the desrired temperature. To this end, particles are chosen at random and their velocities are sampled randomly from the Maxwell-Boltzmann distribution:

\begin{equation}
 p(\vec{v}) = \Big( \frac{m}{2 \pi k_B T} \Big)^{3/2} \text{exp} \Big[- \frac{m |\vec{v}|^2}{2 k_B T} \Big]
\end{equation}

Although this approach generates the correct canonical ensemble propabliities, the molecular kinetics are not reproduced correctly, because the random collisions decorrelate the system.\cite{shell2019lecturenotes}

\subsubsection{Nosé-Hoover Thermostat}

Nosé augmented the Hamiltonian with two extra degrees of freedom representing an imaginary heat bath:

\begin{equation}
\label{nose_hamiltonian}
 H_{\text{Nosé}} = \sum_i^N \Big( \frac{p_i^2}{2 m_i s^2} \Big) + U(\mathbf{r}) + \frac{p_s^2}{2Q} + (3N + 1) \frac{ln(s)}{k_b T}
\end{equation}

Here, $s$ is the position and $p_s$ is the momentum of the heat bath.\cite{frenkel2001understanding} The parameter $Q$ is an effective mass associated with $s$, i.e. $p_s = Q \frac{ds}{dt}$ and its magnitude determines the coupling between the heat bath and the original system. It has to be chosen carefully by the user, as it influences the temperature fluctuations.\cite{Huenenberger2005} Using the Lagrangian, it can be shown that the particles are coupled to the heat bath by scaling the momenta:

\begin{equation}
 p_i = m_i v_i \times s
\end{equation}

The Hamiltonien in Eq. \ref{nose_hamiltonian} can then be used to derive the equations of motions for the extended system, i.e. for both, the heat bath and the original system. Note, that this approach is deterministic as no stochastic element is present. This thermostat generates the correct thermodynamics for the canonical ensemble.\cite{shell2019lecturenotes}

However, scaling of the particle momenta using the position of the imaginary heat bath also implies scaling of the timescale in the extended system.\cite{shell2019lecturenotes} Since the position $s$ is variable, the implied timescale also changes making it difficult to implement the Nose thermostat. To solve this issue, Hoover proposed an alternative by replacing the heat bath momentum $p_s$ with a friction coefficient $\xi = \frac{d ln(s)}{dt}$:

\begin{equation}
 H_{\text{Nosé-Hoover}} = \sum_i^N \Big( \frac{p_i^2}{2 m_i s^2} \Big) + U(\mathbf{r}) + \frac{\xi^2 Q}{2} + 3N k_B T ln(s)
\end{equation}

This approach is known as the Nosé-Hoover thermostat and the modified Hamiltonian yields equations of motion that no longer require a scaling of the time step but still enables to correctly generate a canonical ensemble through MD simulation.\cite{shell2019lecturenotes}

\subsubsection{Parinello-Rahman Barostat}

Similarly to thermostats, barostats are used to maintain constant pressure during the simulation. Again, several different techniques exists and in the following the popular Parinello-Rahman barostat is introduced as an example. 

In its core, the Parinello-Rahman barostat is similar to the Nose-Hoover thermostat, but this time an imaginary pressure bath is couples to the original system instead of a heat bath. The resulting Hamiltonian is 

\begin{equation}
 H_{\text{Parinello-Rahman}} = \sum_i^N \Big( \frac{p_i^2}{2 m_i} \Big) + U(\mathbf{r}) + \sum_j \mathbf{P}_jj V + \sum_{j,k} \frac{1}{2} \mathbf{W}_{jk} \Big( \frac{d b_{jk}}{dt} \Big) .
\end{equation}

Here, $\mathbf{b}$ is a matrix containing the box vectors and $V$ is the volume of the simulation box, $\mathbf{P}$ is the instantenous pressure tensor and $\mathbf{W}$ is the mass parameter matrix.\cite{abraham2014gromacs} The vector of the simulatiin box $b$ is coupled to the pressure bath with the relationships

\begin{equation}
 \frac{db^2}{dt^2} = V \mathbf{W}^{-1 }\mathbf{b}^{'-1}(\mathbf{P}-\mathbf{P}_{ref}) ,
\end{equation}

where $\mathbf{P}_{ref}$ is the reference pressure.\cite{abraham2014gromacs}

Typically, The Nosé-Hoover thermostat and the Parinello-Rahman barostat are used simultaneously yielding a Hamiltonien that includes both, a coupling to a heat bath and a pressure bath. Therefore, the resulting equations of motions allow to sample from the isothermal-isobaric ensemble (NPT).

% -----------------------------------
%   Coarse-Graining
% -----------------------------------

\section{Coarse-Graining}

Coarse-Graining is the process of building a simplified model of a complex system. To this end, a coarse-grained model represents the system at a lower level of resolution, i.e. it reduces the number of degrees of freedom. Obviously, the goal is to keep essential features while ignoring or averaging over less important details. In other words, the simplified model still has to maintain the correct physical behaviour. The benefits of coarse-grained models are twofold: Firstly, the reduced representation allows to access longer length an time scales. This is a direct consequence of the reduced molecular friction and smoothed energy landscape of the coarse-grained model, which effectively accelerates equlibration of the molecular system. In addition, the computational cost reduces with the number of degrees of freedom. Secondly, coarse-graining helps to put the essential features driving the emergend phenomena of interest into the spot light, as disturbing and unnecessary details are removed.

\subsection{Representation}

The basis of a coarse-grained model is the representation of the particles captured in the system. The most fundamental model we could apply is a quantum mechanical description. In this regard, a classical atomistic description is already a coarse-grained model based on ab initio considerations. However, coarse-graining typically refers to an even lower resolution description, where the coarse-grained sites, often called beads, represent multiple atoms. Typically, the beads are associated with specific types reflecting the physiochemical properties of the corresponding group of atoms. Moreover, bonds between those CG beads are introduced to capture the molecular topology. The choice of mapping from the atomistic to the lower resolution representation is crucial and has to be done with great care, as the essential features of the original system still have to be captured by the coarse-grained representation. However, in many cases the mapping is based on the chemical intuition of the user, but more systematic methods have been developed recently.\cite{chakraborty2018encoding, giulini2020information}

It is often required to not only specify the representation but also define a concrete mapping for the coordinates, i.e. a function $\mathbf{M}$ of the atomistic coordinates $\mathbf{r}$ to the coordinates of the CG beads $\mathbf{R}$. Typically, a linear mapping is chosen,

\begin{equation}
  \mathbf{R}_I = \mathbf{M}_{I}(\mathbf{r}) = \sum_{i \in \psi(I)} b_{iI} \mathbf{r}_i 
\end{equation}

where $I$ and $i$ are the indices of the CG bead and atom respectively, $\psi(I)$ is the set of indices of atoms the CG bead $I$ corresponds to and $b_{iI}$ are coefficients of the mapping. In many cases, the coordinate mapping is chosen such that it reflects the center of mass geometry of the group of atoms. 

\subsection{Top-down vs Bottom-up}

Once the representation of the coarse-grained model is defined, the forcefield has to be determined. For this task, a wide range of different schemes have been developed and two schools of thoughts have been established, refered to as bottom-up and top-down approaches. Note, that while a distinction between those two approaches is instructive, a clear classification is not always possible as many coarse-graining schemes incorporate aspects both ideas.

The bottom-up approach utilizes a more detailed model and aims at reproducing the thermodynamic and structural properties of the higher resolution system as closely as possible. In general, the choice of the underlying fine-grained model is not bound to a specific resolution. A common choice is to use the classical atomistic model as a basis. In this case, the accuracy of the coarse-grained model depends on the quality of the fine-grained model, as the atomistic model itself is a approximation of the quantum mechanical description. Once the high-resolution model is chosen, statistical mechanics provides a framework to rigorously derive the forcefield for the coarse-grained system. In theory, the many-body potential of mean force (PMF, see below) is central for this derivation.\cite{noid2013perspective} However, practical application of the PMF is provides significant challenges as it is extremely costly to evaluate for high dimensional systems.

While the spirit of bottom-up approaches is related to inductive reasoning, i.e. generalizing from something that is more specific, the top-down approach can be associated with deduction, i.e. applying general rules to derive a conclusion. Top-down coarse-graining starts with universal physical principles or experimentally observed phenomenas that the model aims to reproduce. In general, top-down models are used to study the consequences of the rules incorporated in the model.\cite{schmid2009toy} In this regard, top-down models are often not chemically-specific, as the set of rules yielding a specific phenomena might not be unique.\cite{noid2013perspective} Furthermore, it is often not sufficient to link a single phenomena to a specific chemical compound, as a full range of properties have to be reproduced to characterize the compound with high confidence. In this sense, top-down models are often under-constrained.

%While a coarse-grained model implies the existence of more fundamental, higher resolution model, the question of how the coarse-grained model is built upon this fine-grained 

\subsection{Consistency Criteria and the Many-Body Potential of Mean Force}

Deriving the coarse-grained potential $U_{\text{CG}}(\mathbf{R})$ for the coarse-grained model is the most challenging task. In the case of bottom-up coarse-graining, the potential $U(\mathbf{R})$ can theoretically be derived exactly from the fine-grained potential $U_{\text{AA}}(\mathbf{r})$ and the mapping $\mathbf{M}(\mathbf{r})$. The underlying criteria for the derivation is called consistency criteria and states that the equilibrium joint probability density $p_{CG} (\mathbf{R}, \mathbf{P})$ in phase space of the coarse-grained coordinates $\mathbf{R}$ and momenta $\mathbf{P}$ have to match the implied atomistic probability density $p_{AA} (\mathbf{R}, \mathbf{P})$.\cite{noid2008multiscale} For simplicity, the following considerations are restricted to the configuration space, i.e. exluding momenta, such that the consistency criteria can be written as

\begin{equation}
\label{consistency1}
  p_{CG} (\mathbf{R}) = p_{AA} (\mathbf{R}) ,
\end{equation}

where $p_{CG} (\mathbf{R})$ is is the equilibrium probability density for a configuration $\mathbf{R}$ in the canonical ensemble of the coarse-grained model

\begin{equation}
\label{consistency2}
  p_{CG} (\mathbf{R}) \propto \text{exp} \Big[ - \frac{U_{\text{CG}}(\mathbf{R})}{k_B T} \Big]
\end{equation}

and $p_{AA} (\mathbf{R})$ is the equilibrium probability density for a coarse-grained configuration $\mathbf{R}$ implied by the mapping $\mathbf{M}(\mathbf{r})$ expressed in terms of the fine-grained model

\begin{equation}
\label{consistency3}
  p_{AA} (\mathbf{R}) \propto \mathcal{Z}(\mathbf{R}) := \int  \text{exp} \Big[ - \frac{U_{\text{AA}}(\mathbf{r})}{k_B T} \Big] \delta(\mathbf{M}(\mathbf{r}) - \mathbf{R}) d\mathbf{r} .
\end{equation}

Plugging Eq. \ref{consistency2} and \ref{consistency3} into \ref{consistency1} and reordering yields

\begin{equation}
\label{mbpmf}
 U(\mathbf{R}) = - k_B T \; \text{ln} (\mathcal{Z}(\mathbf{R})) + \text{const} .
\end{equation}

Eq. \ref{mbpmf} defines the many-body PMF as a projection of the free energy function onto the coarse-grained degrees of freedom. It assigns a weight to each coarse-grained configuration associated with the sum of all the Boltzmann weights for the corresponding atomistic configurations. Recalling the free energy  $F$

\begin{equation}
  F = U - TS = - k_B T \; \text{ln}(\mathcal{Z}) ,
\end{equation}

where $U$ is the internal energy, makes it clear that the many-body PMF is not a regular potential, as it contains both, energetic as well as entropic contributions.\cite{noid2008multiscale} Moreover, as the name suggests, this potential generates the average atomistic forces associated with the atomistic configurations that map to the specific coarse-grained configuration.\cite{noid2013perspective} 

Importantly, $U(\mathbf{R})$ typically introduces many-body interactions that can not be expressed with simple classical force fields as outlined in Sec. \ref{molecular_forcefield}.\cite{noid2013perspective} This is a direct consequence for integrating over degrees of freedom. Moreover, the many-body PMF is extremely high dimensional and therefore becomes infeasible to compute for most systems.\cite{noid2013perspective} For those reasons, a wide range of techniques have been developed to obtain a tractable approximation of the many-body PMF (see below). 


\subsection{Overview of Bottom-Up Approaches and Top-Down Force Fields}

In this section, some popular bottom-up approaches to determine the coarse-grained potential are reviewed. Those approaches include structure based techniques, such as direct Boltzmann inversion (DBI) and iterative Boltzmann inversion (IBI), as well as variational approaches, like the multiscale coarse-graining approach (MS-CG) and the relative entropy (RE) framework. Moreover, two generic top-down force fields are presented, namely the Martini forcefield and the Kremer-Grest model.

\subsubsection{Direct Boltzmann Inversion}

The most simple approach to obtain an approximate coarse-grained potential is direct Boltzmann inversion (DBI).\cite{tschop1998simulation} This approach aims at reproducing certain structural distributions computed from atomistic reference data that is mapped onto the coarse-grained degrees of freedom. The distribution functions for given interactions $\xi$ are denoted with $p_{\xi}(x)$, where $x$ is a scalar variable, such as pairwise distances, angles or dihedrals. The goal is then to find the corresponding potential $U_{\xi}$ that yields the desired distribution. Based on the assumption, that the distribution functions for different mechanical variables $x$ factorize, the probability distributions can be written as Boltzmann factors \cite{tschop1998simulation}

\begin{equation}
  p_{\xi}(x) \propto \text{exp} \Big[ \frac{- U_{\xi}(x)}{k_B T} \Big] .
\end{equation}

In addition, the corresponding volume elements for each distribution has to be taken into account, i.e. the Jacobian element $J_{\xi}(x)$.\cite{tschop1998simulation} The potential can then directly computed as 

\begin{equation}
  U_{\xi} (x) = -k_b T \text{ln} \Big( \frac{p_{\xi}(x)}{J_{\xi}(x)} \Big) .
\end{equation}

Note, that factorizing the probability distributions is a very severe approximation. Therefore, this approach yields accurate results only for interactions that can be regarded as isolated in the coarse-grained model.\cite{noid2013perspective} However, in cases where the coupling between the interactions can not be ignored, DBI yields inaccurate potentials that do not corretly reproduce the sturctural distributions as important cross-correlations are not taken into account.


\subsubsection{Iterative Boltzmann Inversion}

To improve the potentials derived with DBI, an iterative scheme can be applied. This scheme is called iterative Boltzmann inversion (IBI) and consists of the following steps:\cite{muller2002coarse, reith2003deriving} (1) Initial potentials are derived using DBI. (2) Use the derived coarse-grained potentials in a MD simulations to compute the corresponding structural distributions $p_{\xi}(x|U)$. (3) Update the coarse-grained potentials via

\begin{equation}
  U_{\xi, \text{new}} (x) = U_{\xi, \text{old}} (x) - k_B T \text{ln} \Big( \frac{p_{\xi}(x)}{p_{\xi}(x|U)} \Big) .
\end{equation}

Steps (2) and (3) are repeated until the potentials converge. Despite its simplicity, IBI has become very popular and is used especially for complex liquids and polymers. While IBI still treads every interaction $\xi$ seperatly, it implicitly accounts for correlations between the interactions through the iterative scheme.\cite{noid2013perspective} However, convergence of the potentials is not guaranteed.\cite{noid2013perspective}

\subsubsection{Multiscale Coarse-Graining}

Beside approaches that focus on reproducing certain structural properties, variational approaches can be applied to find an approximation of the many-body PMF. A very early work in this field is the Multiscale coarse-graining (MS-CG) method, which introduces a force-matching functional \cite{ercolessi1994interatomic, izvekov2005multiscale, izvekov2005multiscale2}

\begin{equation}
  \chi^2 [\mathbf{F}] = \frac{1}{3N} \Big \langle \sum_{I=1}^N | \mathbf{F}_I(\mathbf{M}(\mathbf{r})) - \mathbf{f}_I(\mathbf{r}) |^2 \Big \rangle_{\text{AA}}  ,
\end{equation}

where $\mathbf{F}_I(\mathbf{M}(\mathbf{r}))$ is the force acting on the coarse-grained site $I$ mapped from the atomistic configuration implied by the trial potential of the coarse-grained model and $\mathbf{f}_I(\mathbf{r})$ is the net force acting the group of atoms associated with site $I$. The angular brackets with subscript \textit{AA} denotes the atomistic canonical ensemble average, which is typically approximated using trajectories obtained in a simulation. The MS-CG method states, that the coarse-grained potential yielding the best approximation of the average net atomistic forces should be used and therefore $\chi^2 [\mathbf{F}]$ has to be minimized. Indeed, the functional $\chi^2 [\mathbf{F}]$ has a unique global minimum given by the actual many-body PMF.\cite{noid2013perspective} In practice, the coarse-grained force $F$ is expressed as a linear combination of basis functions leading to a coupled system of linear equations that can be solved directly.\cite{noid2013perspective} 

\subsubsection{Relative Entropy}

Another variational approach is based on the relative entropy, also knwon as Kullback-Leibler divergence, which is widely used as a asymmetric distance metric between probability distributions.\cite{kullback1951information, shell2008relative, chaimovich2010relative, chaimovich2011coarse} Applied to the coarse-graining problem, the relative entropy can be written 

\begin{equation}
\label{relative_entropy}
  S_{\text{rel}} = \int p_{\text{AA}}(\mathbf{r}) \; \text{ln} \Big( \frac{p_{\text{AA}(\mathbf{r})}}{p_{\text{CG}}(\mathbf{M}(\mathbf{r}))} \Big) d\mathbf{r} + \langle S_{\text{map}} \rangle_{\text{AA}} ,
\end{equation}

where

\begin{equation}
  S_{\text{map}}(\mathbf{R}) = \text{ln} \int \delta [\mathbf{M}(\mathbf{r}) - \mathbf{R}] d\mathbf{r} 
\end{equation}

is the mapping entropy, which accounts for the degeneracy of the mapping $\mathbf{M}$, i.e. a single coarse-grained configuration corresponds to multiple atomistic configurations. The relative entropy $S_{\text{rel}}$ can be interpreted as a measure for the loss of information when changing from an atomistic to a coarse-grained description.\cite{shell2008relative} Moreover, it is related to many different coarse-graining errors and vanishes only if $p_{\text{CG}}(\mathbf{r}) \propto p_{\text{AA}}(\mathbf{r})$.\cite{chaimovich2010relative} 

Inserting the distributions known for the canonical ensemble, i.e. $p_{\text{CG}}(\mathbf{R}) = \mathcal{Z}_{\text{CG}}^{-1} \text{exp} \Big[- \frac{U_{\text{CG}}(\mathbf{R})}{k_B T} \Big]$ and $p_{\text{AA}}(\mathbf{r}) = \mathcal{Z}_{\text{AA}}^{-1} \text{exp} \Big[- \frac{U_{\text{AA}}(\mathbf{r})}{k_B T} \Big]$, into Eq. \ref{relative_entropy} yields

\begin{equation}
  S_{\text{rel}} = \frac{\langle U_{\text{CG}} - U_{\text{AA}} \rangle_{\text{AA}}}{k_B T} - \frac{F_{\text{CG}} - F_{\text{AA}}}{k_B T} + \langle S_{\text{map}} \rangle_{\text{AA}} ,
\end{equation}

where $F = - k_b T \; \text{ln}(\mathcal{Z})$ is the free energy. In order to optimize the parameters $\lambda$ of the coarse-grained potential $U_{\text{CG}}$ the derivative of the relative entropy can be used

\begin{equation}
  \frac{\partial S_{\text{rel}}}{\partial \lambda} = \frac{1}{k_B T} \Big[ \Big \langle \frac{\partial U_{\text{CG}}}{\partial \lambda} \Big \rangle_{\text{AA}} -\Big \langle \frac{\partial U_{\text{CG}}}{\partial \lambda} \Big \rangle_{\text{CG}} \Big] ,
\end{equation}

i.e. the derivatives of $U_{\text{CG}}$ with respect to its parameters have to average to the same value for both, the atomistic and the coarse-grained ensemble.\cite{shell2008relative} Importantly, this approach reproduces the expectation values for every observable, such as distances or angles, that are included in the coarse-grained potential.\cite{shell2008relative} Therefore, the relative entropy framework can be used to generate potentials that capture the correct structural distributions. Numerical minimization of the relative entropy is typically achieved using advanced algorithms, such as the Newton-Raphson method.\cite{shell2008relative}

\subsubsection{Martini Force Field}

The Martini force field is a generic coarse-grained potential for a wide range of soft matter systems. It was developed with an emphasize on biomolecules and its various applications include lipid membranes, proteins, sugars and nucleotides.\cite{marrink2007martini, monticelli2008martini, lopez2009martini, uusitalo2015martini} The parametrization of the martini force field incorporates both coarse-graining philosophies, a top-down approach for the non-bonded interactions and a bottom-up approach for the bonded interactions. While the nonbonded parameters of the model are tuned to reproduce experimental partitioning free energies of water-alkane mixtures, the bonded interactions are optimized to capture the correct conformational distributions of atomistic reference data.\cite{marrink2013perspective} 

Central to the to the Martini model is a robust transferablity across soft matter systems. For this reason, the model is based on modular building blocks, i e. coarse-grained beads, and introduces rules for the mapping from groups of atoms to the beads. On average, four heavy atoms and their associated hydrogens are represented by a single coarse-grained site. The four main building blocks are denoted with charged (Q), polar (P), nonpolar(N), and apolar (C). Each of the main bead types have further subtypes distinguishing either their hydrogen bonding capability (Q and N types) or their degree of polarity (P and C types). The assignment of the specific bead types is based on the hydrophobicity, i.e. the water/organic partition free energy, of the corresponding group of atoms.

Despite its wide use, the Martini model also has its limitations linked to its ambition of being broadly transferable. A major drawback of the Martini model is a less accurate reproduction of structural features. For example, the model does not include size-dependent Lennard-Jones parameters which may lead to artifacts such as increased barriers in dimerization profiles.\cite{alessandri2019pitfalls}

\subsubsection{Kremer-Grest Polymer Model}

The Kremer-Grest (KG) model is top-down model widely used to study generic polymer properties.\cite{kremer1990dynamics, grest1986molecular} The model represents polymers as chains of beads connected via non-linear springs. The spring potential is tuned such that crossing of two polymer chains is avoided in order to correctly simulate the dynamics of polymer melts, in especially entanglement effects of long chains.
More specifically, the potential for bonded beads is given by the finite-extensible-nonlinear spring (FENE) potential

\begin{equation}
  U_{\text{FENE}}(r) = -15 k_B T \Big( \frac{R}{\sigma} \Big)^2 \text{ln} \Big[ 1 - \Big(\frac{r}{R}\Big)^2 \Big] ,
\end{equation}

where $r$ is the distance between the two bonded beads, $\sigma$ is the bead diameter and $R$ defines the distance where the potential divergences. A typical choice is to set $R = 1.5 \sigma$. Additionally, the beads interact through a truncated and shifted Lennard-Jones potential, which is purely repulsive:

\begin{equation}
  U_{\text{WCA}}(r) = \begin{cases}
    4 k_B T \Big[ \Big( \frac{\sigma}{r} \Big)^{-12} - \frac{\sigma}{r} \Big)^{-6} + \frac{1}{4} \Big] \;\; , \;\; \text{if} \; r < 2^{1/6} \sigma \\
    0 \;\; , \;\; \text{otherwise}
  \end{cases} ,
\end{equation}

In order to vary the stiffness of the chains, an additional bending potential can be introduced

\begin{equation}
  U_{\text{bend}}(\Theta) = \kappa k_B T (1 - \text{cos}(\Theta)) , 
\end{equation}

where $\Theta$ is the the bond angle and $\kappa$ defines the stiffness.\cite{faller1999local}

While the KG model is a generic model to study universal phenomenas of polymer melts, the stiffness of the chains can be used to relate the model to real polymers.\cite{everaers2020kremer, svaneborg2020characteristic} To this end, simulated polymer melts can be linked to commodity polymers via their Kuhn length, which is the scale indicating the crossover from a chemistry-specific to a universal random-walk like behaviour.

% -----------------------------------
%   Backmapping
% -----------------------------------

\section{Backmapping}
\label{theory_backmapping}

Reducing the resolution is just one side of the MS philosophie. In order to close the loop, a reverse-mapping that allows to go the other direction is required as well. Such a reverse-mapping, also refered to as \textit{Backmapping}, can be regarded as a magnifying glass to zoom in to the molecular system.

Combining coarse-graining with a reverse-mapping procedure unlocks the true power of MS modeling, as it provides a strategy to take the best of both worlds: Coarse-graining can be used to obtain trajectories of long time scale and large length scale processes that would have been to costly and time consuming to perform using a higher resolution model. However, the coarse-grained model lacks the accuracy and details provided in an atomistic simulation. Those atomistic details are often required for one or more of the following reasons: (1) to rigorously analyze the simulation results on a local scale, (2) to enable a direct comparison to experimental data, for example obtained with spectroscopic methods, (3) to serve as starting point for further simulations, or (4) to asses the stability and accuracy of the obtained coarse-grained structures in order to improve the coarse-grained model.\cite{hess2006long, shimizu2018reconstruction, brocos2012multiscale, stansfeld2011coarse, pandey2014multiscale, pezeshkian2020backmapping, deshmukh2016water} 
Backmapping can circumenvent these limitations by efficiently reintroducing details along the coarse-grained degrees of freedom. Typically, the large scale behaviour of the system is already captured correctly by the coarse-grained model and therefore reintroducing the missing degrees of freedom becomes feasable as they have to be equilibrated only locally. Hence, connecting a coarse-grained model with a backmapping scheme is an efficient approach to yield well equlibrated, high-resolution molecular structures and trajectories for long time and large length scales. 

%protein-DNA complexes. asses stabilities and accuracy of structures sampled in coarse-grained model. Use backmapped structures for further AA MD simulation. Need atomistic details to compare with experiment. \cite{shimizu2018reconstruction}
%generation and subsequent examination of self-assembled structures, including the fine characterization of structural and dynamic properties of the resulting aggregate \cite{brocos2012multiscale}
%assembly and the interactions of membrane protein/lipid complexes
%complex, mixed lipid systems around a membrane protein. backmapping needed to provide models of more specific lipid/protein interactions, including local distortions of bilayer thickness and/or selective interactions with lipid headgroups.\cite{stansfeld2011coarse}
%allow for a direct, unambiguous comparison to experiments, such as neutron spin echo or more specifically dielectric relaxation and NMR spectroscopy. \cite{hess2006long}
%polymer-solid interface. large atomistic structures created serve as starting points for further MD simulations that shed insight into local dynamics of PI on graphite.\cite{pandey2014multiscale}
%temporal enhancement of interactions between viruses, vesicles, and nanoparticles with a membrane \cite{pezeshkian2020backmapping}
%Our findings illustrate how the chemical nature and molecular details of aqueous interfaces control the early stages of PA assembly and provide quantitative insights into the unique role of water by drawing on the interfacial nature of hydration and aggregation kinetics associated with peptide assemblies.\cite{deshmukh2016water}

\subsection{The Challenges of Reintroducing Degrees of Freedom}

While mapping from a higher to a lower resolution is typically straightforward, the opposite direction is more challenging. Formally, let $\{\mathbf{A}_{I} = (\mathbf{R}_I, \mathbf{C}_I) | I = 1, \dots, N \}$ denote the set of $N$ coarse-grained beads. Each bead has position $\mathbf{R}_I$ and an associated type $\mathbf{C}_I$. The type $\mathbf{C}_I$ reflects various attributes, such as the bead mass, the connectivity or associated force field parameters. Similarly, let $\{\mathbf{a}_{I} = (\mathbf{r}_i, \mathbf{c}_i) | i = 1, \dots, n \}$ denote the set of $n$ atoms, with position $\mathbf{r}_i$ and types $\mathbf{c}_i$.
 
A backmapping function $\phi$ takes the coarse-grained information $\mathbf{A} = (\mathbf{A}_1, \dots, \mathbf{A}_N)$ as well as the desired atom types $ \mathbf{c} = (\mathbf{c}_1, \dots, \mathbf{c}_n)$ as input and generates a set of coordinates $\mathbf{r} = (\mathbf{r}_1, \dots, \mathbf{r}_n)$,

\begin{equation}
  %\phi \Big( (\mathbf{A}_1, \dots, \mathbf{A}_N), (\mathbf{c}_1, \dots, \mathbf{c}_n) \Big) = (\mathbf{r}_1, \dots, \mathbf{r}_n) .
  \phi ( \mathbf{A}, \mathbf{c} ) = \mathbf{r}.
\end{equation}

Deriving the function $\phi$ is not a trivial task, as it is constrained by two important aspects. First of all, the mapping has to be consistent, i.e. the missing degrees of freedom have to be reinserted along the coarse-grained degrees of freedom. In other words, applying the coarse-grained mapping $\mathbf{M}$ to the backmapped structure has to yield the original coarse-grained structure,

\begin{equation}
  %\mathbf{M} \Big( \phi \Big( (\mathbf{A}_1, \dots, \mathbf{A}_N), (\mathbf{c}_1, \dots, \mathbf{c}_n) \Big) \Big) = \mathbf{R} .
  \mathbf{M} ( \phi  (\mathbf{A}, \mathbf{c}) ) = \mathbf{R}.
\end{equation}

Secondly, the mapping is not unique, since the reduced resolution implies that many atomistic structures can map to the same coarse-grained configuration. As a consequence, a single coarse-grained structure $\mathbf{R}$ will correspond to an ensemble of atomistic microstates $\{\mathbf{r} | \mathbf{M}(\mathbf{r}) = \mathbf{R}\}$. Therefore, strictly speaking, the coarse-grained mapping is not invertable. 

To take the aforementioned aspects into account, the backmapping problem has to be expressed as a joint conditional probablity distribution $p_{\phi} ( \mathbf{r} | \mathbf{A}, \mathbf{c})$ that assigns a statistical weight to each atomistic microstate given the coarse-grained information as well as the atomistic attributes. Ideally, the coarse-grained model generates the Boltzmann distribution expressed in the coarse-grained degrees of freedom, i.e. the many-body PMF is reproduced perfectly. Consequently, an ideal backmapping scheme also has to reinsert atomistic details with the correct statistical weight, i.e.

\begin{equation}
  p_{\phi} ( \mathbf{r} | \mathbf{A}, \mathbf{c}) = \begin{cases}
    \propto \text{exp} \Big( - \frac{U(\mathbf{r})}{k_B T} \Big) \;\;\;\; & \text{if} \; \tilde{\mathbf{M}}(\mathbf{r}, \mathbf{c}) = (\mathbf{R}, \mathbf{C}) \\
    0 \;\;\;\; & \text{if} \; \tilde{\mathbf{M}}(\mathbf{r}, \mathbf{c}) \neq (\mathbf{R}, \mathbf{C})
  \end{cases}.
\end{equation}

Here, $\tilde{\mathbf{M}}$ is used as an extended coarse-graining mapping function that includes both, the coordinates as well as the types of the atoms and beads, respectively, 

\begin{equation}
  \tilde{\mathbf{M}}(\mathbf{r}, c) = (\mathbf{R}, C),
\end{equation}

where $c = (c_1, \dots, c_n)$ denotes the $n$ atom types and $C =(C_1, \dots, C_N)$ denots the $N$ bead types.

%\begin{equation}
%  p_{\phi}\Big( (\mathbf{r}_1, \dots, \mathbf{r}_n) \Big | (\mathbf{A}_1, \dots, \mathbf{A}_N), (\mathbf{c}_1, \dots, \mathbf{c}_n) \Big) = \begin{cases}
%    \propto \text{exp} \Big( - \frac{E(\mathbf{r})}{k_B T} \Big) \;\;\;\; & \text{if} \; \mathbf{M}(\mathbf{r}) = \mathbf{R} \\
%    0 \;\;\;\; & \text{if} \; \mathbf{M}(\mathbf{r}) \neq \mathbf{R}
%  \end{cases} 
%\end{equation}

%\begin{equation}
  %p_{\phi}\Big( (\mathbf{r}_1, \dots, \mathbf{r}_n) \Big | (\mathbf{A}_1, \dots, \mathbf{A}_N), (\mathbf{c}_1, \dots, \mathbf{c}_n) \Big) ,
  %p_{\phi} ( \mathbf{r} | \mathbf{A}, \mathbf{c}) ,
%\end{equation}

\subsection{Overview of Existing Approaches}

The idea of backmapping is as old as the idea of coarse-graining and several approaches to tackle the backmapping problem exist. Traditional methods include fragment-based approaches that utilize a chemistry specific library of molecular fragments and generic approaches that rely on geometric rules or simple random placement of the atoms. More recently, approaches based on ML techniques emerged that rely on data-driven methods to reconstruct molecular structures from a lower resolution.

\subsubsection{Fragment-based Approaches}

Most traditional backmapping approaches follow a similar procedure: At first, an initial atomistic structure is proposed. In the case of fragment-based backmapping, several trial atomistic fragments corresponding to a single or a small set of beads are superimposed onto the coarse-grained sites.\cite{peter2009multiscale, zhang2019hierarchical, hess2006long, brasiello2012multiscale} Typically, rigid rotation and translation is used to optimize the orientation of the given fragment with respect to some geometric or energetic properties. The trial fragments are usually taken from a presampled library that has to be simulated beforehand. The resulting initial atomistic structure is most likely not representative for the Boltzmann distribution, as a simple projection of fragments onto the coarse-grained sites typically leads to overlaps between reconstructed atoms and distorted bonded structures. It is therefore necessarry to relaxe the initial structure by means of energy minimization based on the atomistic forcefield. Subsequently, the relaxed structure has to be equilibrated using MD simulation. In some cases, where the overall equilibration and diffusion is rather slow compared to the equilibration of local features, the equilibration process is straight forward. In other cases, restraints have to be introduced to prevent the reconstructed atoms to drift to far away from the center of the coarse-grained site. Therefore, an additional potential can be applied that couples the atomistic degrees of freedom to the coarse-grained degrees of freedom

\begin{equation}
  U_{\text{restr}}(\mathbf{r}, \mathbf{R}) = b (\mathbf{M}(\mathbf{r}) - \mathbf{R})^2 ,
\end{equation}

where the prefactor $b$ is used to scale the restraining potential.

\subsubsection{Generic Approaches}
\label{SEC:bm_generic}

Generic backmapping approaches are similar to fragment-based approaches, but differ in the way the initial atomistic structure is derived. Generic schemes do not rely on presampled fragments but project the atomistic degrees of freedom onto the coarse-grained structure using general rules. In the most basic version, the atoms are randomly placed close to their corresponding coarse-grained site, whereas more sophisticated approaches rely on geometric rules to place the atoms.\cite{rzepiela2010reconstruction, wassenaar2014going} The resulting initial atomistic structure is typically even more distorted as in the fragment-based approach. Therefore, the subsequent energy minimization and equilibration procedures have to be performed more carefully and typically involve multiple stages where the interaction potentials are gradually switched on.

\subsubsection{Machine Learning Approaches}

During the course of this PhD several ML approaches have been published to tackle the backmapping problem. This is encouraging, as it indicates an increasing interest to integrate ML techniques to the field of MS simulations and emphasizes the importance of the present work.

Wang et. al utilized a variational autoencoder (see Sec. \ref{}) and treated the coarse-grained degrees of freedom as latent variables (see Sec. \ref{}).\cite{wang2019coarse} Their framework unifies the task of learning the coarse-grained variables, parametrizing the coarse-grained force field and decoding back to atomistic resolution. In contrast to standard variational autoencoder, where the latent distribution is regularized to resemble a Gaussian distribution, the proposed coarse-graining autoencoder utilizes a force-matching functional for regularization. Despite its elegance, the quality of decoded structures regarding some structural quantities, such as the bond length distribution, is rather low. The deterministic decoder trained with mean-square error as reconstruction loss leads unavoidably to a loss of mapping entropy. Therefore, decoded structures represent a mean reconstruction of an ensemble of microstates. However, their work already gives a prospect to improve the model to yield high fidelity reconstructions utilizing a probabilitistic decoder. 

An other approach by Li et. al used a convolutional conditional generative adversarial network (see Sec. \ref{}) for the reconstruction of cis-1,4-polyisoprene melts from a coarse-grained representation.\cite{li2020backmapping} This approach is similar in spirit to the method proposed in this thesis but is based on an image representation where XYZ components of vectors are converted into red–green–blue (RGB) values. While being computational efficient, the method does not fully take the local environment of the polymer chains into account. As a consequence, the overall quality of reconstructed molecular structures is rather low, especially steric overlapps can not be resolved and demand for further relaxation via energy minimization.

Finally, An et. al used several ML approaches including artificial neural networks, k-nearest neighbor, Gaussian process regression and random forest to built regression models for backmapping.\cite{an2020machine} The regression was performed for small molecules in vacuum and the coordinates of the coarse-grained and atomistic structures were directly used as input and output representations for the models. The best performances was achieved using an artificial neural network. However, all of the reported models failed to generate a high quality structure for hexane.
