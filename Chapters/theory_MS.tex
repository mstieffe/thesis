
\chapter{Multiscale Modeling} % Main chapter title

\label{theory_ms} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\cite{peter2010multiscale}, \cite{peter2009multiscale}

% -----------------------------------
%   Statistical Mechanics
% -----------------------------------

\section{Thermodynamics and Statiscal Mechanics}

%\subsection{Microstates vs Macrostates}
\textit{Classical thermodynamics} describes the behaviour of bulk, macroscopic systems in terms of a few macroscopic quantities, such as the total internal energy $E$, the total volume $V$, and the number of particles $N$. Typically, the system is considered at the \textit{thermodynamic equilibrium}, where its properties do not change with time and the actual state is history-independent. 

The basic concepts of classical thermodynamics were developed before the molecular nature of matter was generally accepted. Therefore, it is not surprising that classical thermodynamics is concerned with laws and relationships exclusively for macroscopic quantities without referencing to a more fundamental description on the molecular level. In fact, the laws of classical thermodynamics are based only on a few postulates.\cite{shell2015thermodynamics}

The molecular underpinning of thermodynamics was developed in the field of \textit{statistical mechanics}, where all the microscopic details of individual molecules are taken into account. For example, in the classical picture, a list of positions $\mathbf{r} \in \mathrm{R}^{3N}$ and momenta $\mathbf{p} \in \mathrm{R}^{3N}$ of the atoms are considered, whereas a quantum mechanical description uses quantum states. In the following, the classical picture is used for simplicity and a microscopic state $m = (\mathbf{r}, \mathbf{p})$ is characterized by its $6N$ degrees of freedom, i.e. a point in the $6N$ dimensional \textit{phase space}.

\subsection{The Mircrocanonical Ensemble and the Principle of Equal a Priori Probabilities}

In statistical mechanics, macroscopic quantities measured at equilibrium are described as the average behaviour of many particles. For an isolated system, i.e. a system that can not exchange energy or particles with its surrounding at fixed volume, a macrostate is completely specified by $(E, V, N)$, which remain constant throughout molecular motion.\cite{shell2015thermodynamics} Note that the temperature $T$ and pressure $P$ are not necessary to specify the macroscopic state, as their values can be derived once the vales for $(E, V, N)$ are set. For each macrostate $(E, V, N)$, a collection of possible microstates can be found, i.e. a surface in the phase space of $N$ atoms with constant total energy $E$ at a volume $V$. This collection of microstates together with their associated probabilities is called the \textit{microcanonical ensemble}.

The positions and velocities of the atoms constantly vary under the influence of their mutual interactions. Therefore, the microstate changes constantly even if the macrostate stays fixed. The likelihood that a microstate will be visited by the system is denoted with $p_m$ and the microstate probabilities do not change with time at equilibrium. A cornerstone of statistical mechanics is the statement that the system has no preference for a certain microstate and hence, each microstate is equally likelily.\cite{shell2015thermodynamics} This fundamental rule is called the \textit{principle of equal a priori probabilities}. It allows to write the likelihood in the canonical ensemble as

\begin{equation}
  p_m = \begin{cases}
    \frac{1}{\Omega(E,V,N)} \;\;\;\; & \text{if} \; E_m \neq E \\
    0 \;\;\;\; & \text{if} \; E_m \neq E
  \end{cases} ,
\end{equation}

where $\Omega(E,V,N)$, called the \textit{density of states}, is a function describing the number of accessible microstates for particular $(E,N,V)$.\cite{shell2015thermodynamics}

\subsection{Different views on the Entropy}

A central theme common for thermodynamics, statistical mechanics as well as information theory is the concept of \textit{entropy}.
In classical thermodynamics, the entropy $S$ is regarded as a non-conserved state-function that emerges naturally for systems in equilibrium.\cite{shell2015thermodynamics} It is a function

\begin{equation}
  S = S(E,V,N)
\end{equation}

dependent on the macroscopic quantities $E$, $V$ and $N$. Allowing for heat, volume or mass transfer, the system can change its equilibrium macrostate to another macrostate. This is called a \textit{thermodynamic process}. Historically, entropy was introduced to explain why some thermodynamic processes are irreversible, i.e. the process occurs spontaneously in one direction whereas the reverse does not, although both directions obey the conversation of energy.\cite{simon1997physical} The reason for this is that thermodynamic systems tend to progress towards states with increasing entropy. This is stated in the second law of thermodynamics: The entropy of an isolated system can not decrease as it always evolves to an equilibrium state where the entropy is highest.\cite{shell2015thermodynamics} 

While the specific form of the entropy function is different for every system, all entropy functions have some shared properties. A very important one is the total differential

\begin{equation}
  dS = \frac{1}{T}dE + \frac{P}{T}dV - \frac{\mu}{T}dN ,
\end{equation}

which relates the temperature $T$, the pressure $P$ and the chemical potential $\mu$ to derivatives of the same function and therefore defines a relationships between these variables.\cite{shell2015thermodynamics}

The above definition for the entropy $S$ is exclusively based on macroscopic properties. Boltzmann was the first who gave a definition for the entropy based on microscopic considerations and therefore introduced a connection of thermodynamics to the molecular nature of matter.\cite{shell2015thermodynamics} His famous formula reads

\begin{equation}
\label{boltzmann_entropy}
  S = k_B ln( \Omega(E,V,N) ) ,
\end{equation}

where $k_B$ is proportionality constant, called \textit{Boltzmann's constant}. Eq. \ref{boltzmann_entropy} links the entropy $S$ to the number of accessible microstates for given macrostate. Therefore, the second law of thermodynamics can be interpreted as the tendency of a system to evolve to a state that maximizes the number of accessible microstates.

Based on Boltzmann's equation, Gibbs introduced a more general form of the entropy 

\begin{equation}
\label{gibbs_entropy}
  S = k_B \sum p_m ln( p_m )  ,
\end{equation}

which is equivalent, up to the Boltzmann constant, to the definition of the information entropy by Shannon. Note, that upon application of the principle of equal a priori probailities, i.e. $p_m = \frac{1}{\Omega(E,V,N)}$, the entropy is maximized and Gibbs formulation of the entropy recovers Boltzmann's equation. 

%In this regard, the entropy can interpreted as a measure of uncertainty. Moreover, according to Jayne, the thermodynamic entropy can be regarded as an application of Shannon's information theory: The entropy $S$ expresses the missing information needed to determine the specific microstate of a system that remains uncommunicated due to a description of the system solely in terms of macroscopic quantities. From this point of view, the principle of equal a priori probabilities expresses our ignorance about the microscopic details of the system as it chooses the distribution that maximizes the entropy. 

\subsection{The Canonical Ensemble and the Boltzmann Distribution}

Central to the previous considerations was the ensemble for an system in isolation, i.e. the microcanonical ensemble for fixed $(E,V,N)$. In the following, the \textit{canonical ensemble} is introduced describing a system that is not isolated but at constant temperature. To this end, the system is considered to be in thermal contact with an infinitely large heat bath with a fixed temperature $T$. Therefore, the fixed macroscopic quanties of the system are $(T,V,N)$, while the total energy $E$ is allowed to fluctuate. The composite of the system and the heat bath is again considered to be an isolated system. However, summing over all the microstates of the heat bath allows to derive the probabilities for the microstates $m$ of the system of interest. Importantly, these microstate probabilities are no longer equal but depend on their total energy $E_m$. More specifically, the microstate probabilities can be written as

\begin{equation}
\label{boltzmann_dstr}
  p_m = \frac{e^{- \frac{E_m}{k_B T}}}{Z} ,
\end{equation}

where the normalization constant 

\begin{equation}
  Z = \sum_{\text{all }\, m \, \text{at}\, V,N} e^{- \frac{E_m}{k_B T}}
\end{equation}

is called the \textit{canonical partition function} and the probability distribution in Eq. \ref{boltzmann_dstr} is refered to as \textit{Boltzmann distribution}.\cite{shell2015thermodynamics} Note, that similarly to the microcanonical distribution, the Boltzmann distribution maximizes the entropy for a given macrocospic state $(T,V,N)$. In general, the canonical ensemble is more useful than the microcanonical ensemble in practice, since in most cases systems in thermal equilibrium with their surroundings are considered.

\subsection{Thermodynamic Limit and Statistical Equivalence of Ensembles}

The canonical approach provides an alternative, in addition to the microcanonical approach, to determine the behaviour of a system at a microscopic level. While there are rigorously no fluctuations in the energy in the microcanonical ensemble, energy fluctuates in the canonical ensemble but the temperature is rigorously constant. However, in the \textit{thermodynamic limit}, i.e. when the number of particles and the volume of the system go to inifinity $N \rightarrow \infty$, $V \rightarrow \infty$ while the particle density is held fixed $\frac{N}{V} = \text{constant}$, the differences in macroscopic properties for both ensembles vanish.\cite{shell2015thermodynamics} 

This can be seen clearly considering the distribution of the energy in the canonical ensemble. Using Eq. \ref{boltzmann_dstr} and \ref{boltzmann_entropy} the propability for a specific energy $E$ can be written as

\begin{equation}
  p(E) \propto \Omega(E,V,N) e^{- \frac{E_m}{k_B T}} = e^{\frac{1}{k_B} ( S(E,V,N) - \frac{E_m}{T} ) } .
\end{equation}

This equation shows that two competing terms have to be considered for the probability of energy levels: The first term is the entropy $S$, which is a concave, increasing function of $E$.\cite{shell2015thermodynamics} The second term $-\frac{E}{T}$ decreases linearly with the energy $E$. Therefore, the probability distribution for the energy levels has a maximum at an intermidiate energy $E^*$. Both terms, $S$ and $E$, are extensive quantities, i.e. they scale as $N$. Since the competing terms are within the exponential, the probability distribution becomes sharply peaked at the maximum $p(E^*)$. Therefore, $p(E^*)$ becomes the most dominant term and the impact of microstates with different energies $E \neq E^*$ vanishes.\cite{shell2015thermodynamics}

Another way to view this, is that fluctuations in the total energy $E$ become extremely small in the thermodynamic limit. The variance of the total energy $\sigma_E^2 = <E^2> - <E>^2$ can be linked to the heat capacity, which is an extensive quantity. Therefore, the relative magnitude of energy fluctuations scales as

\begin{eqnarray}
  \frac{\sqrt{\sigma_E^2}}{E} \propto N^{-1/2} .
\end{eqnarray}

As a consequence, a macroscopic systems appear to have constant energies.\cite{shell2015thermodynamics}

\subsection{Information-theoretic View on Statistical Mechanics}

In 1957 Jaynes published two papers emphazising the correspondence between information theory and statistical mechanics.\cite{jaynes1957information,jaynes1957information2} According to Jaynes, Gibbs entropy in statistical mechanics and Shannons information entropy are identical except for the Boltzmann constant.\cite{jaynes1957information} Consequently, statistical mechanics can be viewed from the perpespective of information theory and the seek for microscopic distributions can be treated as an inference problem.\cite{jaynes1957information} 

Jaynes adapted a bayesian perpective (see Sec. \ref{bayes_vs_frequentist}) and treated the testable information, i.e. the macroscopic observables, as prior information.\cite{jaynes1957information} Obviously, information is missing that is needed to determine the specific microstate of a system that remains uncommunicated due to a description of the system solely in terms of macroscopic quantities. Consequently, finding the microscopic probability distribution amounts to assign probabilities to microstates that avoid bias while still aggreeing with the observed macroscopic quantities.\cite{jaynes1957information} In other words, out of all possible distributions the one has to be chosen which is the least-informative to avoid arbitrary assumptions.

Shannon has showed, that the entropy written in Eq. \ref{gibbs_entropy} (without the Boltzmann constant) is a quantity that increases with increasing uncertainty.\cite{shannon1948mathematical} Consequently, the distribution that maximizes the entropy has to be found under the constraints of the given prior information. From this point of view, the principle of equal a priori probabilities is a consequence of our ignorance about the microscopic details of the system. In practice, the problem is solved introducing Lagrange multiplier to impose the constraints. For the microcanonical ensemble, the constraint renders to a zero probability for all microstates with different total energy than the observed one. In the canonical ensemble, the constraint is a fixed expectation value for the energy.\cite{jaynes1957information}

% -----------------------------------
%   Molecular Dynamics Simulation
% -----------------------------------

\section{Molecular Dynamics Simulation: Sampling from the Thermodynamic Ensembles}

In many cases, it is possible to express the basic laws of nature in terms of relatively simple equations. Unfortunately, solving such equations analytically, such as the equations of motion for more than two interacting bodies, becomes impossible in many cases. However, it is central to the statistical mechanics view on thermodynamics to consider systems with an extremely large number of interacting particles. 
Fortunately, Computer simulations can circumvent these issues. The two main branches of computer simulation techniques for molecular systems are molecular dynamics (MD) and Monte Carlo (MC), but a whole range of hybrid techniques exists as well. In the following, the focus is set on MD simulations.

\textit{Molecular Dynamics} (MD) simulation numerically integrats Newtons equations of motion allowing to analyze the physical movement of atoms or molecules and predict bulk properties. The starting point for every MD simulation is to set up a model for the molecular interactions, i.e. the forcefield, and to select initial positions and velocities for the particles. Afterwards, the system is evolved in time using descrete timesteps repeatly updating the particle positions and velocities. To this end, the forces have to be calculated at every step. However, the algorithm and parameters for the numerical intergration have to be taken with great care in order to minimize the effects of cumulative errors. After the system is equilibrated, i.e. it has evolved for a sufficient amount of time such that macroscopic proties do not change anymore, snapshots or whole trajectories of the system can be extracted and the actual measurement of the observable quantity of interest can be performed. In this regard, MD simulation builts a bridge between theory and experiment, as it allows to test a model and compare it with experimental results. 

\cite{shell2019lecturenotes}, \cite{frenkel2001understanding}

\subsection{Molecular Force Fields}

Calculating forces acting on the particles is crucial to evolve the system in time. To this end, a definition for the potential energy function is required describing the interactions between the particles. These potentials can be defined on vary different levels of resolution. In the following, the classical description is used, which ignores the motion of the electrons and focuses solely on the motion of the nuclei. In this regard, the classical description approximates the effect of electrons as a potential energy surface representing the quantum ground-state. This description is reasonable in cases where the Born-Oppenheimer approximation is valid and the electronic structure is not of interest. Furthermore, bond breaking or forming is prohebitted. 

Typically, the potential $U (\mathbf{r})$ is divided into a term representing the bonded interactions and a term for the nonbonded interactions

\begin{equation}
  U = U_{\text{bonded}} + U_{\text{nonbonded}} .
\end{equation}

Most of the forcefields used in classic MD are emperical and are obtained by fitting simple analytic functions against experimental data or detailed electronic calculations, such as density-functional theory (DFT). Bonded interactions are associated with chemical bonds, bond angles and bond dihedrals. A typical choice is to deploy harmonic and cosine potentials to model the bonded interactions, such as

\begin{equation}
  U_{\text{bonded}} = \sum_{\text{bonds}} a(d - d_0)^2 + \sum_{\text{angles}} b(\Phi - \Phi_0)^2 + \sum_{\text{dihedrals}} \Big( \sum_{n} c_n cos(\omega)^n \Big) ,
\end{equation}

where $d$ and $d_0$ are the calculated and the equilibrium bond length respectively, $\Phi$ and $\Phi_0$ the calculated and equilibrium bond angles and $\omega$ is the calculated dihedral angle. The parameters $a$, $b$ and $c_n$ are the strength for the harmonic and the cosine series potential repectively. 

The nonbonded potential is associated with van der Waals attraction, pauli repulsion and electrostatic interactions. The van der Waals attraction arises due to the correlation between instantaneous dipoles between electron clouds of the atoms. The Pauli repulsion occurs upon the overlap of electron clouds and is a consequence of the Pauli principle, which forbids any two electrons from having the same quantum numbers. Both, the van der Waals attraction and the Pauli repulsion, are often combined into a single expression, such as the Lennard-Jones potential. The electrostatic forced arise due to partial or formal charges of the atoms and are taken into account through Coulomb's law. In combination, a typical nonbonded potential is modeled as

\begin{equation}
  U_{\text{nonbonded}} = \sum_{\text{pairs}} \Big( \underbrace{ 4 \epsilon \bigg[ \big( \frac{r_{ij}}{\sigma} \big)^{-12} - \big( \frac{r_{ij}}{\sigma} \big)^{-6} \bigg] }_{\text{Lennard-Jones}} + \underbrace{ \frac{q_i q_j}{4 \pi \epsilon_0 r_{ij}} }_{\text{Coulomb}} \Big) , 
\end{equation}

where $r_{ij}$ is the pairwise distance, $q_i$ and $q_j$ are the net (partial) charges, $\epsilon$ and $\sigma$ are the Lennard-Jones parameters, which depend on the particular atom types and $\epsilon_0$ is the electric permitivity in vacuum. 

Obviously, calculating the nonbonded interactions is computationally more expensive compared to the bonded interactions, since the number of terms in the pairwise atomic sum scales as $N^2$, while the other scale as $N$. To reduce the computational overhead, the Lennard-Jones potential is often truncated as its contribution becomes minimal for large distances. To this end, a cutoff distance of $r_c \approx 2.5 \sigma$ is typically used where the energy is only a few percent of the minimum energy ($U_{\text{LJ}}(r_c) \approx -0.016 \epsilon$). In addition, it is common practice to shift to potential to avoid discontinuities, i.e. subtracting the value of the potential at the cutoff. However, the truncated contributions can become significant for the total energy and pressure of the system. To this end, a correction to the total potential can be introduced, which can be derived analytically for isotropic systems. On the other hand, the long-range Coulomb interaction need a special treadment, as a a a tail correction can not be derived directly. Therefore, Ewald summation is typically used to reduce the computational effort. In this case, the potential is split into a short-range and long-range contribution. The short-range contributions are computed in real space, while the long-range contributions are computed in Fourier space. 

\subsection{Numerical Integration}

The time evolution of the molecular system is described by Newton's equations of motion, i.e. given the potential energy function $U(\mathbf{r})$ the forces $m \frac{d^2 \mathbf{r}}{d t^2}$ acting on the $N$ atoms are computed as

\begin{equation}
\label{newtons_eq_motion}
  m \frac{d^2 \mathbf{r}}{d t^2} = - \frac{d U (\mathbf{r})}{d \mathbf{r}} .
\end{equation}

Starting from initial positions and velocities, the time-evolution of the system traces a path in phase space, called trajectory. This trajectory is a set of states compatible with the starting condition. For classical systems, the phase space trajectory lies on a surface of constant energy, as Newton's equations conserve energy. For ergodic systems, the trajectory will eventually visit all points in phase space compatible with the given total energy, whereas systems that are non-ergodic have areas in phase space that are inacessible. Obviously, it is not possible to sample all states of the trajectory. However, MD simulations aim at sampling the accessible phase space representatively.

Note that Eq. \ref{newtons_eq_motion} is a set of $3N$ second-order, nonlinear, coupled partial differential equations, which can not be solved analytically. Therefore, numerical integration is used to evolve the system in time. Here, the basic idea is to introduce a small timestep $\delta t$ and find the positions of the atoms at consecutive time steps, i.e. 

\begin{equation}
  \mathbf{r}(0), \mathbf{r}(\delta t), \mathbf{r}(2 \delta t), ..
\end{equation}

Many different algorithms exist to progate the system forward in time. As an example, the Verlet algorithm is explained in the following.

\subsubsection{Verlet Algorithm}

Using a Taylor expansion, the position at time $t + \delta t$ can be written as

\begin{equation}
\label{verlet_taylor1}
  \mathbf{r}(t + \delta t) = \mathbf{r}(t) + \frac{d \mathbf{r}(t)}{d t} \delta t + \frac{d^2 \mathbf{r}(t)}{d t^2} \frac{ \delta t^2}{2} + \frac{d^3 \mathbf{r}(t)}{d t^3} \frac{ \delta t^3}{6} + \mathcal{O}(\delta t^4) .
\end{equation}

Similarly, the position at the previous time step can be written as

\begin{equation}
\label{verlet_taylor2}
  \mathbf{r}(t - \delta t) = \mathbf{r}(t) - \frac{d \mathbf{r}(t)}{d t} \delta t + \frac{d^2 \mathbf{r}(t)}{d t^2} \frac{ \delta t^2}{2} - \frac{d^3 \mathbf{r}(t)}{d t^3} \frac{ \delta t^3}{6} + \mathcal{O}(\delta t^4) .
\end{equation}

Adding Eq. \ref{verlet_taylor1} and \ref{verlet_taylor2} and rearranging leads to

\begin{equation}
\label{verlet}
  \mathbf{r}(t + \delta t) = 2 \mathbf{r}(t) - \mathbf{r}(t - \delta t) + \frac{d^2 \mathbf{r}(t)}{d t^2} \frac{ \delta t^2}{2} + \mathcal{O}(\delta t^4) .
\end{equation}

Eq. \ref{verlet} allows to compute the positions at the next time step from the positions at the two previous timesteps and the forces, which can be calculated using Eq. \ref{newtons_eq_motion}. The error of this algorithm is of order $\mathcal{O}(\delta t^4)$ and therefore decreases with the time step.

Note that the Verlet algorithm has two important features: It is time-reversible and symplectic, i.e. volume in phase space is preserved. Those features are crucial to maintain correct statistical sampling and stability, because they assure that a drift of the total energy on a long time scale is avoided. 

% -----------------------------------
%   Coarse-Graining
% -----------------------------------

\section{Coarse-Graining}

\subsection{Top-down vs Bottom-up}

\cite{noid2013perspective}

\subsection{Mapping Operator}

\cite{noid2008multiscale}

\subsection{Many-Body Potential of Mean Force}

\cite{frenkel2001understanding}, \cite{noid2008multiscale}

\subsection{Consistency Criteria}

\cite{noid2008multiscale}

\subsection{Coarse-Grained Force Field}

\subsubsection{Direct Boltzmann Inversion}

\cite{tschop1998simulation}

\subsubsection{Iterative Boltzmann Inversion}

\cite{reith2003deriving}

\subsubsection{Multiscale Coarse-Graining}

\cite{noid2008multiscale}

\subsubsection{Relative Entropy}

\cite{shell2008relative}, \cite{chaimovich2010relative}, \cite{chaimovich2011coarse}

\subsubsection{Martini Force Field}

\cite{marrink2007martini}, \cite{marrink2013perspective}

\subsubsection{Kremer-Grest Polymer Model}

\cite{kremer1990dynamics}, \cite{grest1986molecular}, \cite{faller1999local}, \cite{everaers2020kremer}

% -----------------------------------
%   Backmapping
% -----------------------------------

\section{Backmapping}

\subsection{The Challenges of Reintroducing Degrees of Freedom}

\cite{tschop1998simulation}

\subsubsection{Consistency with Coarse-Grained Mapping}

\subsubsection{One-to-Many Mapping}

\subsection{Overview of Existing Approaches}

\subsubsection{Generic Approaches}

\cite{rzepiela2010reconstruction}, \cite{wassenaar2014going}

\subsubsection{Fragment-based Approaches}

\cite{peter2009multiscale}, \cite{zhang2019hierarchical}, \cite{hess2006long}, \cite{brasiello2012multiscale}

\subsubsection{Machine Laarning Approaches}

\cite{wang2019coarse}, \cite{li2020backmapping}
